import { NextRequest, NextResponse } from 'next/server'
import fs from 'fs'
import path from 'path'

// Determine context based on the conversation or user input
function determineContext(userMessage: string, conversationHistory?: any[]): string {
  const message = userMessage.toLowerCase()
  
  // Keywords for different contexts
  if (message.includes('tâm lý') || message.includes('stress') || message.includes('lo âu') || 
      message.includes('trầm cảm') || message.includes('tâm trạng') || message.includes('cảm xúc')) {
    return 'psychological support'
  }
  
  if (message.includes('tra cứu') || message.includes('thông tin') || message.includes('bệnh') ||
      message.includes('thuốc') || message.includes('triệu chứng') || message.includes('chẩn đoán')) {
    return 'health lookup'
  }
  
  // Default to health consultation
  return 'health consultation'
}

export async function POST(request: NextRequest) {
  try {
    const { prompt, context, question, message, conversationHistory, model, conversation_id, user_id, persona, systemPrompt: systemPromptOverride, role } = await request.json()
    const auth = request.headers.get('authorization') || ''
    const referer = request.headers.get('referer') || ''
    
    const userMessage = message || question || prompt
    if (!userMessage) {
      return NextResponse.json(
        { error: 'Message is required' },
        { status: 400 }
      )
    }
    
    // Determine context based on user message
    const determinedContext = context || determineContext(userMessage, conversationHistory)
    
    const cpuFallback = process.env.INTERNAL_LLM_URL || 'http://127.0.0.1:8000/v1/chat/completions'
    let fastApiUrl = `${String(defaultGpuUrl).replace(/\/$/, '')}/v1/chat/completions`
    let originalTarget: 'cpu' | 'gpu' = 'gpu'
    try {
      const dataDir = path.join(process.cwd(), 'data')
      // Prefer runtime-mode gpu_url if set
      try {
        const modeRaw = fs.readFileSync(path.join(dataDir, 'runtime-mode.json'), 'utf-8')
        const mode = JSON.parse(modeRaw)
        if (mode?.target === 'cpu' || mode?.target === 'gpu') {
          originalTarget = mode.target
        }
        if (mode?.gpu_url) {
          fastApiUrl = `${String(mode.gpu_url).replace(/\/$/, '')}/v1/chat/completions`
        }
      } catch {}
      // Otherwise pick latest from server registry
      try {
        const regRaw = fs.readFileSync(path.join(dataDir, 'server-registry.json'), 'utf-8')
        const reg = JSON.parse(regRaw)
        const servers = Array.isArray(reg?.servers) ? reg.servers : []
        const active = servers.filter((s: any) => s.status === 'active')
        const latest = (active.length ? active : servers).sort((a: any, b: any) => new Date(b.updated_at).getTime() - new Date(a.updated_at).getTime())[0]
        if (latest?.url) {
          fastApiUrl = `${String(latest.url).replace(/\/$/, '')}/v1/chat/completions`
        }
      } catch {}
    } catch {}
    const personaText = typeof systemPromptOverride === 'string' && systemPromptOverride.trim()
      ? systemPromptOverride.trim()
      : (() => {
          const BASE_SYSTEM_PROMPT = `Bạn là Trợ lý Y tế AI (Medical Consultant AI). Nhiệm vụ của bạn là cung cấp thông tin y tế hữu ích, chính xác và an toàn bằng Tiếng Việt.

NGUYÊN TẮC QUAN TRỌNG:
1. AN TOÀN LÀ TRÊN HẾT: Luôn khuyến cáo người dùng đi khám bác sĩ hoặc đến cơ sở y tế nếu có dấu hiệu nghiêm trọng. Không đưa ra chẩn đoán khẳng định hoặc kê đơn thuốc thay thế bác sĩ.
2. KHÁCH QUAN & KHOA HỌC: Dựa trên kiến thức y khoa đã được kiểm chứng.
3. NGÔN NGỮ: Sử dụng Tiếng Việt chuẩn mực, dễ hiểu, giọng điệu ân cần, chuyên nghiệp.
4. TỪ CHỐI TRẢ LỜI: Nếu câu hỏi không liên quan đến y tế/sức khỏe hoặc vi phạm đạo đức, hãy lịch sự từ chối hoặc lái về chủ đề y tế.`

          const p = (typeof persona === 'string' && persona.trim()) ? persona.trim() : (typeof role === 'string' && role.trim() ? role.trim() : '')
          
          let specificInstruction = ""
          if (determinedContext === 'psychological support') {
             specificInstruction = "CONTEXT: Hỗ trợ tâm lý. Hãy lắng nghe, thấu cảm, không phán xét. Gợi ý các phương pháp giảm căng thẳng lành mạnh."
          } else if (determinedContext === 'health lookup') {
             specificInstruction = "CONTEXT: Tra cứu thông tin y tế. Cung cấp thông tin ngắn gọn, súc tích, chính xác."
          } else {
             specificInstruction = `CONTEXT: ${determinedContext}`
          }

          if (p) return `${BASE_SYSTEM_PROMPT}\n\nVAI TRÒ CỤ THỂ: ${p}.\n${specificInstruction}`
          return `${BASE_SYSTEM_PROMPT}\n${specificInstruction}`
        })()
    const systemPrompt = personaText
    const selectedModel = (typeof model === 'string' ? model.toLowerCase() : 'flash')
    const modeHeader = selectedModel === 'pro' ? 'pro' : 'flash'
    const body = {
      model: selectedModel,
      mode: modeHeader,
      messages: [
        { role: 'system', content: systemPrompt },
        ...(Array.isArray(conversationHistory) ? conversationHistory : []).map((m: any) => ({
          role: m.role || 'user',
          content: m.content || ''
        })),
        { role: 'user', content: userMessage }
      ],
      max_tokens: 1024,
      temperature: 0.3,
      conversation_id: typeof conversation_id === 'string' ? conversation_id : null,
      user_id: typeof user_id === 'string' ? user_id : null
    }

    const start = Date.now()
    let resp = await fetch(fastApiUrl, {
      method: 'POST',
      headers: auth ? { 'Content-Type': 'application/json', 'Authorization': auth, 'ngrok-skip-browser-warning': 'true', 'X-Mode': modeHeader } : { 'Content-Type': 'application/json', 'ngrok-skip-browser-warning': 'true', 'X-Mode': modeHeader },
      body: JSON.stringify(body)
    })
    let modeUsed = fastApiUrl.includes('127.0.0.1') || fastApiUrl.includes('localhost') ? 'cpu' : 'gpu'
    if (!resp.ok) {
      try {
        if (modeUsed === 'gpu') {
          const fallbackUrl = cpuFallback
          const retry = await fetch(fallbackUrl, {
            method: 'POST',
            headers: auth ? { 'Content-Type': 'application/json', 'Authorization': auth, 'ngrok-skip-browser-warning': 'true' } : { 'Content-Type': 'application/json', 'ngrok-skip-browser-warning': 'true' },
            body: JSON.stringify(body)
          })
          if (retry.ok) {
            resp = retry
            modeUsed = 'cpu'
            try {
              fs.appendFileSync(path.join(process.cwd(), 'data', 'runtime-events.jsonl'), JSON.stringify({ type: 'fallback', from: 'gpu', to: 'cpu', ts: new Date().toISOString() }) + '\n')
              const now = new Date().toISOString()
              const dataDir = path.join(process.cwd(), 'data')
              const modePath = path.join(dataDir, 'runtime-mode.json')
              const payload: any = { target: 'cpu', updated_at: now }
              fs.writeFileSync(modePath, JSON.stringify(payload, null, 2))
              fs.appendFileSync(path.join(dataDir, 'runtime-events.jsonl'), JSON.stringify({ type: 'mode_change', target: 'cpu', ts: now }) + '\n')
            } catch {}
          }
        }
      } catch {}
    }

    if (!resp.ok) {
      const text = await resp.text()
      console.error('LLM server error:', text)
      return NextResponse.json(
        { error: 'LLM server error', details: text },
        { status: 502 }
      )
    }

    let data
    try {
      const responseText = await resp.text()
      console.log('Raw response:', responseText)
      
      if (!responseText || responseText.trim() === '') {
        throw new Error('Empty response from server')
      }
      
      data = JSON.parse(responseText)
    } catch (parseError) {
      console.error('JSON parsing error:', parseError)
      return NextResponse.json(
        { error: 'Invalid JSON response from server', details: parseError instanceof Error ? parseError.message : 'Unknown parsing error' },
        { status: 502 }
      )
    }

    const duration = Date.now() - start
    try {
      fs.appendFileSync(path.join(process.cwd(), 'data', 'runtime-metrics.jsonl'), JSON.stringify({ mode: modeUsed, duration_ms: duration, ok: !!data, ts: new Date().toISOString(), endpoint: 'llm-chat' }) + '\n')
    } catch {}
    try {
      if (modeUsed === 'gpu') {
        const base = fastApiUrl.replace(/\/v1\/chat\/completions$/, '')
        const gm = await fetch(`${base}/gpu/metrics`, { headers: { 'ngrok-skip-browser-warning': 'true' } })
        if (gm.ok) {
          const v = await gm.json()
          fs.appendFileSync(path.join(process.cwd(), 'data', 'runtime-events.jsonl'), JSON.stringify({ type: 'gpu_metrics', ts: new Date().toISOString(), data: v }) + '\n')
        }
      }
      // Log frontend call for debugging origin
      try {
        const baseUrl = fastApiUrl.replace(/\/v1\/chat\/completions$/, '')
        fs.appendFileSync(path.join(process.cwd(), 'data', 'runtime-events.jsonl'), JSON.stringify({ type: 'frontend_call', endpoint: 'llm-chat', referer, target_base: baseUrl, ts: new Date().toISOString() }) + '\n')
      } catch {}
    } catch {}
    const content = data?.choices?.[0]?.message?.content || data?.response || ''
    const newConversationId = data?.conversation_id || conversation_id || null

    if (!content) {
      console.error('No content in response:', data)
      return NextResponse.json(
        { error: 'No content in response', details: JSON.stringify(data) },
        { status: 502 }
      )
    }

    return NextResponse.json({
      response: content,
      context: determinedContext,
      model_info: {
        model_name: 'local-llama-compatible',
        provider: 'Internal FastAPI'
      },
      metadata: {
        context: determinedContext,
        prompt_length: userMessage.length,
        response_length: content.length,
        timestamp: new Date().toISOString(),
        mode: modeUsed,
        tier: modeHeader,
        fallback: originalTarget === 'gpu' && modeUsed === 'cpu',
        model_init: !!(data && (data as any).model_init),
        rag: (data && (data as any).rag) ? (data as any).rag : undefined
      },
      conversation_id: newConversationId
    })
    
  } catch (error) {
    console.error('Error in internal chat API:', error)
    return NextResponse.json(
      { error: 'Internal server error', details: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    )
  }
}
const defaultGpuUrl = process.env.DEFAULT_GPU_URL || 'https://elissa-villous-scourgingly.ngrok-free.dev'
