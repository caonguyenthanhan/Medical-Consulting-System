# -*- coding: utf-8 -*-
"""funny_companion_full_training_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Loh18Cka4yAqg-cdZpGYmon2KkbWPOy
"""

# @title CELL 0 â€” MOUNT GOOGLE DRIVE
from google.colab import drive
drive.mount('/content/drive')

# @title CUDA MEMORY CONF
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# @title CELL 1 â€” INSTALL LIBS
!pip install -q transformers datasets peft accelerate bitsandbytes

# @title CELL 2 â€” IMPORTS & PATHS
import os, glob, json
import torch

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model

TRAIN_DIR = "/content/drive/MyDrive/DoctorAI - Trá»£ lÃ½ Sá»©c khá»e CÃ¡ nhÃ¢n ThÃ´ng minh/tam_ly/raw_data"
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"  # hoáº·c 3B

# @title CHECK TRAIN FILES (COLAB)
import glob, os

files = glob.glob(f"{TRAIN_DIR}/*.jsonl")
print("Sá»‘ file train:", len(files))
print(files[:5])

for f in files[:3]:
    print(f, "size =", os.path.getsize(f))

# @title CELL 3 â€” LOAD DATASET
files = [f for f in glob.glob(f"{TRAIN_DIR}/*.jsonl") if os.path.getsize(f) > 0]
print("Sá»‘ file dÃ¹ng train:", len(files))

dataset = load_dataset(
    "json",
    data_files=files,
    split="train"
)

print("Sá»‘ máº«u:", len(dataset))
print(dataset[0])

# @title CELL 4 â€” TOKENIZER
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.model_max_length = 1024

# @title CELL 5 â€” FORMAT CHAT

def formatting_prompts_func(example):
    return {
        "text": tokenizer.apply_chat_template(
            example["messages"],
            tokenize=False,
            add_generation_prompt=False
        )
    }

dataset = dataset.map(formatting_prompts_func)
print(dataset[0]["text"][:500])

# @title CELL 6 â€” TOKENIZE + LABELS

def tokenize_function(batch):
    tokens = tokenizer(
        batch["text"],
        truncation=True,
        max_length=768,        # ğŸ”´ GIáº¢M Tá»ª 1024
        padding=False          # ğŸ”´ KHÃ”NG PAD
    )
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens


dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["messages", "text"]
)

print(dataset[0].keys())

# @title CELL 7 â€” MODEL + LORA + MEMORY FIX

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# ğŸ”’ Giáº£m VRAM máº¡nh
model.gradient_checkpointing_enable()
model.config.use_cache = False

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# @title CELL 8 â€” TRAINER

from transformers import TrainingArguments, DataCollatorForSeq2Seq

training_args = TrainingArguments(
    output_dir="./output",
    num_train_epochs=2,
    per_device_train_batch_size=2,   # ğŸ”’ batch nhá»
    gradient_accumulation_steps=8,   # ğŸ”’ batch hiá»‡u dá»¥ng = 16
    learning_rate=2e-4,
    logging_steps=10,
    save_steps=500,
    bf16=True,
    report_to="none",
    optim="paged_adamw_8bit"
)

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding=True,              # ğŸ”’ padding Ä‘á»™ng
    label_pad_token_id=-100    # ğŸ”’ mask padding khá»i loss
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator
)

# @title CELL 9 â€” TRAIN
trainer.train()

# @title CELL 10 â€” SAVE
trainer.model.save_pretrained("./lora_model")
tokenizer.save_pretrained("./lora_model")

# @title FIXED TEST â€” CHAT TEMPLATE

messages = [
    {"role": "user", "content": "HÃ´m nay tao má»‡t quÃ¡, Ä‘i lÃ m vá» chá»‰ muá»‘n náº±m im."}
]

prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True   # ğŸ”´ Ráº¤T QUAN TRá»ŒNG
)

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=120,
    do_sample=True,
    temperature=0.9,
    top_p=0.92,
    repetition_penalty=1.1
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# @title CELL 12 â€” UPLOAD LORA TO HUGGING FACE
!pip install -q huggingface_hub

# @title LOGIN HF (HUGGINGFACE_HUB_TOKEN)
import os
from huggingface_hub import login

login(token=os.environ["HF_TOKEN_WIRTE"])

# @title PUSH LORA MODEL
from huggingface_hub import HfApi

REPO_ID = "An-CNT/doctorai-tamly-lora"  # ğŸ”´ Äá»”I THÃ€NH REPO Cá»¦A Báº N

api = HfApi()
api.create_repo(repo_id=REPO_ID, exist_ok=True)

trainer.model.push_to_hub(REPO_ID)
tokenizer.push_to_hub(REPO_ID)

print("âœ… ÄÃ£ upload LoRA lÃªn Hugging Face:", REPO_ID)