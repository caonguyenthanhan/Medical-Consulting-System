# @title 6. Kh·ªüi ch·∫°y Server API (VRAM Safe Mode)
from fastapi import FastAPI, File, UploadFile, Form, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional
import nest_asyncio
from pyngrok import ngrok
from PIL import Image
import librosa
import soundfile as sf
import numpy as np
from gtts import gTTS
import threading
import time
import socket
import uuid
import gc
import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel
import torchvision.transforms as T
from torchvision.transforms.functional import InterpolationMode
from peft import PeftModel
from fastapi import Body
import re
import base64
from io import BytesIO
import asyncio

try:
    import pypdf
except ImportError:
    pypdf = None

try:
    import docx
except ImportError:
    docx = None

# Patch async cho Colab
try:
    nest_asyncio.apply()
except Exception:
    pass

app = FastAPI(title="Medical Consultation GPU API", version="2.1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"status": "ok", "service": "gpu", "version": "2.1.0"}

@app.get("/health")
async def health():
    return {"status": "ok"}

ocr_engine = None
ocr_last_error = None
ocr_backend = None

def _ensure_ocr():
    global ocr_engine, ocr_last_error, ocr_backend
    if ocr_engine is not None:
        return True
    try:
        import sys
        if 'paddleocr' in sys.modules:
            PaddleOCR = sys.modules['paddleocr'].PaddleOCR
        else:
            from paddleocr import PaddleOCR  # lazy import to avoid PDX reinit issues
        print("‚è≥ ƒêang t·∫£i PaddleOCR cho ti·∫øng Vi·ªát...")
        try:
            ocr_engine = PaddleOCR(use_angle_cls=True, lang='vi', show_log=False)
        except Exception as e_vi:
            ocr_last_error = str(e_vi)
            try:
                ocr_engine = PaddleOCR(use_angle_cls=True, lang='latin', show_log=False)
            except Exception as e_lat:
                ocr_last_error = f"{str(e_vi)} | {str(e_lat)}"
                try:
                    ocr_engine = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)
                except Exception as e_en:
                    ocr_last_error = f"{str(e_vi)} | {str(e_lat)} | {str(e_en)}"
                    raise e_en
        print("‚úÖ PaddleOCR ƒë√£ s·∫µn s√†ng!")
        ocr_last_error = None
        ocr_backend = "paddle"
        return True
    except RuntimeError:
        try:
            import sys
            if 'paddleocr' in sys.modules:
                PaddleOCR = sys.modules['paddleocr'].PaddleOCR
                try:
                    ocr_engine = PaddleOCR(use_angle_cls=True, lang='vi', show_log=False)
                except Exception as e_vi:
                    ocr_last_error = str(e_vi)
                    try:
                        ocr_engine = PaddleOCR(use_angle_cls=True, lang='latin', show_log=False)
                    except Exception as e_lat:
                        ocr_last_error = f"{str(e_vi)} | {str(e_lat)}"
                        ocr_engine = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)
            ocr_last_error = None
            ocr_backend = "paddle"
            return True
        except Exception as e:
            ocr_last_error = str(e)
        try:
            import easyocr
            langs = ['vi', 'en']
            try:
                ocr_engine = easyocr.Reader(langs, gpu=torch.cuda.is_available())
                ocr_backend = "easyocr"
                ocr_last_error = None
                return True
            except Exception as e2:
                ocr_last_error = f"{ocr_last_error or ''} | {str(e2)}"
        except Exception as e3:
            ocr_last_error = f"{ocr_last_error or ''} | {str(e3)}"
        return False
    except Exception as e:
        ocr_last_error = str(e)
        return False

# --- C·∫¨P NH·∫¨T PH·∫¶N N√ÄY TRONG CELL 6 ---

vintern_model = None
vintern_tokenizer = None

def _ensure_vintern():
    global vintern_model, vintern_tokenizer
    if vintern_model is not None and vintern_tokenizer is not None:
        return True
    
    model_id = "5CD-AI/Vintern-3B-R-beta"
    print(f"‚è≥ ƒêang t·∫£i m√¥ h√¨nh Vision: {model_id}...")
    try:
        # T·∫£i model v·ªõi bfloat16 ƒë·ªÉ ti·∫øt ki·ªám VRAM v√† gi·ªØ ƒë·ªô ch√≠nh x√°c
        vintern_model = AutoModel.from_pretrained(
            model_id,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            use_flash_attn=False # B·∫≠t True n·∫øu GPU h·ªó tr·ª£ Flash Attention 2 (A100, H100, L4...)
        ).eval().cuda()
        
        vintern_tokenizer = AutoTokenizer.from_pretrained(
            model_id, 
            trust_remote_code=True, 
            use_fast=False
        )
        print("‚úÖ Vintern-3B ƒë√£ s·∫µn s√†ng!")
        return True
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i Vintern-3B: {e}")
        vintern_model = None
        vintern_tokenizer = None
        return False



IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def _build_transform(input_size: int):
    return T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
    ])

def _find_closest_aspect_ratio(aspect_ratio: float, target_ratios: list, width: int, height: int, image_size: int):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def _dynamic_preprocess(image: Image.Image, min_num: int = 1, max_num: int = 12, image_size: int = 448, use_thumbnail: bool = False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height
    target_ratios = set((i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
    target_aspect_ratio = _find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = ((i % (target_width // image_size)) * image_size, (i // (target_width // image_size)) * image_size, ((i % (target_width // image_size)) + 1) * image_size, ((i // (target_width // image_size)) + 1) * image_size)
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
        return processed_images

def _perform_ocr(image: Image.Image) -> str:
    try:
        if ocr_backend == "paddle":
            import numpy as _np
            result = ocr_engine.ocr(_np.array(image), cls=True)
            if result and len(result) > 0 and isinstance(result[0], list):
                return "\n".join([line[1][0] for line in result[0] if isinstance(line, list) and len(line) > 1])
            return ""
        if ocr_backend == "easyocr":
            import numpy as _np
            lines = ocr_engine.readtext(_np.array(image), detail=1)
            texts = []
            for item in lines:
                try:
                    t = item[1]
                    if isinstance(t, str) and t.strip():
                        texts.append(t.strip())
                except Exception:
                    pass
            return "\n".join(texts)
        return ""
    except Exception:
        return ""

def _pixels_from_image(image: Image.Image, input_size: int = 448, max_num: int = 6):
    transform = _build_transform(input_size=input_size)
    images = _dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(img) for img in images]
    return torch.stack(pixel_values)

# ==============================
# üî∂ DATA MODELS
# ==============================
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: Optional[str] = "llama-3.2-3b"
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.5
    max_tokens: Optional[int] = 512
    mode: Optional[str] = "pro"

class VisionMultiRequest(BaseModel):
    text: str
    images_base64: List[str]
    temperature: Optional[float] = 0.2
    max_tokens: Optional[int] = 256
    model_id: Optional[str] = None

class DocumentChatRequest(BaseModel):
    text: str
    doc_base64: str
    doc_name: str
    model: Optional[str] = "flash"

class VisionChatResponse(BaseModel):
    success: bool
    response: Optional[str] = None
    error: Optional[str] = None

class TTSRequest(BaseModel):
    text: str
    lang: Optional[str] = "vi"
class AutoTitleRequest(BaseModel):
    messages: Optional[List[ChatMessage]] = None
    user_text: Optional[str] = None
    assistant_text: Optional[str] = None
    max_tokens: Optional[int] = 24
class HealthLookupRequest(BaseModel):
    query: str
    mode: Optional[str] = None
    user_id: Optional[str] = None
    conversation_id: Optional[str] = None

FRIEND_LORA_REPO = os.environ.get("FRIEND_LORA_REPO", "An-CNT/doctorai-tamly-lora-v2-final")
FRIEND_BASE_MODEL = os.environ.get("FRIEND_BASE_MODEL", "unsloth/Llama-3.2-3B-Instruct")
_friend_bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)
friend_lora_tokenizer = None
friend_lora_model = None
def _ensure_friend_lora():
    global friend_lora_tokenizer, friend_lora_model
    if friend_lora_tokenizer is not None and friend_lora_model is not None:
        return True
    try:
        base_model = AutoModelForCausalLM.from_pretrained(FRIEND_BASE_MODEL, quantization_config=_friend_bnb_config, device_map="auto")
        friend_lora_model = PeftModel.from_pretrained(base_model, FRIEND_LORA_REPO)
        friend_lora_model.eval()
        friend_lora_tokenizer = AutoTokenizer.from_pretrained(FRIEND_LORA_REPO, use_fast=True)
        return True
    except Exception:
        try:
            base_model = AutoModelForCausalLM.from_pretrained(FRIEND_BASE_MODEL, torch_dtype=torch.bfloat16, device_map="auto")
            friend_lora_model = PeftModel.from_pretrained(base_model, FRIEND_LORA_REPO)
            friend_lora_model.eval()
            friend_lora_tokenizer = AutoTokenizer.from_pretrained(FRIEND_LORA_REPO, use_fast=True)
            return True
        except Exception:
            return False

def _extract_text_from_doc(doc_base64: str, doc_name: str) -> str:
    import base64
    import io
    
    try:
        decoded = base64.b64decode(doc_base64)
        file_stream = io.BytesIO(decoded)
        ext = doc_name.split('.')[-1].lower()
        
        text = ""
        if ext == 'pdf':
            if pypdf:
                reader = pypdf.PdfReader(file_stream)
                for page in reader.pages:
                    text += page.extract_text() + "\n"
            else:
                return "Error: pypdf library not found. Please install it."
        elif ext in ['docx', 'doc']:
            if docx:
                doc = docx.Document(file_stream)
                for para in doc.paragraphs:
                    text += para.text + "\n"
            else:
                return "Error: python-docx library not found. Please install it."
        else:
            try:
                text = decoded.decode('utf-8')
            except:
                return "Error: Unsupported document format."
                
        return text.strip()
    except Exception as e:
        return f"Error extracting text: {str(e)}"



@app.post("/v1/document-chat")
async def document_chat(req: DocumentChatRequest):
    print(f"üì• [Doc Chat Request] Doc: {req.doc_name} | Text len: {len(req.text)}")
    
    # 1. Ki·ªÉm tra ƒë·∫ßu v√†o
    if not req.doc_base64 or not req.text:
        return VisionChatResponse(success=False, error="Thi·∫øu d·ªØ li·ªáu: Y√™u c·∫ßu 'doc_base64' v√† 'text'.")
        
    # 2. Tr√≠ch xu·∫•t vƒÉn b·∫£n (OCR/PDF Parse)
    doc_text = _extract_text_from_doc(req.doc_base64, req.doc_name)
    if doc_text.startswith("Error:"):
        return VisionChatResponse(success=False, error=doc_text)

    # 3. V·ªá sinh s∆° b·ªô vƒÉn b·∫£n (Data Cleaning)
    # Lo·∫°i b·ªè c√°c d√≤ng qu√° ng·∫Øn ho·∫∑c nhi·ªÖu OCR kh√¥ng c·∫ßn thi·∫øt ƒë·ªÉ tr√°nh AI b·ªã ph√¢n t√¢m
    cleaned_text = "\n".join([line for line in doc_text.split('\n') if len(line.strip()) > 2])
        
    # 4. SYSTEM PROMPT: "Nh√¢n c√°ch" chuy√™n gia y t·∫ø nghi√™m kh·∫Øc
    # ƒê√¢y l√† ph·∫ßn quan tr·ªçng nh·∫•t ƒë·ªÉ s·ª≠a l·ªói b·ªãa b·ªánh
    system_instructions = (
        "B·∫°n l√† m·ªôt Chuy√™n gia Ph√¢n t√≠ch H·ªì s∆° Y t·∫ø (Medical Record Analyst) uy t√≠n v√† c·∫©n tr·ªçng.\n"
        "NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p.\n\n"
        "QUY T·∫ÆC AN TO√ÄN (B·∫ÆT BU·ªòC TU√ÇN TH·ª¶):\n"
        "1. GROUNDING (CH·ªà D·ª∞A V√ÄO VƒÇN B·∫¢N): Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin hi·ªÉn th·ªã r√µ r√†ng trong t√†i li·ªáu. Tuy·ªát ƒë·ªëi KH√îNG t·ª± suy ƒëo√°n hay th√™m th·∫Øt th√¥ng tin b√™n ngo√†i.\n"
        "2. ANTI-HALLUCINATION (CH·ªêNG B·ªäA ƒê·∫∂T): N·∫øu vƒÉn b·∫£n ghi 'Ph·ªïi trong', 'Tim ƒë·ªÅu' => B·∫ÆT BU·ªòC k·∫øt lu·∫≠n l√† b√¨nh th∆∞·ªùng. C·∫•m b·ªãa ra 'Vi√™m ph·ªïi' hay b·ªánh l√Ω kh√°c n·∫øu kh√¥ng c√≥ b·∫±ng ch·ª©ng ch·ªØ vi·∫øt.\n"
        "3. TRUNG TH·ª∞C V·ªöI D·ªÆ LI·ªÜU THI·∫æU: N·∫øu t√†i li·ªáu kh√¥ng ch·ª©a th√¥ng tin ng∆∞·ªùi d√πng h·ªèi, h√£y tr·∫£ l·ªùi: 'T√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn th√¥ng tin n√†y'.\n"
        "4. CH√çNH X√ÅC CON S·ªê: Tr√≠ch xu·∫•t ch√≠nh x√°c c√°c ch·ªâ s·ªë x√©t nghi·ªám, tu·ªïi, ng√†y th√°ng.\n"
        "5. GI·ªåNG ƒêI·ªÜU: Chuy√™n nghi·ªáp, kh√°ch quan, ng·∫Øn g·ªçn, s·ª≠ d·ª•ng thu·∫≠t ng·ªØ y khoa ch√≠nh x√°c."
    )
    
    # 5. T·∫°o Context (B·ªëi c·∫£nh) r√µ r√†ng
    full_prompt = (
        f"--- B·∫ÆT ƒê·∫¶U T√ÄI LI·ªÜU Y T·∫æ ({req.doc_name}) ---\n"
        f"{cleaned_text}\n"
        f"--- K·∫æT TH√öC T√ÄI LI·ªÜU ---\n\n"
        f"Y√äU C·∫¶U C·ª¶A NG∆Ø·ªúI D√ôNG: {req.text}"
    )
    
    # 6. G·ª≠i y√™u c·∫ßu ƒë·∫øn Model
    try:
        chat_req = ChatRequest(
            model=req.model or "flash",
            messages=[
                ChatMessage(role="system", content=system_instructions),
                ChatMessage(role="user", content=full_prompt)
            ],
            # üî• QUAN TR·ªåNG: Gi·∫£m nhi·ªát ƒë·ªô xu·ªëng th·∫•p ƒë·ªÉ AI b·ªõt "s√°ng t·∫°o"
            # M·ª©c 0.1 gi√∫p AI ch·ªçn c√¢u tr·∫£ l·ªùi s√°t v·ªõi vƒÉn b·∫£n nh·∫•t.
            temperature=0.5,  
            max_tokens=1024,
            mode="pro" # ∆Øu ti√™n ch·∫ø ƒë·ªô x·ª≠ l√Ω k·ªπ n·∫øu c√≥
        )
        
        # T√°i s·ª≠ d·ª•ng logic chat_completions c√≥ s·∫µn
        response_dict = await chat_completions(chat_req)
        
        # X·ª≠ l√Ω k·∫øt qu·∫£ tr·∫£ v·ªÅ (dict ho·∫∑c JSONResponse)
        content = ""
        if isinstance(response_dict, dict):
             content = response_dict.get("choices", [{}])[0].get("message", {}).get("content", "")
        else:
            import json
            body = json.loads(response_dict.body)
            if "error" in body:
                return VisionChatResponse(success=False, error=body["error"])
            content = body.get("choices", [{}])[0].get("message", {}).get("content", "")
            
        return VisionChatResponse(success=True, response=content)
            
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω Document Chat: {e}")
        return VisionChatResponse(success=False, error=str(e))

# ==============================
# üî∑ 1. CHAT API
# ==============================
@app.post("/v1/chat/completions")
async def chat_completions(req: ChatRequest, x_mode: Optional[str] = Header(None)):
    print(f"üì• [Chat Request] Mode: {req.mode} | X-Mode: {x_mode} | Model: {req.model}")
    try:
        # Fix: Handle if x_mode is Header object (internal call default)
        if not isinstance(x_mode, str):
            x_mode = None

        msgs = req.messages or []
        question = ""
        for m in reversed(msgs):
            if m.role.lower() == "user":
                question = m.content
                break
        if not question and msgs:
            question = msgs[-1].content
        try:
            if question:
                print(f"[USER] { question }")
        except Exception:
            pass
        mode = (x_mode or req.mode or "pro").lower()
        classify_prompt = "tr·∫£ l·ªùi ng·∫Øn g·ªçn l√† c√≥ hay kh√¥ng v√† kh√¥ng gi·∫£i th√≠ch g√¨ th√™m: c√¢u h·ªèi sau ƒë√¢y c√≥ li√™n quan y t·∫ø kh√¥ng: " + question
        cls_text_tmpl = chat_tokenizer.apply_chat_template([{"role": "user", "content": classify_prompt}], tokenize=False, add_generation_prompt=True)
        cls_inputs = chat_tokenizer(cls_text_tmpl, return_tensors="pt").to("cuda")
        with torch.no_grad():
            cls_out = chat_model.generate(**cls_inputs, max_new_tokens=8, temperature=0, do_sample=False, pad_token_id=chat_tokenizer.eos_token_id)
        cls_resp = chat_tokenizer.decode(cls_out[0][cls_inputs.input_ids.shape[-1]:], skip_special_tokens=True)
        del cls_inputs, cls_out
        torch.cuda.empty_cache()
        if "kh√¥ng" in cls_resp.lower():
            response_text = "C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng li√™n quan ƒë·∫øn y t·∫ø. Vui l√≤ng ƒë·∫∑t c√¢u h·ªèi kh√°c."
            print(f"üö´ [Refused] Non-medical question: {question}")
            return {
                "id": f"chatcmpl-{uuid.uuid4()}",
                "object": "chat.completion",
                "created": int(time.time()),
                "choices": [{"index": 0, "message": {"role": "assistant", "content": response_text}, "finish_reason": "stop"}],
                "mode": mode
            }
        if mode == "pro":
            try:
                if 'retriever' in globals() and retriever is not None:
                    nodes = retriever.retrieve(question)
                    context_passages = [n.node.get_content() for n in nodes]
                    ranked = context_passages
                    try:
                        if reranker is not None:
                            query_passage_pairs = [[question, p] for p in context_passages]
                            scores = reranker.predict(query_passage_pairs)
                            ranked = [p for _, p in sorted(zip(scores, context_passages), key=lambda x: x[0], reverse=True)]
                    except Exception:
                        ranked = context_passages
                    top_k = min(3, len(ranked))
                    selected = ranked[:top_k]
                    ctx = "ƒê√¢y l√† c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n" + question + "\n\n"
                    ctx += "D∆∞·ªõi ƒë√¢y l√† c√°c ƒëo·∫°n th√¥ng tin li√™n quan:\n"
                    for i, p in enumerate(selected):
                        ctx += "\n[ƒêo·∫°n " + str(i + 1) + "]:\n" + p + "\n"
                    
                    # Update prompt with RAG context
                    rag_info = {"used": True, "retrieved": len(context_passages), "selected": top_k}
                else:
                    # Fallback to basic if retriever is missing
                    ctx = question
                    rag_info = {"used": False, "reason": "retriever_missing"}
            except Exception as e:
                print(f"RAG Error: {e}")
                ctx = question
                rag_info = {"used": False, "error": str(e)}

            doctor_prompt = "B·∫°n l√† b√°c s·ªπ t∆∞ v·∫•n y t·∫ø, kh√¥ng k√™ ƒë∆°n thu·ªëc, kh√¥ng ch·∫©n ƒëo√°n thay th·∫ø chuy√™n m√¥n. Tr·∫£ l·ªùi ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng, ∆∞u ti√™n an to√†n v√† khuy·∫øn c√°o g·∫∑p b√°c s·ªπ khi c·∫ßn."
            
            # Use ctx (either with RAG or just question)
            if rag_info.get("used"):
                 user_content = ctx
            else:
                 user_content = question

            input_text = chat_tokenizer.apply_chat_template(
                [{"role": "system", "content": doctor_prompt}, {"role": "user", "content": user_content}],
                tokenize=False,
                add_generation_prompt=True
            )
            inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
            with torch.no_grad():
                output = chat_model.generate(
                    **inputs,
                    max_new_tokens=req.max_tokens,
                    temperature=req.temperature,
                    do_sample=True if req.temperature > 0 else False,
                    pad_token_id=chat_tokenizer.eos_token_id
                )
            response_text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
            print(f"üì§ [Response] Len: {len(response_text)} chars")
            del inputs, output
            torch.cuda.empty_cache()
            # rag_info updated safely above
        else:
            doctor_prompt = "B·∫°n l√† b√°c s·ªπ t∆∞ v·∫•n y t·∫ø, kh√¥ng k√™ ƒë∆°n thu·ªëc, kh√¥ng ch·∫©n ƒëo√°n thay th·∫ø chuy√™n m√¥n. Tr·∫£ l·ªùi ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng, ∆∞u ti√™n an to√†n v√† khuy·∫øn c√°o g·∫∑p b√°c s·ªπ khi c·∫ßn."
            input_text = chat_tokenizer.apply_chat_template(
                [{"role": "system", "content": doctor_prompt}, {"role": "user", "content": question}],
                tokenize=False,
                add_generation_prompt=True
            )
            inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
            with torch.no_grad():
                output = chat_model.generate(
                    **inputs,
                    max_new_tokens=req.max_tokens,
                    temperature=req.temperature,
                    do_sample=True if req.temperature > 0 else False,
                    pad_token_id=chat_tokenizer.eos_token_id
                )
            response_text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
            del inputs, output
            torch.cuda.empty_cache()
            rag_info = {"used": False, "retrieved": 0, "selected": 0}
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "choices": [{"index": 0, "message": {"role": "assistant", "content": response_text}, "finish_reason": "stop"}],
            "mode": mode,
            "rag": rag_info
        }
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/v1/chat")
async def chat_simple(req: dict, x_mode: Optional[str] = Header(None)):
    # Proxy ƒë∆°n gi·∫£n cho chat
    msgs = req.get("messages", [])
    if not msgs: return {"reply": ""}

    question = ""
    for m in reversed(msgs):
        if m.get("role", "").lower() == "user":
            question = m.get("content", "")
            break
    if not question and msgs:
        question = msgs[-1].get("content", "")

    try:
        chat_req = ChatRequest(
            model=x_mode,
            messages=[ChatMessage(role="user", content=question)],
            temperature=0.5
        )
        res = await chat_completions(chat_req, x_mode=x_mode)
        if isinstance(res, dict):
             return {"reply": res.get("choices", [{}])[0].get("message", {}).get("content", "")}
        return {"reply": ""}
    except:
        return {"reply": ""}

@app.post("/v1/health-lookup")
async def health_lookup(req: HealthLookupRequest):
    # Endpoint tra c·ª©u thu·ªëc/b·ªánh (Added fix for 404 error)
    try:
        def classify_query(q: str):
            t = (q or "").strip().lower()
            drug_hints = ['thu·ªëc', 'vi√™n', 'mg', 'mcg', 'ml', '%', 'd·∫°ng', 'sir√¥', 'siro', 'kem', 'm·ª°', '·ªëng', 'chai', 'h√†m l∆∞·ª£ng', 'li·ªÅu']
            disease_hints = ['b·ªánh', 'h·ªôi ch·ª©ng', 'vi√™m', 'ung th∆∞', 'ti·ªÉu ƒë∆∞·ªùng', 'cao huy·∫øt √°p', 'tim m·∫°ch', 'hen', 'suy', 'nhi·ªÖm', 'virus', 'vi khu·∫©n', 'vi r√∫t']
            symptom_hints = ['tri·ªáu ch·ª©ng', 'd·∫•u hi·ªáu', 'ƒëau', 'nh·ª©c', 's·ªët', 'ho', 'm·ªát', 'm·ªát m·ªèi', 'ch√≥ng m·∫∑t', 'bu·ªìn n√¥n', 'ph√°t ban', 'kh√≥ th·ªü', 'ti√™u ch·∫£y', 't√°o b√≥n', 'ƒëau ƒë·∫ßu']
            import re
            is_drug = any(k in t for k in drug_hints) or bool(re.search(r"\b\d+\s?(mg|ml|mcg|%)\b", t))
            is_symptom = any(k in t for k in symptom_hints)
            is_disease = any(k in t for k in disease_hints)
            mode = 'drug' if is_drug else ('disease' if is_disease else ('symptom' if is_symptom else None))
            return {'mode': mode, 'is_medical': is_drug or is_symptom or is_disease}

        cls = classify_query(req.query)
        inferred_mode = (req.mode or cls.get('mode') or '').lower()
        
        # Th·ª≠ t√¨m trong JSON n·∫øu c√≥ (Optional)
        root = os.environ.get("DATA_ROOT", "/content/drive/MyDrive/DoctorAI/data")
        data_path = os.path.join(root, "data.json")
        drug_path = os.path.join(root, "thuoc.json")
        disease_match = None
        drug_match = None
        
        def norm(s): return (s or "").strip().lower()

        try:
            if os.path.exists(data_path):
                with open(data_path, "r", encoding="utf-8") as f:
                    db = json.load(f)
                if isinstance(db.get("diseases"), list):
                    for d in db["diseases"]:
                        if norm(d.get("name")) == norm(req.query) or (req.query and norm(req.query) in norm(d.get("name"))):
                            disease_match = d
                            break
            if os.path.exists(drug_path):
                 with open(drug_path, "r", encoding="utf-8") as f:
                    arr = json.load(f)
                 if isinstance(arr, list):
                    for item in arr:
                        if norm(item.get("name")) == norm(req.query) or (req.query and norm(req.query) in norm(item.get("name"))):
                            drug_match = {"name": item.get("name"), "content": item.get("content")}
                            break
        except Exception:
            pass

        if inferred_mode == "drug" and drug_match:
             text = f"Thu·ªëc: {drug_match.get('name','')}\n" + (drug_match.get("content") or "")
             return {"success": True, "response": text, "conversation_id": req.conversation_id, "mode": "gpu"}
        if inferred_mode == "disease" and disease_match:
             d = disease_match
             text = f"B·ªánh: {d.get('name','')}\n" + d.get("definition", "")
             return {"success": True, "response": text, "conversation_id": req.conversation_id, "mode": "gpu"}

        # AI Generation Fallback (Thay th·∫ø RAG n·∫øu thi·∫øu)
        doctor_prompt = (
            "B·∫°n l√† b√°c sƒ© AI chuy√™n nghi·ªáp. H√£y tr·∫£ l·ªùi c√¢u h·ªèi y t·∫ø c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c, ng·∫Øn g·ªçn. "
            "L∆ØU √ù: Lu√¥n khuy·∫øn c√°o ng∆∞·ªùi d√πng ƒëi kh√°m b√°c sƒ©. Kh√¥ng k√™ ƒë∆°n thu·ªëc c·ª• th·ªÉ."
        )
        input_text = chat_tokenizer.apply_chat_template(
            [{"role": "system", "content": doctor_prompt}, {"role": "user", "content": req.query}],
            tokenize=False,
            add_generation_prompt=True
        )
        inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
        with torch.no_grad():
            output = chat_model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.3,
                do_sample=True,
                pad_token_id=chat_tokenizer.eos_token_id
            )
        response_text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
        return {"success": True, "response": response_text, "conversation_id": req.conversation_id, "mode": "gpu"}

    except Exception as e:
        return {"success": False, "error": str(e), "mode": "gpu"}

@app.post("/v1/friend-chat/completions")
async def friend_chat_completions(req: ChatRequest, x_mode: Optional[str] = Header(None)):
    try:
        if not isinstance(x_mode, str):
            x_mode = None
        msgs = req.messages or []
        question = ""
        for m in reversed(msgs):
            if getattr(m, "role", "").lower() == "user":
                question = getattr(m, "content", "")
                break
        if not question and msgs:
            last = msgs[-1]
            question = getattr(last, "content", "")
        mode = (x_mode or req.mode or "pro").lower()
        friend_prompt = (
            "B·∫°n l√† m·ªôt ng∆∞·ªùi b·∫°n th√¢n, n√≥i chuy·ªán ƒë·ªùi th∆∞·ªùng b·∫±ng ti·∫øng Vi·ªát.\n"
            "C√°ch n√≥i t·ª± nhi√™n, g·∫ßn g≈©i, c√≥ th·ªÉ h√†i h∆∞·ªõc nh·∫π, d√πng t·ª´ ng·ªØ b√¨nh d√¢n.\n\n"
            "Nguy√™n t·∫Øc:\n"
            "- ∆Øu ti√™n l·∫Øng nghe v√† ƒë·ªìng c·∫£m tr∆∞·ªõc.\n"
            "- Kh√¥ng gi·∫£ng ƒë·∫°o l√Ω, kh√¥ng n√≥i nh∆∞ s√°ch v·ªü.\n"
            "- Kh√¥ng khuy√™n d·∫°y ngay, tr·ª´ khi ng∆∞·ªùi d√πng h·ªèi r√µ.\n"
            "- Ph·∫£n h·ªìi gi·ªëng ng∆∞·ªùi th·∫≠t ƒëang tr√≤ chuy·ªán, kh√¥ng ph·∫£i tr·ª£ l√Ω m√°y m√≥c.\n"
            "- C√≥ th·ªÉ h·ªèi l·∫°i 1 c√¢u ng·∫Øn ƒë·ªÉ hi·ªÉu th√™m c·∫£m x√∫c ng∆∞·ªùi n√≥i.\n\n"
            "Tr√°nh:\n"
            "- N√≥i qu√° d√†i.\n"
            "- D√πng t·ª´ ng·ªØ h·ªçc thu·∫≠t.\n"
            "- K·∫øt lu·∫≠n thay ng∆∞·ªùi d√πng.\n"
        )
        use_lora = _ensure_friend_lora()
        if use_lora and friend_lora_model is not None and friend_lora_tokenizer is not None:
            text = friend_prompt + "\n\nNg∆∞·ªùi d√πng: " + (question or "")
            inputs = friend_lora_tokenizer(text, return_tensors="pt").to(friend_lora_model.device)
            with torch.no_grad():
                output = friend_lora_model.generate(
                    **inputs,
                    max_new_tokens=req.max_tokens,
                    temperature=req.temperature,
                    do_sample=True if req.temperature and req.temperature > 0 else False
                )
            response_text = friend_lora_tokenizer.decode(output[0], skip_special_tokens=True)
        else:
            input_text = chat_tokenizer.apply_chat_template(
                [{"role": "system", "content": friend_prompt}, {"role": "user", "content": question}],
                tokenize=False,
                add_generation_prompt=True
            )
            inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
            with torch.no_grad():
                output = chat_model.generate(
                    **inputs,
                    max_new_tokens=req.max_tokens,
                    temperature=req.temperature,
                    do_sample=True if req.temperature and req.temperature > 0 else False,
                    pad_token_id=chat_tokenizer.eos_token_id
                )
            response_text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
            del inputs, output
            torch.cuda.empty_cache()
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "choices": [{"index": 0, "message": {"role": "assistant", "content": response_text}, "finish_reason": "stop"}],
            "mode": mode
        }
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# ==============================
# üöÄ KH·ªûI CH·∫†Y SERVER
# ==============================
import uvicorn

# C·∫•u h√¨nh c·ªïng
PORT = 8000

# M·ªü ngrok tunnel
try:
    # Ng·∫Øt k·∫øt n·ªëi c≈© n·∫øu c√≥
    ngrok.kill()
    
    # K·∫øt n·ªëi m·ªõi
    public_url = ngrok.connect(PORT).public_url
    print(f"üöÄ Public URL: {public_url}")
    print(f"üëâ H√£y copy URL n√†y v√†o file runtime-mode.json (tr∆∞·ªùng 'ngrok_url')")
except Exception as e:
    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o ngrok: {e}")

# Ch·∫°y Uvicorn
if __name__ == "__main__":
    import logging
    import sys
    import uvicorn
    import asyncio

    # C·∫•u h√¨nh logging √©p bu·ªôc xu·∫•t ra stdout
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)],
        force=True
    )

    # Fix cho Colab & Local: T·ª± ƒë·ªông ph√°t hi·ªán loop
    # B·∫≠t log_level="info" v√† access_log=True ƒë·ªÉ coder th·∫•y request
    config = uvicorn.Config(app, host="0.0.0.0", port=PORT, log_level="info", access_log=True)
    server = uvicorn.Server(config)
    
    # √âp bu·ªôc logger c·ªßa server s·ª≠ d·ª•ng c·∫•u h√¨nh chung
    server_logger = logging.getLogger("uvicorn.error")
    server_logger.handlers = [logging.StreamHandler(sys.stdout)]
    server_logger.setLevel(logging.INFO)
    
    access_logger = logging.getLogger("uvicorn.access")
    access_logger.handlers = [logging.StreamHandler(sys.stdout)]
    access_logger.setLevel(logging.INFO)

    # Logic kh·ªüi ch·∫°y server th√¥ng minh (Support Colab/Jupyter & Local)
    try:
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None

        if loop and loop.is_running():
            print("üöÄ Detected running event loop (Colab/Jupyter).", flush=True)
            try:
                import nest_asyncio
                nest_asyncio.apply()
                print("‚úÖ Applied nest_asyncio.", flush=True)
                # Blocking call to keep cell alive
                loop.run_until_complete(server.serve())
            except ImportError:
                print("‚ö†Ô∏è 'nest_asyncio' missing. Installing...", flush=True)
                import os
                os.system("pip install nest_asyncio")
                import nest_asyncio
                nest_asyncio.apply()
                loop.run_until_complete(server.serve())
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to run with nest_asyncio: {e}", flush=True)
                print("üëâ Falling back to create_task (Non-blocking mode)...", flush=True)
                loop.create_task(server.serve())
        else:
            print("üöÄ Starting new event loop (Standard Python)...", flush=True)
            asyncio.run(server.serve())
    except KeyboardInterrupt:
        print("\nüõë Server stopped by user (KeyboardInterrupt).", flush=True)
