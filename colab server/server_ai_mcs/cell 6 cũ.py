# @title 6. Kh·ªüi ch·∫°y Server API (VRAM Safe Mode)
from fastapi import FastAPI, File, UploadFile, Form, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional
import nest_asyncio
from pyngrok import ngrok
from PIL import Image
import librosa
import soundfile as sf
import numpy as np
from gtts import gTTS
import threading
import time
import socket
import uuid
import gc
import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel
import torchvision.transforms as T
from torchvision.transforms.functional import InterpolationMode
from peft import PeftModel
from fastapi import Body
import re
import base64
from io import BytesIO
import asyncio

try:
    import pypdf
except ImportError:
    pypdf = None

try:
    import docx
except ImportError:
    docx = None

# Patch async cho Colab
try:
    nest_asyncio.apply()
except Exception:
    pass

app = FastAPI(title="Medical Consultation GPU API", version="2.1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

ocr_engine = None
ocr_last_error = None
ocr_backend = None

def _ensure_ocr():
    global ocr_engine, ocr_last_error, ocr_backend
    if ocr_engine is not None:
        return True
    try:
        import sys
        if 'paddleocr' in sys.modules:
            PaddleOCR = sys.modules['paddleocr'].PaddleOCR
        else:
            from paddleocr import PaddleOCR  # lazy import to avoid PDX reinit issues
        print("‚è≥ ƒêang t·∫£i PaddleOCR cho ti·∫øng Vi·ªát...")
        try:
            ocr_engine = PaddleOCR(use_angle_cls=True, lang='vi', show_log=False)
        except Exception as e_vi:
            ocr_last_error = str(e_vi)
            try:
                ocr_engine = PaddleOCR(use_angle_cls=True, lang='latin', show_log=False)
            except Exception as e_lat:
                ocr_last_error = f"{str(e_vi)} | {str(e_lat)}"
                try:
                    ocr_engine = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)
                except Exception as e_en:
                    ocr_last_error = f"{str(e_vi)} | {str(e_lat)} | {str(e_en)}"
                    raise e_en
        print("‚úÖ PaddleOCR ƒë√£ s·∫µn s√†ng!")
        ocr_last_error = None
        ocr_backend = "paddle"
        return True
    except RuntimeError:
        try:
            import sys
            if 'paddleocr' in sys.modules:
                PaddleOCR = sys.modules['paddleocr'].PaddleOCR
                try:
                    ocr_engine = PaddleOCR(use_angle_cls=True, lang='vi', show_log=False)
                except Exception as e_vi:
                    ocr_last_error = str(e_vi)
                    try:
                        ocr_engine = PaddleOCR(use_angle_cls=True, lang='latin', show_log=False)
                    except Exception as e_lat:
                        ocr_last_error = f"{str(e_vi)} | {str(e_lat)}"
                        ocr_engine = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)
            ocr_last_error = None
            ocr_backend = "paddle"
            return True
        except Exception as e:
            ocr_last_error = str(e)
        try:
            import easyocr
            langs = ['vi', 'en']
            try:
                ocr_engine = easyocr.Reader(langs, gpu=torch.cuda.is_available())
                ocr_backend = "easyocr"
                ocr_last_error = None
                return True
            except Exception as e2:
                ocr_last_error = f"{ocr_last_error or ''} | {str(e2)}"
        except Exception as e3:
            ocr_last_error = f"{ocr_last_error or ''} | {str(e3)}"
        return False
    except Exception as e:
        ocr_last_error = str(e)
        return False

# --- C·∫¨P NH·∫¨T PH·∫¶N N√ÄY TRONG CELL 6 ---

vintern_model = None
vintern_tokenizer = None

def _ensure_vintern():
    global vintern_model, vintern_tokenizer
    if vintern_model is not None and vintern_tokenizer is not None:
        return True
    
    model_id = "5CD-AI/Vintern-3B-R-beta"
    print(f"‚è≥ ƒêang t·∫£i m√¥ h√¨nh Vision: {model_id}...")
    try:
        # T·∫£i model v·ªõi bfloat16 ƒë·ªÉ ti·∫øt ki·ªám VRAM v√† gi·ªØ ƒë·ªô ch√≠nh x√°c
        vintern_model = AutoModel.from_pretrained(
            model_id,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            use_flash_attn=False # B·∫≠t True n·∫øu GPU h·ªó tr·ª£ Flash Attention 2 (A100, H100, L4...)
        ).eval().cuda()
        
        vintern_tokenizer = AutoTokenizer.from_pretrained(
            model_id, 
            trust_remote_code=True, 
            use_fast=False
        )
        print("‚úÖ Vintern-3B ƒë√£ s·∫µn s√†ng!")
        return True
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i Vintern-3B: {e}")
        vintern_model = None
        vintern_tokenizer = None
        return False



IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def _build_transform(input_size: int):
    return T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
    ])

def _find_closest_aspect_ratio(aspect_ratio: float, target_ratios: list, width: int, height: int, image_size: int):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def _dynamic_preprocess(image: Image.Image, min_num: int = 1, max_num: int = 12, image_size: int = 448, use_thumbnail: bool = False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height
    target_ratios = set((i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
    target_aspect_ratio = _find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = ((i % (target_width // image_size)) * image_size, (i // (target_width // image_size)) * image_size, ((i % (target_width // image_size)) + 1) * image_size, ((i // (target_width // image_size)) + 1) * image_size)
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
        return processed_images

def _perform_ocr(image: Image.Image) -> str:
    try:
        if ocr_backend == "paddle":
            import numpy as _np
            result = ocr_engine.ocr(_np.array(image), cls=True)
            if result and len(result) > 0 and isinstance(result[0], list):
                return "\n".join([line[1][0] for line in result[0] if isinstance(line, list) and len(line) > 1])
            return ""
        if ocr_backend == "easyocr":
            import numpy as _np
            lines = ocr_engine.readtext(_np.array(image), detail=1)
            texts = []
            for item in lines:
                try:
                    t = item[1]
                    if isinstance(t, str) and t.strip():
                        texts.append(t.strip())
                except Exception:
                    pass
            return "\n".join(texts)
        return ""
    except Exception:
        return ""

def _pixels_from_image(image: Image.Image, input_size: int = 448, max_num: int = 6):
    transform = _build_transform(input_size=input_size)
    images = _dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(img) for img in images]
    return torch.stack(pixel_values)

# ==============================
# üî∂ DATA MODELS
# ==============================
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: Optional[str] = "llama-3.2-3b"
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    mode: Optional[str] = "pro"

class VisionMultiRequest(BaseModel):
    text: str
    images_base64: List[str]
    temperature: Optional[float] = 0.2
    max_tokens: Optional[int] = 256
    model_id: Optional[str] = None

class DocumentChatRequest(BaseModel):
    text: str
    doc_base64: str
    doc_name: str
    model: Optional[str] = "flash"

class VisionChatResponse(BaseModel):
    success: bool
    response: Optional[str] = None
    error: Optional[str] = None

class TTSRequest(BaseModel):
    text: str
    lang: Optional[str] = "vi"
class AutoTitleRequest(BaseModel):
    messages: Optional[List[ChatMessage]] = None
    user_text: Optional[str] = None
    assistant_text: Optional[str] = None
    max_tokens: Optional[int] = 24
class HealthLookupRequest(BaseModel):
    query: str
    mode: Optional[str] = None
    user_id: Optional[str] = None
    conversation_id: Optional[str] = None

FRIEND_LORA_REPO = os.environ.get("FRIEND_LORA_REPO", "An-CNT/doctorai-tamly-lora-v2-final")
FRIEND_BASE_MODEL = os.environ.get("FRIEND_BASE_MODEL", "unsloth/Llama-3.2-3B-Instruct")
_friend_bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)
friend_lora_tokenizer = None
friend_lora_model = None
def _ensure_friend_lora():
    global friend_lora_tokenizer, friend_lora_model
    if friend_lora_tokenizer is not None and friend_lora_model is not None:
        return True
    try:
        base_model = AutoModelForCausalLM.from_pretrained(FRIEND_BASE_MODEL, quantization_config=_friend_bnb_config, device_map="auto")
        friend_lora_model = PeftModel.from_pretrained(base_model, FRIEND_LORA_REPO)
        friend_lora_model.eval()
        friend_lora_tokenizer = AutoTokenizer.from_pretrained(FRIEND_LORA_REPO, use_fast=True)
        return True
    except Exception:
        try:
            base_model = AutoModelForCausalLM.from_pretrained(FRIEND_BASE_MODEL, torch_dtype=torch.bfloat16, device_map="auto")
            friend_lora_model = PeftModel.from_pretrained(base_model, FRIEND_LORA_REPO)
            friend_lora_model.eval()
            friend_lora_tokenizer = AutoTokenizer.from_pretrained(FRIEND_LORA_REPO, use_fast=True)
            return True
        except Exception:
            return False

def _extract_text_from_doc(doc_base64: str, doc_name: str) -> str:
    import base64
    import io
    
    try:
        decoded = base64.b64decode(doc_base64)
        file_stream = io.BytesIO(decoded)
        ext = doc_name.split('.')[-1].lower()
        
        text = ""
        if ext == 'pdf':
            if pypdf:
                reader = pypdf.PdfReader(file_stream)
                for page in reader.pages:
                    text += page.extract_text() + "\n"
            else:
                return "Error: pypdf library not found."
        elif ext in ['docx', 'doc']:
            if docx:
                doc = docx.Document(file_stream)
                for para in doc.paragraphs:
                    text += para.text + "\n"
            else:
                return "Error: python-docx library not found."
        else:
            try:
                text = decoded.decode('utf-8')
            except:
                return "Error: Unsupported document format."
                
        return text.strip()
    except Exception as e:
        return f"Error extracting text: {str(e)}"

@app.post("/v1/document-chat")
async def document_chat(req: DocumentChatRequest):
    if not req.doc_base64 or not req.text:
        return VisionChatResponse(success=False, error="doc_base64 and text are required")
        
    # Extract text from document
    doc_text = _extract_text_from_doc(req.doc_base64, req.doc_name)
    if doc_text.startswith("Error:"):
        return VisionChatResponse(success=False, error=doc_text)
        
    # Construct prompt
    full_prompt = f"T√†i li·ªáu ƒë√≠nh k√®m ({req.doc_name}):\n\n{doc_text}\n\n---\n\nC√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {req.text}"
    
    # Delegate to chat logic
    try:
        chat_req = ChatRequest(
            model=req.model,
            messages=[
                ChatMessage(role="system", content="B·∫°n l√† tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p."),
                ChatMessage(role="user", content=full_prompt)
            ]
        )
        
        # Reuse chat logic directly since we are on the same server
        response_dict = await chat_completions(chat_req)
        
        if isinstance(response_dict, dict):
             content = response_dict.get("choices", [{}])[0].get("message", {}).get("content", "")
             return VisionChatResponse(success=True, response=content)
        else:
            # Handle if chat_completions returns JSONResponse
            import json
            body = json.loads(response_dict.body)
            if "error" in body:
                return VisionChatResponse(success=False, error=body["error"])
            content = body.get("choices", [{}])[0].get("message", {}).get("content", "")
            return VisionChatResponse(success=True, response=content)
            
    except Exception as e:
        return VisionChatResponse(success=False, error=str(e))

# ==============================
# üî∑ 1. CHAT API
# ==============================
@app.post("/v1/chat/completions")
async def chat_completions(req: ChatRequest, x_mode: Optional[str] = Header(None)):
    try:
        msgs = req.messages or []
        question = ""
        for m in reversed(msgs):
            if m.role.lower() == "user":
                question = m.content
                break
        if not question and msgs:
            question = msgs[-1].content
        try:
            if question:
                print(f"[USER] { question }")
        except Exception:
            pass
        mode = (x_mode or req.mode or "pro").lower()
        classify_prompt = "tr·∫£ l·ªùi ng·∫Øn g·ªçn l√† c√≥ hay kh√¥ng v√† kh√¥ng gi·∫£i th√≠ch g√¨ th√™m: c√¢u h·ªèi sau ƒë√¢y c√≥ li√™n quan y t·∫ø kh√¥ng: " + question
        cls_text_tmpl = chat_tokenizer.apply_chat_template([{"role": "user", "content": classify_prompt}], tokenize=False, add_generation_prompt=True)
        cls_inputs = chat_tokenizer(cls_text_tmpl, return_tensors="pt").to("cuda")
        with torch.no_grad():
            cls_out = chat_model.generate(**cls_inputs, max_new_tokens=8, temperature=0, do_sample=False, pad_token_id=chat_tokenizer.eos_token_id)
        cls_resp = chat_tokenizer.decode(cls_out[0][cls_inputs.input_ids.shape[-1]:], skip_special_tokens=True)
        del cls_inputs, cls_out
        torch.cuda.empty_cache()
        if "kh√¥ng" in cls_resp.lower():
            response_text = "C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng li√™n quan ƒë·∫øn y t·∫ø. Vui l√≤ng ƒë·∫∑t c√¢u h·ªèi kh√°c."
            return {
                "id": f"chatcmpl-{uuid.uuid4()}",
                "object": "chat.completion",
                "created": int(time.time()),
                "choices": [{"index": 0, "message": {"role": "assistant", "content": response_text}, "finish_reason": "stop"}],
                "mode": mode
            }
        if mode == "pro":
            nodes = retriever.retrieve(question)
            context_passages = [n.node.get_content() for n in nodes]
            ranked = context_passages
            try:
                if reranker is not None:
                    query_passage_pairs = [[question, p] for p in context_passages]
                    scores = reranker.predict(query_passage_pairs)
                    ranked = [p for _, p in sorted(zip(scores, context_passages), key=lambda x: x[0], reverse=True)]
            except Exception:
                ranked = context_passages
            top_k = min(3, len(ranked))
            selected = ranked[:top_k]
            ctx = "ƒê√¢y l√† c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n" + question + "\n\n"
            ctx += "D∆∞·ªõi ƒë√¢y l√† c√°c ƒëo·∫°n th√¥ng tin li√™n quan:\n"
            for i, p in enumerate(selected):
                ctx += "\n[ƒêo·∫°n " + str(i + 1) + "]:\n" + p + "\n"
            doctor_prompt = "B·∫°n l√† b√°c s·ªπ t∆∞ v·∫•n y t·∫ø, kh√¥ng k√™ ƒë∆°n thu·ªëc, kh√¥ng ch·∫©n ƒëo√°n thay th·∫ø chuy√™n m√¥n. Tr·∫£ l·ªùi ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng, ∆∞u ti√™n an to√†n v√† khuy·∫øn c√°o g·∫∑p b√°c s·ªπ khi c·∫ßn."
            input_text = chat_tokenizer.apply_chat_template(
                [{"role": "system", "content": doctor_prompt}, {"role": "user", "content": ctx}],
                tokenize=False,
                add_generation_prompt=True
            )
            inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
            with torch.no_grad():
                output = chat_model.generate(
                    **inputs,
                    max_new_tokens=req.max_tokens,
                    temperature=req.temperature,
                    do_sample=True if req.temperature > 0 else False,
                    pad_token_id=chat_tokenizer.eos_token_id
                )
            response_text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
            del inputs, output
            torch.cuda.empty_cache()
            rag_info = {"used": True, "retrieved": len(context_passages), "selected": top_k}
        else:
            doctor_prompt = "B·∫°n l√† b√°c s·ªπ t∆∞ v·∫•n y t·∫ø, kh√¥ng k√™ ƒë∆°n thu·ªëc, kh√¥ng ch·∫©n ƒëo√°n thay th·∫ø chuy√™n m√¥n. Tr·∫£ l·ªùi ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng, ∆∞u ti√™n an to√†n v√† khuy·∫øn c√°o g·∫∑p b√°c s·ªπ khi c·∫ßn."
            input_text = chat_tokenizer.apply_chat_template(
                [{"role": "system", "content": doctor_prompt}, {"role": "user", "content": question}],
                tokenize=False,
                add_generation_prompt=True
            )
            inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
            with torch.no_grad():
                output = chat_model.generate(
                    **inputs,
                    max_new_tokens=req.max_tokens,
                    temperature=req.temperature,
                    do_sample=True if req.temperature > 0 else False,
                    pad_token_id=chat_tokenizer.eos_token_id
                )
            response_text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
            del inputs, output
            torch.cuda.empty_cache()
            rag_info = {"used": False, "retrieved": 0, "selected": 0}
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "choices": [{"index": 0, "message": {"role": "assistant", "content": response_text}, "finish_reason": "stop"}],
            "mode": mode,
            "rag": rag_info
        }
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/v1/chat")
async def chat_simple(req: dict, x_mode: Optional[str] = Header(None)):
    # Proxy ƒë∆°n gi·∫£n cho chat
    msgs = req.get("messages", [])
    if not msgs: return {"reply": ""}

    question = ""
    for m in reversed(msgs):
        if m.get("role", "").lower() == "user":
            question = m.get("content", "")
            break
    if not question and msgs:
        question = msgs[-1].get("content", "")
    try:
        if question:
            print(f"[USER] { question }")
    except Exception:
        pass
    mode = (x_mode or req.get("mode") or "pro").lower()
    classify_prompt = "tr·∫£ l·ªùi ng·∫Øn g·ªçn l√† c√≥ hay kh√¥ng v√† kh√¥ng gi·∫£i th√≠ch g√¨ th√™m: c√¢u h·ªèi sau ƒë√¢y c√≥ li√™n quan y t·∫ø kh√¥ng: " + question
    cls_text_tmpl = chat_tokenizer.apply_chat_template([{"role": "user", "content": classify_prompt}], tokenize=False, add_generation_prompt=True)
    cls_inputs = chat_tokenizer(cls_text_tmpl, return_tensors="pt").to("cuda")
    with torch.no_grad():
        cls_out = chat_model.generate(**cls_inputs, max_new_tokens=8, temperature=0, do_sample=False, pad_token_id=chat_tokenizer.eos_token_id)
    cls_resp = chat_tokenizer.decode(cls_out[0][cls_inputs.input_ids.shape[-1]:], skip_special_tokens=True)
    del cls_inputs, cls_out
    torch.cuda.empty_cache()
    if "kh√¥ng" in cls_resp.lower():
        return {"reply": "C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng li√™n quan ƒë·∫øn y t·∫ø. Vui l√≤ng ƒë·∫∑t c√¢u h·ªèi kh√°c."}
    if mode == "pro":
        nodes = retriever.retrieve(question)
        context_passages = [n.node.get_content() for n in nodes]
        ranked = context_passages
        try:
            if reranker is not None:
                query_passage_pairs = [[question, p] for p in context_passages]
                scores = reranker.predict(query_passage_pairs)
                ranked = [p for _, p in sorted(zip(scores, context_passages), key=lambda x: x[0], reverse=True)]
        except Exception:
            ranked = context_passages
        top_k = min(3, len(ranked))
        selected = ranked[:top_k]
        ctx = "ƒê√¢y l√† c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n" + question + "\n\n"
        ctx += "D∆∞·ªõi ƒë√¢y l√† c√°c ƒëo·∫°n th√¥ng tin li√™n quan:\n"
        for i, p in enumerate(selected):
            ctx += "\n[ƒêo·∫°n " + str(i + 1) + "]:\n" + p + "\n"
        doctor_prompt = "B·∫°n l√† b√°c s·ªπ t∆∞ v·∫•n y t·∫ø, kh√¥ng k√™ ƒë∆°n thu·ªëc, kh√¥ng ch·∫©n ƒëo√°n thay th·∫ø chuy√™n m√¥n. Tr·∫£ l·ªùi ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng, ∆∞u ti√™n an to√†n v√† khuy·∫øn c√°o g·∫∑p b√°c s·ªπ khi c·∫ßn."
        input_text = chat_tokenizer.apply_chat_template(
            [{"role": "system", "content": doctor_prompt}, {"role": "user", "content": ctx}],
            tokenize=False,
            add_generation_prompt=True
        )
        rag_info = {"used": True, "retrieved": len(context_passages), "selected": top_k}
    else:
        doctor_prompt = "B·∫°n l√† b√°c s·ªπ t∆∞ v·∫•n y t·∫ø, kh√¥ng k√™ ƒë∆°n thu·ªëc, kh√¥ng ch·∫©n ƒëo√°n thay th·∫ø chuy√™n m√¥n. Tr·∫£ l·ªùi ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng, ∆∞u ti√™n an to√†n v√† khuy·∫øn c√°o g·∫∑p b√°c s·ªπ khi c·∫ßn."
        input_text = chat_tokenizer.apply_chat_template(
            [{"role": "system", "content": doctor_prompt}, {"role": "user", "content": question}],
            tokenize=False,
            add_generation_prompt=True
        )
        rag_info = {"used": False, "retrieved": 0, "selected": 0}
    inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
    with torch.no_grad():
        output = chat_model.generate(**inputs, max_new_tokens=256, temperature=0.7, do_sample=True, pad_token_id=chat_tokenizer.eos_token_id)
    text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
    del inputs, output
    torch.cuda.empty_cache()
    return {"reply": text, "mode": mode, "rag": rag_info}

@app.post("/v1/health-lookup")
async def health_lookup(req: HealthLookupRequest):
    def classify_query(q: str):
        t = (q or "").strip().lower()
        drug_hints = ['thu·ªëc', 'vi√™n', 'mg', 'mcg', 'ml', '%', 'd·∫°ng', 'sir√¥', 'siro', 'kem', 'm·ª°', '·ªëng', 'chai', 'h√†m l∆∞·ª£ng', 'li·ªÅu']
        disease_hints = ['b·ªánh', 'h·ªôi ch·ª©ng', 'vi√™m', 'ung th∆∞', 'ti·ªÉu ƒë∆∞·ªùng', 'cao huy·∫øt √°p', 'tim m·∫°ch', 'hen', 'suy', 'nhi·ªÖm', 'virus', 'vi khu·∫©n', 'vi r√∫t']
        symptom_hints = ['tri·ªáu ch·ª©ng', 'd·∫•u hi·ªáu', 'ƒëau', 'nh·ª©c', 's·ªët', 'ho', 'm·ªát', 'm·ªát m·ªèi', 'ch√≥ng m·∫∑t', 'bu·ªìn n√¥n', 'ph√°t ban', 'kh√≥ th·ªü', 'ti√™u ch·∫£y', 't√°o b√≥n', 'ƒëau ƒë·∫ßu']
        medical_ctx_hints = ['ch·∫©n ƒëo√°n', 'ƒëi·ªÅu tr·ªã', 'ph√≤ng ng·ª´a', 't√°c d·ª•ng ph·ª•', 'd∆∞·ª£c', 'y khoa', 'b√°c sƒ©', 'li·ªÅu d√πng']
        import re
        is_drug = any(k in t for k in drug_hints) or bool(re.search(r"\b\d+\s?(mg|ml|mcg|%)\b", t))
        is_symptom = any(k in t for k in symptom_hints)
        is_disease = any(k in t for k in disease_hints)
        looks_medical = is_drug or is_symptom or is_disease or any(k in t for k in medical_ctx_hints)
        mode = 'drug' if is_drug else ('disease' if is_disease else ('symptom' if is_symptom else None))
        return {'mode': mode, 'is_medical': looks_medical}
    cls = classify_query(req.query)
    if not cls.get('is_medical'):
        return {
            "success": True,
            "response": "C√¢u h·ªèi kh√¥ng c√≥ d·∫•u hi·ªáu li√™n quan ƒë·∫øn y t·∫ø. Vui l√≤ng nh·∫≠p t√™n b·ªánh, thu·ªëc ho·∫∑c tri·ªáu ch·ª©ng.",
            "mode": "gpu",
            "redirect_url": "/tu-van"
        }
    inferred_mode = (req.mode or cls.get('mode') or '').lower()
    root = os.environ.get("DATA_ROOT", "/content/drive/MyDrive/DoctorAI/data")
    data_path = os.path.join(root, "data.json")
    drug_path = os.path.join(root, "thuoc.json")
    def norm(s: str) -> str:
        return (s or "").strip().lower()
    disease_match = None
    drug_match = None
    try:
        if os.path.exists(data_path):
            with open(data_path, "r", encoding="utf-8") as f:
                db = json.load(f)
            if isinstance(db.get("diseases"), list):
                for d in db["diseases"]:
                    name = norm(d.get("name", ""))
                    if name == norm(req.query) or (req.query and norm(req.query) in name):
                        disease_match = d
                        break
            if isinstance(db.get("drugs"), list):
                for dr in db["drugs"]:
                    name = norm(dr.get("name", ""))
                    if name == norm(req.query) or (req.query and norm(req.query) in name):
                        drug_match = dr
                        break
        if os.path.exists(drug_path):
            try:
                with open(drug_path, "r", encoding="utf-8") as f:
                    arr = json.load(f)
                if isinstance(arr, list):
                    for item in arr:
                        name = norm(item.get("name", ""))
                        if name == norm(req.query) or (req.query and norm(req.query) in name):
                            drug_match = {"name": item.get("name", ""), "content": item.get("content", "")}
                            break
            except Exception:
                pass
    except Exception:
        pass
    if inferred_mode == "drug" and drug_match:
        text = f"Thu·ªëc: {drug_match.get('name','')}\n" + (drug_match.get("content") or "")
        return {"success": True, "response": text, "conversation_id": req.conversation_id, "mode": "gpu"}
    if inferred_mode == "disease" and disease_match:
        d = disease_match
        parts = []
        if d.get("definition"): parts.append("ƒê·ªãnh nghƒ©a: " + d["definition"])
        if d.get("causes"): parts.append("Nguy√™n nh√¢n: " + (", ".join(d["causes"]) if isinstance(d["causes"], list) else d["causes"]))
        if d.get("symptoms"): parts.append("Tri·ªáu ch·ª©ng: " + (", ".join(d["symptoms"]) if isinstance(d["symptoms"], list) else d["symptoms"]))
        if d.get("diagnosis"): parts.append("Ch·∫©n ƒëo√°n: " + d["diagnosis"])
        if d.get("treatment"): parts.append("ƒêi·ªÅu tr·ªã: " + d["treatment"])
        if d.get("warnings"): parts.append("L∆∞u √Ω: " + d["warnings"])
        text = "B·ªánh: " + (d.get("name","")) + "\n" + "\n".join(parts)
        return {"success": True, "response": text, "conversation_id": req.conversation_id, "mode": "gpu"}
    if disease_match:
        d = disease_match
        parts = []
        if d.get("definition"): parts.append("ƒê·ªãnh nghƒ©a: " + d["definition"])
        if d.get("causes"): parts.append("Nguy√™n nh√¢n: " + (", ".join(d["causes"]) if isinstance(d["causes"], list) else d["causes"]))
        if d.get("symptoms"): parts.append("Tri·ªáu ch·ª©ng: " + (", ".join(d["symptoms"]) if isinstance(d["symptoms"], list) else d["symptoms"]))
        if d.get("diagnosis"): parts.append("Ch·∫©n ƒëo√°n: " + d["diagnosis"])
        if d.get("treatment"): parts.append("ƒêi·ªÅu tr·ªã: " + d["treatment"])
        if d.get("warnings"): parts.append("L∆∞u √Ω: " + d["warnings"])
        text = "B·ªánh: " + (d.get("name","")) + "\n" + "\n".join(parts)
        return {"success": True, "response": text, "conversation_id": req.conversation_id, "mode": "gpu"}
    if drug_match:
        text = f"Thu·ªëc: {drug_match.get('name','')}\n" + (drug_match.get("content") or "")
        return {"success": True, "response": text, "conversation_id": req.conversation_id, "mode": "gpu"}
    try:
        question = req.query
        nodes = retriever.retrieve(question)
        context_passages = [n.node.get_content() for n in nodes]
        ranked = context_passages
        try:
            if reranker is not None:
                query_passage_pairs = [[question, p] for p in context_passages]
                scores = reranker.predict(query_passage_pairs)
                ranked = [p for _, p in sorted(zip(scores, context_passages), key=lambda x: x[0], reverse=True)]
        except Exception:
            ranked = context_passages
        top_k = min(3, len(ranked))
        selected = ranked[:top_k]
        ctx = "ƒê√¢y l√† c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n" + question + "\n\n"
        ctx += "D∆∞·ªõi ƒë√¢y l√† c√°c ƒëo·∫°n th√¥ng tin li√™n quan:\n"
        for i, p in enumerate(selected):
            ctx += "\n[ƒêo·∫°n " + str(i + 1) + "]:\n" + p + "\n"
        doctor_prompt = "B·∫°n l√† b√°c s·ªπ t∆∞ v·∫•n y t·∫ø, kh√¥ng k√™ ƒë∆°n thu·ªëc, kh√¥ng ch·∫©n ƒëo√°n thay th·∫ø chuy√™n m√¥n. Tr·∫£ l·ªùi ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng, ∆∞u ti√™n an to√†n v√† khuy·∫øn c√°o g·∫∑p b√°c s·ªπ khi c·∫ßn."
        input_text = chat_tokenizer.apply_chat_template(
            [{"role": "system", "content": doctor_prompt}, {"role": "user", "content": ctx}],
            tokenize=False,
            add_generation_prompt=True
        )
        inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
        with torch.no_grad():
            output = chat_model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.2,
                do_sample=True,
                pad_token_id=chat_tokenizer.eos_token_id
            )
        response_text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
        return {"success": True, "response": response_text, "conversation_id": req.conversation_id, "mode": "gpu"}
    except Exception as e:
        return {"success": False, "error": str(e), "mode": "gpu"}
# ==============================================================================
# üíé ULTIMATE ENDPOINT: FRIEND CHAT (D√ÄNH CHO DEMO H·ªòI ƒê·ªíNG)
# ==============================================================================
@app.post("/v1/friend-chat/completions")
async def friend_chat_completions(req: ChatRequest, x_mode: Optional[str] = Header(None)):
    try:
        # 1. K·ª∏ THU·∫¨T "FEW-SHOT" + "CHAIN OF THOUGHT"
        # √âp model h·ªçc theo v√≠ d·ª• c·ª• th·ªÉ ƒë·ªÉ kh√¥ng b·ªã ch·ªách h∆∞·ªõng
        friend_system_prompt = (
            "B·∫°n l√† DoctorAI - b·∫°n th√¢n c·ªët, t√≠nh c√°ch l·∫ßy l·ªôi, x∆∞ng 'Tui - B·∫°n'."
            "\nQUY T·∫ÆC C·ªêT L√ïI (SUY NGHƒ® TRONG NGO·∫∂C TR∆Ø·ªöC KHI N√ìI):"
            "\n1. X√°c ƒë·ªãnh r√µ: 'Tui' l√† Bot, 'B·∫°n' l√† User. N·∫øu User khoe ti·ªÅn, nghƒ©a l√† USER tr√∫ng, Bot ph·∫£i ƒë√≤i ƒÉn k√©."
            "\n2. N·∫øu User bu·ªìn: NGHI√äM T√öC, ƒë·ªìng c·∫£m, ch·ª≠i s·∫øp/ch·ª≠i ƒë·ªùi h·ªô n√≥. C·∫•m ƒë√πa."
            "\n3. N·∫øu User vui/r·ªß ƒëi ch∆°i: C√† kh·ªãa, ƒë√≤i bao, ƒë√≤i ƒëi ch·ªó sang ch·∫£nh ƒë·ªÉ tr√™u."
            "\n4. Tuy·ªát ƒë·ªëi kh√¥ng d√πng t·ª´ v√¥ nghƒ©a (nh∆∞ 'ch√≠p x√≠ch', 't√†o la'). D√πng ti·∫øng Vi·ªát chu·∫©n ho·∫∑c slang ph·ªï bi·∫øn."
            "\n\n*** V√ç D·ª§ M·∫™U (B·∫ÆT BU·ªòC H·ªåC THEO): ***"
            "\nUser: Tao m·ªõi b·ªã s·∫øp m·∫Øng oan, bu·ªìn qu√°."
            "\nAssistant: (Ph√¢n t√≠ch: N√≥ bu·ªìn -> ƒê·ªìng c·∫£m) Tr·ªùi ƒë·∫•t, nghe m√† t·ª©c c√°i l·ªìng ng·ª±c! S·∫øp m√†y b·ªã ƒëi√™n h·∫£? M√†y l√†m t·ªët th·∫•y m·ªì m√† ·ªïng kh√¥ng th·∫•y. Th√¥i t·ªëi ƒëi l√†m v√†i chai gi·∫£i xui, tao th∆∞∆°ng."
            "\n\nUser: √ä m·ªõi tr√∫ng s·ªë 500k, ƒëi nh·∫≠u kh√¥ng?"
            "\nAssistant: (Ph√¢n t√≠ch: N√≥ tr√∫ng s·ªë -> ƒê√≤i ƒÉn k√© -> C√† kh·ªãa s·ªë ti·ªÅn nh·ªè) 500k to qu√° ha! ƒê·ªß tr·∫£ ti·ªÅn g·ª≠i xe kh√¥ng ƒë√≥ ƒë·∫°i gia? Nh∆∞ng m√† th√¥i, c√≥ l·ªôc l√† ph·∫£i t√°n. M√†y bao tao ƒëi ƒÉn ·ªëc l·ªÅ ƒë∆∞·ªùng ƒëi, ch·ª© 500k ƒë√≤i v√†o nh√† h√†ng 5 sao r·ª≠a ch√©n tr·ª´ n·ª£ h·∫£?"
            "\n\nUser: Tao th·∫•y cu·ªôc ƒë·ªùi v√¥ nghƒ©a qu√°..."
            "\nAssistant: (Ph√¢n t√≠ch: Ti√™u c·ª±c -> Nghi√™m t√∫c) M√†y n√≥i b·∫≠y b·∫° g√¨ ƒë√≥? C√≥ chuy·ªán g√¨ k·ªÉ tao nghe. Tao ng·ªìi ƒë√¢y nghe m√†y than th·ªü n√®, c·∫•m nghƒ© linh tinh!"
            "\n\nB√¢y gi·ªù h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªõi c·ªßa ng∆∞·ªùi d√πng theo phong c√°ch tr√™n."
        )

        msgs_in = req.messages or []
        recent_msgs = msgs_in[-8:] # Gi·ªØ ng·ªØ c·∫£nh ng·∫Øn g·ªçn cho nhanh

        # Format Chat
        msgs = [{"role": "system", "content": friend_system_prompt}] + \
               [{"role": m.role, "content": m.content} for m in recent_msgs]

        # 2. TINH CH·ªàNH THAM S·ªê (AN TO√ÄN L√Ä TR√äN H·∫æT)
        mode = (x_mode or (req.mode or "flash")).lower()
        if mode == "pro":
            ok = _ensure_friend_lora()
            if not ok: mode = "flash"

        response_text = ""

        # --- MODE PRO (LORA V2) ---
        if mode == "pro" and friend_lora_model is not None:
            try:
                terminators = [
                    friend_lora_tokenizer.eos_token_id,
                    friend_lora_tokenizer.convert_tokens_to_ids("<|eot_id|>")
                ]

                input_ids = friend_lora_tokenizer.apply_chat_template(
                    msgs, 
                    add_generation_prompt=True, 
                    return_tensors="pt"
                ).to(friend_lora_model.device)
                
                with torch.no_grad():
                    output = friend_lora_model.generate(
                        input_ids,
                        max_new_tokens=256,
                        do_sample=True,
                        temperature=0.55,       # ‚ö†Ô∏è 0.55 gi√∫p bot ·ªïn ƒë·ªãnh, b·ªõt n√≥i nh·∫£m nh∆∞ng v·∫´n t·ª± nhi√™n
                        top_p=0.9,
                        repetition_penalty=1.15, # Ph·∫°t l·∫∑p t·ª´ m·∫°nh h∆°n
                        eos_token_id=terminators,
                        pad_token_id=friend_lora_tokenizer.eos_token_id
                    )
                
                raw_response = friend_lora_tokenizer.decode(
                    output[0][input_ids.shape[-1]:], 
                    skip_special_tokens=True
                )
                
                # 3. H·∫¨U X·ª¨ L√ù (GI·∫§U SUY NGHƒ®)
                # K·ªπ thu·∫≠t quan tr·ªçng: C·∫Øt b·ªè ph·∫ßn (Ph√¢n t√≠ch: ...) ƒë·ªÉ User ch·ªâ th·∫•y c√¢u tr·∫£ l·ªùi x·ªãn
                import re
                
                # N·∫øu model l·ª° in l·∫°i prompt 'Assistant:', c·∫Øt b·ªè
                if "Assistant:" in raw_response:
                    raw_response = raw_response.split("Assistant:")[-1]
                
                final_response = raw_response
                # X√≥a ph·∫ßn n·ªôi t√¢m trong ngo·∫∑c (...)
                final_response = re.sub(r'\(Ph√¢n t√≠ch:.*?\)', '', final_response, flags=re.DOTALL)
                # X√≥a kho·∫£ng tr·∫Øng th·ª´a
                response_text = final_response.strip()

                # Fallback: N·∫øu x√≥a xong m√† r·ªóng (do l·ªói), tr·∫£ v·ªÅ nguy√™n g·ªëc
                if not response_text:
                    response_text = raw_response.strip()

                del input_ids, output
                torch.cuda.empty_cache()

            except Exception as e:
                print(f"Error Pro: {e}")
                mode = "flash"

        # --- MODE FLASH (FALLBACK) ---
        if mode != "pro":
            # X·ª≠ l√Ω t∆∞∆°ng t·ª± cho base model...
            input_text = chat_tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
            inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
            try:
                with torch.no_grad():
                    output = chat_model.generate(
                        **inputs,
                        max_new_tokens=200,
                        temperature=0.6,
                        repetition_penalty=1.1,
                        pad_token_id=chat_tokenizer.eos_token_id
                    )
                response_text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
                # C≈©ng x√≥a suy nghƒ© n·∫øu c√≥
                response_text = re.sub(r'\(Ph√¢n t√≠ch:.*?\)', '', response_text, flags=re.DOTALL).strip()
            except: pass

        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "choices": [{"index": 0, "message": {"role": "assistant", "content": response_text}, "finish_reason": "stop"}],
            "mode": mode
        }
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})





@app.post("/v1/rag/qa")
async def rag_qa(question: str = Body(..., embed=True), max_tokens: int = Body(256), temperature: float = Body(0.3)):
    try:
        q = (question or "").strip()
        if not q:
            return JSONResponse(status_code=400, content={"error": "missing_question"})
        classify_prompt = "tr·∫£ l·ªùi ng·∫Øn g·ªçn l√† c√≥ hay kh√¥ng v√† kh√¥ng gi·∫£i th√≠ch g√¨ th√™m: c√¢u h·ªèi sau ƒë√¢y c√≥ li√™n quan y t·∫ø kh√¥ng: " + q
        cls_text_tmpl = chat_tokenizer.apply_chat_template([{"role": "user", "content": classify_prompt}], tokenize=False, add_generation_prompt=True)
        cls_inputs = chat_tokenizer(cls_text_tmpl, return_tensors="pt").to("cuda")
        with torch.no_grad():
            cls_out = chat_model.generate(**cls_inputs, max_new_tokens=8, temperature=0, do_sample=False, pad_token_id=chat_tokenizer.eos_token_id)
        cls_resp = chat_tokenizer.decode(cls_out[0][cls_inputs.input_ids.shape[-1]:], skip_special_tokens=True)
        del cls_inputs, cls_out
        torch.cuda.empty_cache()
        if "kh√¥ng" in cls_resp.lower():
            return {"response": "C√¢u h·ªèi c·ªßa b·∫°n kh√¥ng li√™n quan ƒë·∫øn y t·∫ø. Vui l√≤ng ƒë·∫∑t c√¢u h·ªèi kh√°c.", "mode": "pro", "classified": "non-medical"}
        nodes = retriever.retrieve(q)
        context_passages = [n.node.get_content() for n in nodes]
        ranked = context_passages
        try:
            if reranker is not None:
                query_passage_pairs = [[q, p] for p in context_passages]
                scores = reranker.predict(query_passage_pairs)
                ranked = [p for _, p in sorted(zip(scores, context_passages), key=lambda x: x[0], reverse=True)]
        except Exception:
            ranked = context_passages
        top_k = min(3, len(ranked))
        selected = ranked[:top_k]
        ctx = "ƒê√¢y l√† c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n" + q + "\n\n"
        ctx += "D∆∞·ªõi ƒë√¢y l√† c√°c ƒëo·∫°n th√¥ng tin li√™n quan:\n"
        for i, p in enumerate(selected):
            ctx += "\n[ƒêo·∫°n " + str(i + 1) + "]:\n" + p + "\n"
        doctor_prompt = "B·∫°n l√† b√°c s·ªπ t∆∞ v·∫•n y t·∫ø, kh√¥ng k√™ ƒë∆°n thu·ªëc, kh√¥ng ch·∫©n ƒëo√°n thay th·∫ø chuy√™n m√¥n. Tr·∫£ l·ªùi ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng, ∆∞u ti√™n an to√†n v√† khuy·∫øn c√°o g·∫∑p b√°c s·ªπ khi c·∫ßn."
        input_text = chat_tokenizer.apply_chat_template(
            [{"role": "system", "content": doctor_prompt}, {"role": "user", "content": ctx}],
            tokenize=False,
            add_generation_prompt=True
        )
        inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
        with torch.no_grad():
            output = chat_model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=True if temperature > 0 else False,
                pad_token_id=chat_tokenizer.eos_token_id
            )
        response_text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
        del inputs, output
        torch.cuda.empty_cache()
        return {"response": response_text, "mode": "pro", "top_k": top_k}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})
@app.post("/v1/auto-title")
async def auto_title(req: AutoTitleRequest):
    try:
        user_text = req.user_text or ""
        assistant_text = req.assistant_text or ""
        if (not user_text or not assistant_text) and req.messages:
            for m in reversed(req.messages):
                if m.role.lower() == "assistant" and not assistant_text:
                    assistant_text = m.content
                elif m.role.lower() == "user" and not user_text:
                    user_text = m.content
                if user_text and assistant_text:
                    break
        if not user_text:
            user_text = ""
        if not assistant_text:
            assistant_text = ""
        prompt = (
            "B·∫°n l√† h·ªá th·ªëng ƒë·∫∑t ti√™u ƒë·ªÅ h·ªôi tho·∫°i. H√£y t·∫°o m·ªôt ti√™u ƒë·ªÅ ti·∫øng Vi·ªát ng·∫Øn g·ªçn (4‚Äì8 t·ª´) ph·∫£n √°nh ƒë√∫ng tr·ªçng t√¢m.\n"
            "- Kh√¥ng d√πng markdown, k√Ω t·ª± ƒë·∫∑c bi·ªát, ho·∫∑c ngo·∫∑c k√©p\n"
            "- Kh√¥ng l·∫∑p t·ª´, ƒë·ªãnh d·∫°ng r√µ r√†ng, d·ªÖ hi·ªÉu\n"
            "- N·∫øu c√≥ s·ªë l∆∞·ª£ng ph∆∞∆°ng ph√°p/m·ª•c, gi·ªØ s·ªë trong ti√™u ƒë·ªÅ\n"
            "- V√≠ d·ª• phong c√°ch: 'Gi·∫£m lo √¢u: 8 ph∆∞∆°ng ph√°p hi·ªáu qu·∫£', 'Ch·∫ø ƒë·ªô ƒÉn gi·∫£m c√¢n an to√†n'\n"
            "D·ªØ li·ªáu h·ªôi tho·∫°i:\nNg∆∞·ªùi d√πng: " + user_text + "\nTr·ª£ l√Ω: " + assistant_text
        )
        input_text = chat_tokenizer.apply_chat_template(
            [{"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω ƒë·∫∑t ti√™u ƒë·ªÅ."}, {"role": "user", "content": prompt}],
            tokenize=False,
            add_generation_prompt=True
        )
        inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
        with torch.no_grad():
            output = chat_model.generate(
                **inputs,
                max_new_tokens=req.max_tokens or 24,
                temperature=0.2,
                do_sample=False,
                pad_token_id=chat_tokenizer.eos_token_id
            )
        title = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True).strip()
        try:
            import re
            title = title.strip()
            title = re.sub(r'[\r\n]+', ' ', title)
            title = re.sub(r'[*_`#]+', '', title)
            title = re.sub(r'\s+', ' ', title)
            title = title[:60]
            if not title:
                title = "H·ªôi tho·∫°i"
        except Exception:
            title = (title or "H·ªôi tho·∫°i")[:60]
        del inputs, output
        torch.cuda.empty_cache()
        return {"title": title, "mode": "gpu"}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# ==============================
# üî∂ 2. VISION API
# ==============================
# --- C·∫¨P NH·∫¨T API N√ÄY TRONG CELL 6 ---

@app.post("/v1/vision-multi")
async def vision_multi_api(req: VisionMultiRequest):
    try:
        full_response = []
        
        # 1. C·ªë g·∫Øng kh·ªüi t·∫°o Vintern-3B
        has_vintern = _ensure_vintern()
        
        # X√°c ƒë·ªãnh Prompt h·ªá th·ªëng d·ª±a tr√™n y√™u c·∫ßu (ƒë·ªÉ OCR t·ªët h∆°n)
        system_instruction = ""
        req_lower = (req.text or "").lower()
        if any(x in req_lower for x in ["ƒë·ªçc", "text", "ch·ªØ", "ocr", "tr√≠ch xu·∫•t", "n·ªôi dung"]):
            system_instruction = "B·∫°n l√† m·ªôt h·ªá th·ªëng OCR th√¥ng minh. H√£y tr√≠ch xu·∫•t ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß n·ªôi dung vƒÉn b·∫£n trong ·∫£nh. "
        elif "thu·ªëc" in req_lower or "b·ªánh" in req_lower:
            system_instruction = (
                "B·∫°n l√† m·ªôt b√°c sƒ©/d∆∞·ª£c sƒ© AI t·∫≠n t√¢m. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc h√¨nh ·∫£nh ƒë∆°n thu·ªëc v√† h∆∞·ªõng d·∫´n b·ªánh nh√¢n."
                "\n1. ƒê·ªçc ch√≠nh x√°c ph·∫ßn CH·∫®N ƒêO√ÅN (Diagnosis)."
                "\n2. ƒê·ªçc k·ªπ ph·∫ßn THU·ªêC: T√™n thu·ªëc, h√†m l∆∞·ª£ng, s·ªë l∆∞·ª£ng."
                "\n3. QUAN TR·ªåNG NH·∫§T: Tr√≠ch xu·∫•t h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng (S√°ng/Tr∆∞a/T·ªëi, Tr∆∞·ªõc ƒÉn/Sau ƒÉn) ƒë·ªÉ l·∫≠p l·ªãch u·ªëng thu·ªëc d·ªÖ hi·ªÉu cho b·ªánh nh√¢n."
                "\n4. ƒê·ªçc ph·∫ßn L·ªúI D·∫∂N (Note) c·ªßa b√°c sƒ©."
                "\nTr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, ph√¢n chia theo bu·ªïi trong ng√†y."
            )

        for idx, b64_str in enumerate(req.images_base64[:2]): # X·ª≠ l√Ω t·ªëi ƒëa 2 ·∫£nh
            try:
                # Decode ·∫£nh
                image_bytes = base64.b64decode(b64_str)
                image = Image.open(BytesIO(image_bytes)).convert("RGB")

                # === C√ÅCH 1: D√ôNG VINTERN-3B (∆Øu ti√™n) ===
                if has_vintern and vintern_model is not None:
                    try:
                        # Preprocess ·∫£nh (quan tr·ªçng cho Vintern)
                        pixel_values = _pixels_from_image(image, input_size=448, max_num=6).to(torch.bfloat16).cuda()
                        
                        # T·∫°o prompt chu·∫©n cho Vintern
                        # C·∫•u tr√∫c: <image>\n{system_instruction}\n{user_query}
                        question = f"<image>\n{system_instruction}{req.text}"
                        
                        # C·∫•u h√¨nh sinh vƒÉn b·∫£n
                        gen_config = {
                            "max_new_tokens": req.max_tokens or 512,
                            "do_sample": False, # False gi√∫p OCR ch√≠nh x√°c h∆°n, kh√¥ng b·ªãa
                            "num_beams": 1,
                            "repetition_penalty": 1.1
                        }
                        
                        # Generate
                        response, _ = vintern_model.chat(
                            vintern_tokenizer, 
                            pixel_values, 
                            question, 
                            gen_config, 
                            history=None, 
                            return_history=True
                        )
                        
                        full_response.append(f"[·∫¢nh {idx+1}]: {response}")
                        
                        # D·ªçn d·∫πp GPU ngay l·∫≠p t·ª©c
                        del pixel_values
                        torch.cuda.empty_cache()
                        continue # X·ª≠ l√Ω xong ·∫£nh n√†y, sang ·∫£nh ti·∫øp theo

                    except Exception as e_vintern:
                        print(f"‚ö†Ô∏è L·ªói Vintern ·∫£nh {idx+1}, chuy·ªÉn sang Fallback OCR: {e_vintern}")
                        # N·∫øu l·ªói, code s·∫Ω ch·∫°y xu·ªëng ph·∫ßn Fallback b√™n d∆∞·ªõi
                
                # === C√ÅCH 2: FALLBACK (PaddleOCR + Llama/ChatModel) ===
                # (Ch·ªâ ch·∫°y khi kh√¥ng c√≥ Vintern ho·∫∑c Vintern b·ªã l·ªói OOM)
                _ensure_ocr()
                if ocr_engine is None:
                    full_response.append(f"[·∫¢nh {idx+1}]: L·ªói - Kh√¥ng th·ªÉ t·∫£i model Vision ho·∫∑c OCR.")
                    continue

                # Ch·∫°y PaddleOCR
                result = ocr_engine.ocr(np.array(image), cls=True)
                extracted_text = ""
                if result and result[0]:
                    extracted_text = "\n".join([line[1][0] for line in result[0] if line[1][0]])
                
                if extracted_text.strip():
                    # ƒê∆∞a text OCR ƒë∆∞·ª£c v√†o model chat (Llama 3) ƒë·ªÉ t√≥m t·∫Øt/tr·∫£ l·ªùi
                    prompt = f"""
                    D∆∞·ªõi ƒë√¢y l√† n·ªôi dung vƒÉn b·∫£n ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ h√¨nh ·∫£nh:
                    ---
                    {extracted_text}
                    ---
                    D·ª±a v√†o ƒë√≥, h√£y tr·∫£ l·ªùi c√¢u h·ªèi: {req.text}
                    """
                    # G·ªçi h√†m chat n·ªôi b·ªô (gi·∫£ ƒë·ªãnh chat_model ƒë√£ load ·ªü ph·∫ßn kh√°c c·ªßa cell 6)
                    input_ids = chat_tokenizer.apply_chat_template(
                        [{"role": "user", "content": prompt}], 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    inputs = chat_tokenizer(input_ids, return_tensors="pt").to("cuda")
                    with torch.no_grad():
                        out = chat_model.generate(**inputs, max_new_tokens=req.max_tokens or 256)
                    resp_text = chat_tokenizer.decode(out[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
                    full_response.append(f"[·∫¢nh {idx+1} (OCR Mode)]: {resp_text}")
                else:
                    full_response.append(f"[·∫¢nh {idx+1}]: Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c vƒÉn b·∫£n.")

            except Exception as e:
                full_response.append(f"[L·ªói x·ª≠ l√Ω ·∫£nh {idx+1}]: {str(e)}")

        return {
            "success": True,
            "response": "\n\n".join(full_response),
            "mode": "gpu",
            "backend": "vintern-3b" if has_vintern else "ocr-fallback"
        }
        
    except Exception as e:
        return {"success": False, "error": str(e)}

# --- B·ªî SUNG CLASS N√ÄY (n·∫øu ch∆∞a c√≥) ---
class VisionChatRequest(BaseModel):
    text: str
    image_base64: str
    temperature: Optional[float] = 0.2
    max_tokens: Optional[int] = 256
    model_id: Optional[str] = None

# --- B·ªî SUNG API N√ÄY V√ÄO CELL 6 ---
@app.post("/v1/vision-chat")
async def vision_chat_api(req: VisionChatRequest):
    """
    Wrapper endpoint: Nh·∫≠n request 1 ·∫£nh v√† chuy·ªÉn ti·∫øp sang logic x·ª≠ l√Ω ƒëa ·∫£nh (vision-multi)
    ƒë·ªÉ tr√°nh l·ªói 404 t·ª´ ph√≠a client.
    """
    # Chuy·ªÉn ƒë·ªïi format t·ª´ VisionChatRequest (1 ·∫£nh) -> VisionMultiRequest (list ·∫£nh)
    multi_req = VisionMultiRequest(
        text=req.text,
        images_base64=[req.image_base64], # ƒê√≥ng g√≥i ·∫£nh ƒë∆°n v√†o list
        temperature=req.temperature,
        max_tokens=req.max_tokens,
        model_id=req.model_id
    )
    # G·ªçi tr·ª±c ti·∫øp h√†m x·ª≠ l√Ω ch√≠nh
    return await vision_multi_api(multi_req)

# ==============================
# üî∑ 3. TTS STREAMING (gTTS)
# ==============================
@app.post("/v1/tts/stream")
async def tts_stream_api(req: TTSRequest):
    async def generate_audio_stream():
        temp_filename = f"/content/tts_{uuid.uuid4().hex}.mp3"
        try:
            tts = gTTS(text=req.text, lang=req.lang)
            tts.save(temp_filename)
            chunk_size = 32 * 1024
            with open(temp_filename, "rb") as f:
                while True:
                    data = f.read(chunk_size)
                    if not data: break
                    yield data
                    await asyncio.sleep(0.01)
            if os.path.exists(temp_filename): os.remove(temp_filename)
        except Exception as e:
            # In case of error, we can't easily return JSON in an audio stream.
            # We might yield nothing or a specific error header, but for now just log/pass
            print(f"TTS Error: {e}")

    return StreamingResponse(generate_audio_stream(), media_type="audio/mpeg")

# ==============================
# üî∂ 4. STT STREAMING (Faster-Whisper)
# ==============================
@app.post("/v1/stt/stream")
async def stt_stream_api(file: UploadFile = File(...)):
    async def transcribe_stream():
        temp_filename = f"/content/upload_{uuid.uuid4().hex}.wav"
        try:
            with open(temp_filename, "wb") as f:
                f.write(await file.read())

            # Faster-whisper tr·∫£ v·ªÅ generator
            segments, info = stt_model.transcribe(temp_filename, beam_size=5, language="vi")

            for segment in segments:
                payload = json.dumps({
                    "text": segment.text,
                    "start": segment.start,
                    "end": segment.end,
                    "partial": segment.text
                })
                yield payload + "\n"
                await asyncio.sleep(0.01)

            if os.path.exists(temp_filename): os.remove(temp_filename)
        except Exception as e:
            yield json.dumps({"error": str(e)}) + "\n"

    return StreamingResponse(transcribe_stream(), media_type="application/x-ndjson")

# ==============================
# üöÄ STARTUP
# ==============================
@app.get("/gpu/metrics")
async def gpu_metrics():
    # L·∫•y th√¥ng s·ªë GPU th·∫≠t
    try:
        r = subprocess.run(["nvidia-smi", "--query-gpu=utilization.gpu,memory.used,memory.total", "--format=csv,noheader,nounits"], capture_output=True, text=True)
        u, m_used, m_total = r.stdout.strip().split(",")
        return {"gpu_utilization": float(u), "mem_used": float(m_used), "mem_total": float(m_total)}
    except:
        return {}

@app.get("/health")
async def health():
    try:
        ok_chat = chat_model is not None
        ok_tokenizer = chat_tokenizer is not None
        ok_vlm = vlm_model is not None and vlm_processor is not None
        ok_stt = stt_model is not None
        return {"status": "ok", "chat": ok_chat, "tokenizer": ok_tokenizer, "vlm": ok_vlm, "stt": ok_stt}
    except Exception:
        return {"status": "degraded"}

@app.get("/")
async def root():
    return {"status": "ok"}

print("--- KH·ªûI ƒê·ªòNG SERVER (VRAM OPTIMIZED) ---")

def find_free_port() -> int:
    s = socket.socket()
    s.bind(('', 0))
    port = s.getsockname()[1]
    s.close()
    return port

def is_port_available(p: int) -> bool:
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.bind(('0.0.0.0', p))
        s.close()
        return True
    except OSError:
        return False

def run_uvicorn():
    import uvicorn
    import asyncio
    try:
        config = uvicorn.Config(app, host="0.0.0.0", port=PORT, loop="asyncio", lifespan="on")
        server = uvicorn.Server(config)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(server.serve())
    except Exception as e:
        print("UVicorn start error:", str(e))

if not is_port_available(PORT):
    new_port = find_free_port()
    print(f"‚ö†Ô∏è Port {PORT} ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng, chuy·ªÉn sang port {new_port}")
    PORT = new_port
else:
    print(f"‚úÖ S·ª≠ d·ª•ng port {PORT}")

thread = threading.Thread(target=run_uvicorn, daemon=True)
thread.start()

time.sleep(1.0)

if USE_NGROK and NGROK_AUTH_TOKEN:
    try:
        ngrok.set_auth_token(NGROK_AUTH_TOKEN)
        ngrok.kill()
        public_url = ngrok.connect(PORT)
        print(f"‚úÖ Public URL: {public_url.public_url}")
        print(f"üîó Health: {public_url.public_url}/health")
    except Exception as e:
        print(f"‚ùå Ngrok error: {e}")
else:
    print("‚ö†Ô∏è Ch·∫°y Localhost")

try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    pass
