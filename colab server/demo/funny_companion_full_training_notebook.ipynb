{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b408c4ac78b049e3bd87c6fee4c6d284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_729243f1f41d4c4d8dd4e68e0308b3bd",
              "IPY_MODEL_0474e606404845d193cd66867b89d1a9",
              "IPY_MODEL_9dddf8193d4a47d4acf0be028b48c166"
            ],
            "layout": "IPY_MODEL_7327fd1271d84770bde9515f09d5f891"
          }
        },
        "729243f1f41d4c4d8dd4e68e0308b3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_066f8aa6c344483c91e7542929c5a2cf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_305914faefea4208a034e278cda69711",
            "value": "Map:‚Äá100%"
          }
        },
        "0474e606404845d193cd66867b89d1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a24a5eb7c0bd435fb1693c1d8077ae5f",
            "max": 959,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_393b505e361a4151b44786b0098b0d96",
            "value": 959
          }
        },
        "9dddf8193d4a47d4acf0be028b48c166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06b91ee7220e45a68e6dd0215f8eb0da",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d5b3da7f41b04731a740acc8c569943d",
            "value": "‚Äá959/959‚Äá[00:09&lt;00:00,‚Äá30.42‚Äáexamples/s]"
          }
        },
        "7327fd1271d84770bde9515f09d5f891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "066f8aa6c344483c91e7542929c5a2cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "305914faefea4208a034e278cda69711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a24a5eb7c0bd435fb1693c1d8077ae5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "393b505e361a4151b44786b0098b0d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06b91ee7220e45a68e6dd0215f8eb0da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5b3da7f41b04731a740acc8c569943d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 0 ‚Äî C√†i th∆∞ vi·ªán"
      ],
      "metadata": {
        "id": "KHnDHXu3x1UJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5gd0lYwxvTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ce6fe1-b4cf-433b-a3c0-ba3635b8424a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate peft datasets bitsandbytes trl sentencepiece\n",
        "!pip install -q huggingface_hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 1 ‚Äî Import th∆∞ vi·ªán"
      ],
      "metadata": {
        "id": "Zh6YKl0vx7GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from glob import glob\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "from huggingface_hub import notebook_login\n"
      ],
      "metadata": {
        "id": "JIy_gktdx-ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 2 ‚Äî K·∫øt n·ªëi Google Drive"
      ],
      "metadata": {
        "id": "DDohKNPoyATo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR-Vhrdrf1Ay",
        "outputId": "471c7acd-4269-4cbe-fade-0ad68c752c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RAW_DIR = \"/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data\"\n",
        "CLEAN_DIR = \"/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly\"\n",
        "os.makedirs(CLEAN_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "WJW991cDyATo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # CELL 3 ‚Äî Load c√°c file 6 c·∫∑p t·ª´ raw_data/ v√† merge th√†nh 1 dataset_cleaned.jsonl\n",
        "\n",
        "import os\n",
        "import json\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "\n",
        "# C√°c file d·∫°ng topic_01_raw_6pairs.jsonl, topic_02_raw_6pairs.jsonl...\n",
        "files = sorted(glob(f\"{RAW_DIR}/topic_*_raw_6pairs.jsonl\"))\n",
        "\n",
        "print(\"üìÅ Found files:\", files)\n",
        "\n",
        "all_items = []\n",
        "\n",
        "for f in files:\n",
        "    print(\"üëâ Reading:\", f)\n",
        "    with open(f, \"r\", encoding=\"utf-8\") as fr:\n",
        "        for line in fr:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "                # ki·ªÉm tra format chu·∫©n\n",
        "                if \"messages\" in obj and isinstance(obj[\"messages\"], list):\n",
        "                    all_items.append(obj)\n",
        "            except Exception as e:\n",
        "                print(\"‚ùå Parse error:\", f, e)\n",
        "                continue\n",
        "\n",
        "print(\"üìä T·ªïng s·ªë h·ªôi tho·∫°i h·ª£p l·ªá:\", len(all_items))\n",
        "\n",
        "# L∆∞u file cleaned h·ª£p nh·∫•t\n",
        "clean_file = os.path.join(CLEAN_DIR, \"dataset_cleaned.jsonl\")\n",
        "with open(clean_file, \"w\", encoding=\"utf-8\") as fw:\n",
        "    for item in all_items:\n",
        "        fw.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"‚úî ƒê√£ t·∫°o file cleaned:\", clean_file)\n"
      ],
      "metadata": {
        "id": "QenDIqw1yAiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54081488-0805-4f03-fea3-cccf9fa38564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Found files: ['/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_01_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_02_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_03_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_04_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_05_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_06_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_07_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_08_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_09_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_10_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_11_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_12_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_13_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_14_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_15_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_16_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_17_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_18_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_19_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_20_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_21_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_22_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_23_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_24_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_25_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_26_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_27_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_28_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_29_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_30_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_31_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_32_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_33_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_34_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_35_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_36_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_37_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_38_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_39_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_40_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_41_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_42_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_43_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_44_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_45_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_46_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_47_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_48_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_49_raw_6pairs.jsonl', '/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_50_raw_6pairs.jsonl']\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_01_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_02_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_03_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_04_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_05_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_06_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_07_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_08_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_09_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_10_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_11_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_12_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_13_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_14_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_15_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_16_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_17_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_18_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_19_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_20_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_21_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_22_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_23_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_24_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_25_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_26_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_27_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_28_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_29_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_30_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_31_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_32_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_33_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_34_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_35_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_36_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_37_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_38_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_39_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_40_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_41_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_42_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_43_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_44_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_45_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_46_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_47_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_48_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_49_raw_6pairs.jsonl\n",
            "üëâ Reading: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data/topic_50_raw_6pairs.jsonl\n",
            "üìä T·ªïng s·ªë h·ªôi tho·∫°i h·ª£p l·ªá: 959\n",
            "‚úî ƒê√£ t·∫°o file cleaned: /content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/dataset_cleaned.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # CELL 4 ‚Äî Ph√¢n t√≠ch dataset cleaned\n",
        "\n",
        "print(\"üìä T·ªïng h·ªôi tho·∫°i:\", len(all_items))\n",
        "\n",
        "lengths = [len(x[\"messages\"]) for x in all_items]\n",
        "avg_len = sum(lengths) / len(lengths)\n",
        "\n",
        "print(\"üìè S·ªë messages trung b√¨nh / h·ªôi tho·∫°i:\", avg_len)\n",
        "print(\"üìè T·ªëi thi·ªÉu:\", min(lengths))\n",
        "print(\"üìè T·ªëi ƒëa:\", max(lengths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbH_-tcPCECx",
        "outputId": "611b30bc-b2de-4190-eec4-47b8786d8623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä T·ªïng h·ªôi tho·∫°i: 959\n",
            "üìè S·ªë messages trung b√¨nh / h·ªôi tho·∫°i: 12.0\n",
            "üìè T·ªëi thi·ªÉu: 12\n",
            "üìè T·ªëi ƒëa: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 4 ‚Äî Ph√¢n t√≠ch ng·∫Øn dataset"
      ],
      "metadata": {
        "id": "Wp8929a6yAmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"S·ªë m·∫´u:\", len(all_items))\n",
        "lens = [len(x[\"messages\"]) for x in all_items]\n",
        "print(\"S·ªë messages trung b√¨nh:\", sum(lens)/len(lens))\n"
      ],
      "metadata": {
        "id": "clHvNyJOyAmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361142a0-d8c0-4380-e6ce-34f848ec51a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ªë m·∫´u: 959\n",
            "S·ªë messages trung b√¨nh: 12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 5 ‚Äî TRAIN V·ªöI LoRA"
      ],
      "metadata": {
        "id": "P0flnL1_yApW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# training_v3_full_for_unsloth.ipynb\n",
        "# CELL 0 ‚Äî Install libs (ch·∫°y 1 l·∫ßn)\n",
        "# ============================\n",
        "!pip install -q transformers accelerate peft datasets bitsandbytes trl sentencepiece safetensors huggingface_hub wandb\n"
      ],
      "metadata": {
        "id": "0FK8mAT8yka4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CELL 1 ‚Äî Imports + constants\n",
        "# ============================\n",
        "import os, json, re\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# --- Paths (ch·ªânh n·∫øu c·∫ßn) ---\n",
        "RAW_DIR = \"/content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data\"\n",
        "CLEAN_DIR = \"/content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly\"\n",
        "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
        "\n",
        "# Model to use\n",
        "MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# Grouping: m·ªói dialogue = N_pairs (N_pairs * 2 messages)\n",
        "PAIRS_PER_DIALOGUE = 6\n",
        "MAX_LENGTH = 1024\n",
        "\n",
        "print(\"RAW_DIR:\", RAW_DIR)\n",
        "print(\"CLEAN_DIR:\", CLEAN_DIR)\n",
        "print(\"MODEL_NAME:\", MODEL_NAME)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTaoFnDPymnr",
        "outputId": "e9d591bf-9d20-450d-f855-000a4bda5c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW_DIR: /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data\n",
            "CLEAN_DIR: /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly\n",
            "MODEL_NAME: unsloth/Llama-3.2-1B-Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CELL 2 ‚Äî Utility: extract messages robustly from file text\n",
        "# - h·ªó tr·ª£ multi-line JSON, pretty-print, broken lines\n",
        "# - c·ªë g·∫Øng b·∫Øt \"role\" & \"content\" b·∫±ng regex linh ho·∫°t\n",
        "# ============================\n",
        "def extract_messages_from_text(text: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Tr√≠ch role+content t·ª´ m·ªôt vƒÉn b·∫£n b·∫•t k·ª≥ b·∫±ng regex an to√†n.\n",
        "    Tr·∫£ v·ªÅ list c√°c dict {\"role\":..., \"content\":...} theo th·ª© t·ª± xu·∫•t hi·ªán.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    # Regex an to√†n: t√¨m \"role\": \"xxx\" v√† \"content\": \"yyy\" (allow escaped quotes)\n",
        "    pattern = re.compile(r'\"role\"\\s*:\\s*\"([^\"]+)\"\\s*[,}].*?\"content\"\\s*:\\s*\"((?:[^\"\\\\]|\\\\.)*)\"', re.DOTALL)\n",
        "    for m in pattern.finditer(text):\n",
        "        role = m.group(1).strip()\n",
        "        content = m.group(2).encode('utf-8').decode('unicode_escape')  # unescape\n",
        "        content = content.replace(\"\\n\", \" \").strip()\n",
        "        messages.append({\"role\": role, \"content\": content})\n",
        "    return messages\n",
        "\n",
        "def load_messages_from_file(path: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    N·∫øu file l√† JSONL (m·ªói d√≤ng 1 object) th√¨ parse d√≤ng;\n",
        "    else fallback ƒë·ªçc to√†n file v√† extract b·∫±ng regex.\n",
        "    \"\"\"\n",
        "    msgs = []\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            # quick check: if file looks like JSONL (many lines each starting with {\"messages\")\n",
        "            sample = f.read(1000)\n",
        "            f.seek(0)\n",
        "            if \"\\n\" in sample and '\"messages\"' in sample:\n",
        "                # treat as JSONL or 1-line JSONL\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "                    try:\n",
        "                        obj = json.loads(line)\n",
        "                        if isinstance(obj, dict) and \"messages\" in obj:\n",
        "                            for m in obj[\"messages\"]:\n",
        "                                if isinstance(m, dict) and \"role\" in m and \"content\" in m:\n",
        "                                    msgs.append({\"role\": m[\"role\"], \"content\": m[\"content\"]})\n",
        "                        else:\n",
        "                            # fallback: try extract role/content inside this line\n",
        "                            msgs += extract_messages_from_text(line)\n",
        "                    except Exception:\n",
        "                        # fallback to regex extraction on the line\n",
        "                        msgs += extract_messages_from_text(line)\n",
        "            else:\n",
        "                # not JSONL-like -> read whole file and extract\n",
        "                full = f.read()\n",
        "                msgs = extract_messages_from_text(full)\n",
        "    except Exception as e:\n",
        "        print(\"Error reading\", path, e)\n",
        "    return msgs\n"
      ],
      "metadata": {
        "id": "XBQ6ECjDyo9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # CELL 3 ‚Äî X·ª≠ l√Ω d·ªØ li·ªáu theo logic clear.py\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from glob import glob\n",
        "\n",
        "# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---\n",
        "# H√£y ƒë·∫£m b·∫£o RAW_DIR tr·ªè ƒë√∫ng t·ªõi th∆∞ m·ª•c ch·ª©a c√°c file topic_*.jsonl tr√™n Drive\n",
        "RAW_DIR = \"/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly/raw_data\"\n",
        "CLEAN_DIR = \"/content/drive/MyDrive/DoctorAI - TroÃõÃ£ lyÃÅ SuÃõÃÅc khoÃâe CaÃÅ nhaÃÇn ThoÃÇng minh/tam_ly\"\n",
        "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
        "\n",
        "# --- H√ÄM T·ª™ FILE CLEAR.PY ---\n",
        "def extract_messages_safely(text):\n",
        "    \"\"\"\n",
        "    Tr√≠ch t·∫•t c·∫£ messages t·ª´ file b·∫±ng Regex (logic t·ª´ clear.py)\n",
        "    \"\"\"\n",
        "    pattern = r'\"role\"\\s*:\\s*\"([^\"]+)\"\\s*,\\s*\"content\"\\s*:\\s*\"((?:[^\"\\\\]|\\\\.)*)\"'\n",
        "    matches = re.findall(pattern, text, flags=re.DOTALL)\n",
        "\n",
        "    messages = []\n",
        "    for role, content in matches:\n",
        "        # X·ª≠ l√Ω unescape k√Ω t·ª± unicode n·∫øu c·∫ßn, v√† x√≥a xu·ªëng d√≤ng th·ª´a\n",
        "        # Logic g·ªëc c·ªßa clear.py: content.replace('\\n', ' ').strip()\n",
        "        cleaned_content = content.replace('\\\\n', ' ').replace('\\n', ' ').strip()\n",
        "\n",
        "        # Chu·∫©n h√≥a role cho Llama 3 (user/assistant)\n",
        "        # clear.py gi·ªØ nguy√™n role g·ªëc, nh∆∞ng khi train c·∫ßn chu·∫©n h√≥a\n",
        "        role_norm = \"user\" if \"user\" in role.lower() else \"assistant\"\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": role_norm,\n",
        "            \"content\": cleaned_content\n",
        "        })\n",
        "    return messages\n",
        "\n",
        "def group_into_6pair_dialogues(messages):\n",
        "    \"\"\"\n",
        "    Gom th√†nh c√°c h·ªôi tho·∫°i, m·ªói h·ªôi tho·∫°i ch·ª©a 6 c·∫∑p (12 messages).\n",
        "    \"\"\"\n",
        "    dialogues = []\n",
        "    buffer = []\n",
        "    pair_count = 0\n",
        "\n",
        "    for msg in messages:\n",
        "        buffer.append(msg)\n",
        "\n",
        "        # ƒë·ªß 2 messages (1 user, 1 assistant) = 1 c·∫∑p\n",
        "        if len(buffer) % 2 == 0:\n",
        "            pair_count += 1\n",
        "\n",
        "        # ƒë·ªß 6 c·∫∑p = 12 messages\n",
        "        if pair_count == 6:\n",
        "            dialogues.append({\"messages\": buffer.copy()})\n",
        "            buffer.clear()\n",
        "            pair_count = 0\n",
        "\n",
        "    return dialogues\n",
        "\n",
        "# --- TH·ª∞C THI ---\n",
        "files = sorted(glob(f\"{RAW_DIR}/*.jsonl\"))\n",
        "print(f\"üìÅ T√¨m th·∫•y {len(files)} files raw.\")\n",
        "\n",
        "all_dialogues = []\n",
        "\n",
        "for f in files:\n",
        "    try:\n",
        "        with open(f, \"r\", encoding=\"utf-8\") as fr:\n",
        "            text = fr.read()\n",
        "\n",
        "        # 1. Tr√≠ch xu·∫•t\n",
        "        msgs = extract_messages_safely(text)\n",
        "\n",
        "        # 2. Gom nh√≥m\n",
        "        dialogs = group_into_6pair_dialogues(msgs)\n",
        "\n",
        "        if len(dialogs) > 0:\n",
        "            all_dialogues.extend(dialogs)\n",
        "            print(f\"  ‚úî {os.path.basename(f)}: Extracted {len(msgs)} msgs -> {len(dialogs)} dialogues\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è {os.path.basename(f)}: Kh√¥ng t·∫°o ƒë∆∞·ª£c h·ªôi tho·∫°i n√†o (Messages: {len(msgs)})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå L·ªói ƒë·ªçc file {f}: {e}\")\n",
        "\n",
        "print(f\"\\nüìä T·ªîNG C·ªòNG: {len(all_dialogues)} h·ªôi tho·∫°i s·∫°ch (m·ªói h·ªôi tho·∫°i 12 tin nh·∫Øn).\")\n",
        "\n",
        "# L∆∞u file k·∫øt qu·∫£\n",
        "clean_file = os.path.join(CLEAN_DIR, \"dataset_cleaned.jsonl\")\n",
        "with open(clean_file, \"w\", encoding=\"utf-8\") as fw:\n",
        "    for d in all_dialogues:\n",
        "        fw.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"üíæ ƒê√£ l∆∞u dataset s·∫°ch t·∫°i: {clean_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH84i-LAys6h",
        "outputId": "94bb8320-a4a6-498e-a3c3-145306fa8425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found raw files: 50\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_01_raw_6pairs.jsonl : extracted 1056 messages\n",
            "    ‚Üí 88 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_02_raw_6pairs.jsonl : extracted 492 messages\n",
            "    ‚Üí 41 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_03_raw_6pairs.jsonl : extracted 576 messages\n",
            "    ‚Üí 48 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_04_raw_6pairs.jsonl : extracted 552 messages\n",
            "    ‚Üí 46 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_05_raw_6pairs.jsonl : extracted 504 messages\n",
            "    ‚Üí 42 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_06_raw_6pairs.jsonl : extracted 600 messages\n",
            "    ‚Üí 50 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_07_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_08_raw_6pairs.jsonl : extracted 984 messages\n",
            "    ‚Üí 82 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_09_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_10_raw_6pairs.jsonl : extracted 1104 messages\n",
            "    ‚Üí 92 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_11_raw_6pairs.jsonl : extracted 504 messages\n",
            "    ‚Üí 42 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_12_raw_6pairs.jsonl : extracted 876 messages\n",
            "    ‚Üí 73 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_13_raw_6pairs.jsonl : extracted 600 messages\n",
            "    ‚Üí 50 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_14_raw_6pairs.jsonl : extracted 504 messages\n",
            "    ‚Üí 42 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_15_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_16_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_17_raw_6pairs.jsonl : extracted 972 messages\n",
            "    ‚Üí 81 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_18_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_19_raw_6pairs.jsonl : extracted 504 messages\n",
            "    ‚Üí 42 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_20_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_21_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_22_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_23_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_24_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_25_raw_6pairs.jsonl : extracted 504 messages\n",
            "    ‚Üí 42 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_26_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_27_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_28_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_29_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_30_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_31_raw_6pairs.jsonl : extracted 540 messages\n",
            "    ‚Üí 45 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_32_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_33_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_34_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_35_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_36_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_37_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_38_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_39_raw_6pairs.jsonl : extracted 636 messages\n",
            "    ‚Üí 53 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_40_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_41_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_42_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_43_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_44_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_45_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_46_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_47_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_48_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_49_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            " -> /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/raw_data/topic_50_raw_6pairs.jsonl : extracted 0 messages\n",
            "    ‚Üí 0 dialogues from file\n",
            "Total dialogues collected: 959\n",
            "Saved cleaned dataset: /content/drive/MyDrive/DoctorAI - Tr·ª£ l√Ω S·ª©c kh·ªèe C√° nh√¢n Th√¥ng minh/tam_ly/dataset_cleaned.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CELL 4 ‚Äî Quick validation & stats\n",
        "# ============================\n",
        "from collections import Counter\n",
        "lens = [len(d[\"messages\"]) for d in all_dialogues]\n",
        "print(\"Dialogues:\", len(all_dialogues))\n",
        "print(\"Messages per dialog - min/avg/max:\", min(lens), sum(lens)/len(lens), max(lens))\n",
        "# check sample\n",
        "print(\"Sample dialogue 0:\", all_dialogues[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95QQ0tdsyvcd",
        "outputId": "41793690-47b2-41d1-a997-b904a516828a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogues: 959\n",
            "Messages per dialog - min/avg/max: 12 12.0 12\n",
            "Sample dialogue 0: {'messages': [{'role': 'user', 'content': \"Tr√°¬ª\\x9di √Ü¬°i, tao m√°¬ª\\x87t qu√É¬° m√É\\xa0y √Ü¬°i. Tu√°¬∫¬ßn n√É\\xa0y ng√É\\xa0y n√É\\xa0o c√Ö¬©ng 10 gi√°¬ª\\x9d √Ñ\\x91√É¬™m m√°¬ª\\x9bi v√°¬ª\\x81. √É\\x94ng s√°¬∫¬øp tao c√°¬ª¬© b√°¬∫¬£o 'c√°¬ª\\x91 th√É¬™m ch√É¬∫t n√°¬ª¬Øa', m√É\\xa0 c√°¬ª\\x91 th√É¬™m l√É\\xa0 kh√É¬¥ng c√É¬≥ ti√°¬ª\\x81n th√É¬™m √Ñ\\x91√É¬≥ tr√°¬ª\\x9di!\"}, {'role': 'assistant', 'content': \"√É\\x94i gi√°¬ª\\x9di √Ü¬°i, √Ñ\\x91√°¬ª\\x93ng √Ñ\\x91√°¬ª\\x99i c√°¬ª¬ßa nh√°¬ª¬Øng 'ng√Ü¬∞√°¬ª\\x9di √Ñ\\x91√Ü¬∞√°¬ª¬£c ch√°¬ª\\x8dn' √Ñ\\x91√É¬¢y r√°¬ª\\x93i! Tao hi√°¬ª\\x83u m√É\\xa0, c√°¬∫¬£m gi√É¬°c y nh√Ü¬∞ b√°¬ª\\x8b h√É¬∫t c√°¬∫¬°n n√Ñ\\x83ng l√Ü¬∞√°¬ª¬£ng √Ñ\\x91√°¬ª\\x83 l√É\\xa0m pin cho √Ñ\\x91√É¬®n pin c√°¬ª¬ßa c√É¬¥ng ty v√°¬∫\\xady. S√°¬∫¬øp m√É\\xa0y t√Ü¬∞√°¬ª\\x9fng m√É\\xa0y l√É\\xa0 si√É¬™u nh√É¬¢n √É\\xa0, hay l√É\\xa0 m√É¬°y ch√°¬∫¬°y b√°¬∫¬±ng kh√É¬¥ng kh√É\\xad? R√°¬ª\\x93i sao n√°¬ª¬Øa? √°¬∫¬¢nh b√°¬∫¬Øt m√É\\xa0y l√É\\xa0m c√É¬°i g√É¬¨ m√É\\xa0 kinh kh√°¬ª¬ßng d√°¬ª¬Ø v√°¬∫\\xady?\"}, {'role': 'user', 'content': \"To√É\\xa0n m√°¬∫¬•y c√É¬°i task kh√É¬¥ng t√É¬™n, deadline th√É¬¨ tr√É¬™n tr√°¬ª\\x9di. L√É¬∫c tao h√°¬ª\\x8fi l√Ü¬∞√Ü¬°ng OT th√É¬¨ √°¬∫¬£nh c√Ü¬∞√°¬ª\\x9di tr√°¬ª¬´ b√°¬∫¬£o 'C√Ü¬° h√°¬ª\\x99i h√°¬ª\\x8dc h√°¬ª\\x8fi, c√Ü¬° h√°¬ª\\x99i c√°¬ª\\x91ng hi√°¬∫¬øn'. C√°¬ª\\x91ng hi√°¬∫¬øn b√°¬∫¬±ng m√°¬ª\\x93 h√É¬¥i n√Ü¬∞√°¬ª\\x9bc m√°¬∫¬Øt m√É\\xa0 kh√É¬¥ng th√°¬∫¬•y c√°¬ª\\x91ng hi√°¬∫¬øn b√°¬∫¬±ng ti√°¬ª\\x81n b√°¬∫¬°c √Ñ\\x91√É¬¢u h√°¬∫¬øt!\"}, {'role': 'assistant', 'content': \"Tr√°¬ª\\x9di √Ñ\\x91√°¬∫¬•t, 'C√Ü¬° h√°¬ª\\x99i h√°¬ª\\x8dc h√°¬ª\\x8fi' c√É¬°i g√É¬¨! H√°¬ª\\x8dc h√°¬ª\\x8fi c√É¬°ch l√É\\xa0m n√É¬¥ l√°¬ª\\x87 th√°¬ª\\x9di hi√°¬ª\\x87n √Ñ\\x91√°¬∫¬°i h√°¬∫¬£? C√É¬°i √Ñ\\x91√É¬≥ kh√É¬¥ng ph√°¬∫¬£i c√Ü¬∞√°¬ª\\x9di tr√°¬ª¬´, c√É¬°i √Ñ\\x91√É¬≥ l√É\\xa0 c√Ü¬∞√°¬ª\\x9di kh√°¬∫¬©y! Ki√°¬ª\\x83u n√É\\xa0y ch√°¬∫¬Øc s√°¬∫¬øp m√É\\xa0y ngh√Ñ¬© 'nh√É¬¢n vi√É¬™n' l√É\\xa0 t√°¬ª¬´ vi√°¬∫¬øt t√°¬∫¬Øt c√°¬ª¬ßa 'Nh√°¬∫¬´n N√°¬∫¬°i V√É¬¥ Bi√É¬™n, √°¬ª\\x80 √°¬∫¬®n L√É\\xa0m Vi√°¬ª\\x87c' qu√É¬°. B√°¬ª\\x99 √°¬∫¬£nh t√Ü¬∞√°¬ª\\x9fng ti√°¬ª\\x81n l√É\\xa0 l√É¬° c√É¬¢y hay sao m√É\\xa0 keo ki√°¬ª\\x87t d√°¬ª¬Ø th√°¬∫¬ßn v√°¬∫\\xady? Thi√°¬ª\\x87t lu√É¬¥n √É¬°, k√°¬ª\\x83 chi ti√°¬∫¬øt coi, c√É¬°i task 'kh√É¬¥ng t√É¬™n' n√É\\xa0o l√É\\xa0m m√É\\xa0y mu√°¬ª\\x91n b√°¬ª\\x91c h√°¬ª\\x8fa nh√°¬∫¬•t?\"}, {'role': 'user', 'content': \"H√É¬¥m qua l√É\\xa0 c√É¬°i b√É¬°o c√É¬°o tu√°¬∫¬ßn. √Ñ\\x90√É¬£ xong h√°¬∫¬øt r√°¬ª\\x93i, t√°¬ª¬± nhi√É¬™n √°¬∫¬£nh b√°¬∫¬£o 'L√É\\xa0m th√É¬™m c√É¬°i slide t√°¬ª\\x95ng h√°¬ª¬£p √É¬Ω ch√É\\xadnh g√°¬ª\\xadi ri√É¬™ng cho anh, ch√°¬ª\\x89 c√°¬∫¬ßn 5 ph√É¬∫t th√É¬¥i'. R√°¬ª\\x93i 5 ph√É¬∫t √Ñ\\x91√É¬≥ k√É¬©o d√É\\xa0i √Ñ\\x91√°¬∫¬øn 1 ti√°¬∫¬øng r√Ü¬∞√°¬ª¬°i.\"}, {'role': 'assistant', 'content': \"Aha! C√É¬°i chi√É¬™u 'ch√°¬ª\\x89 c√°¬∫¬ßn 5 ph√É¬∫t th√É¬¥i' huy√°¬ª\\x81n tho√°¬∫¬°i √Ñ\\x91√É¬≥! 5 ph√É¬∫t c√°¬ª¬ßa s√°¬∫¬øp l√É\\xa0 5 gi√°¬ª\\x9d √Ñ\\x91au kh√°¬ª\\x95 c√°¬ª¬ßa nh√É¬¢n vi√É¬™n. Tao c√É¬° l√É\\xa0 √°¬ª\\x95ng t√É\\xadnh th√°¬ª\\x9di gian b√°¬∫¬±ng h√°¬ª\\x87 √Ñ\\x91o l√Ü¬∞√°¬ª\\x9dng c√°¬ª¬ßa sao H√°¬ª\\x8fa, ch√°¬ª¬© 5 ph√É¬∫t ki√°¬ª\\x83u g√É¬¨ m√É\\xa0 ra 90 ph√É¬∫t √Ñ\\x91√Ü¬∞√°¬ª¬£c! M√É\\xa0y mu√°¬ª\\x91n √Ñ\\x91√°¬∫¬•m l√É\\xa0 ph√°¬∫¬£i r√°¬ª\\x93i, tao c√É¬≤n mu√°¬ª\\x91n c√°¬∫¬ßm c√É¬°i b√É¬°o c√É¬°o √Ñ\\x91√É¬≥ cu√°¬ª\\x99n l√°¬∫¬°i r√°¬ª\\x93i... th√É¬¥i, kh√É¬¥ng n√É¬≥i n√°¬ª¬Øa. Nh√Ü¬∞ng m√É\\xa0 sau c√É¬°i 1 ti√°¬∫¬øng r√Ü¬∞√°¬ª¬°i √Ñ\\x91√É¬≥, √°¬ª\\x95ng c√É¬≥ n√É¬≥i g√É¬¨ n√°¬ª¬Øa kh√É¬¥ng? K√°¬ª\\x83 ti√°¬∫¬øp coi, tao h√É¬≥ng c√É¬°i s√°¬ª¬± b√É¬≥c l√°¬ª\\x99t n√É\\xa0y qu√É¬°!\"}, {'role': 'user', 'content': \"Tao ki√°¬ª\\x87t s√°¬ª¬©c r√°¬ª\\x93i. L√°¬∫¬°i v√°¬ª¬´a ph√°¬∫¬£i √°¬ª\\x9f l√°¬∫¬°i √Ñ\\x91√°¬∫¬øn 9h t√°¬ª\\x91i ch√°¬ª\\x89 v√É¬¨ s√°¬∫¬øp b√°¬∫¬£o 'c√°¬∫¬ßn g√°¬∫¬•p' c√É¬°i slide tr√É¬¨nh b√É\\xa0y v√É\\xa0o s√É¬°ng mai m√É\\xa0 l√°¬∫¬Ω ra ph√°¬∫¬£i l√É\\xa0m t√°¬ª¬´ tu√°¬∫¬ßn tr√Ü¬∞√°¬ª\\x9bc. OT kh√É¬¥ng l√Ü¬∞√Ü¬°ng lu√É¬¥n nha!\"}, {'role': 'assistant', 'content': \"√É\\x94i b√°¬∫¬°n √Ü¬°i, tao nghe m√É\\xa0 tim tao nh√É¬≥i l√É¬™n. C√°¬∫¬£m gi√É¬°c c√É¬°i l√Ü¬∞ng c√°¬ª¬ßa m√É\\xa0y n√É¬≥ √Ñ\\x91ang g√É\\xa0o th√É¬©t 'M√É\\xa0y l√É\\xa0 ng√Ü¬∞√°¬ª\\x9di, kh√É¬¥ng ph√°¬∫¬£i c√É¬°i m√É¬°y in!' √°¬∫¬•y nh√°¬ª\\x89. C√É¬°i ki√°¬ª\\x83u 'c√°¬∫¬ßn g√°¬∫¬•p' m√É\\xa0 l√É\\xa0 l√°¬ª\\x97i c√°¬ª¬ßa √°¬ª\\x95ng th√É¬¨ √Ñ\\x91√É¬∫ng l√É\\xa0 mu√°¬ª\\x91n √Ñ\\x91√°¬ª\\x99n th√°¬ª\\x95. S√°¬∫¬øp m√É\\xa0y c√É¬≥ b√°¬ª\\x8b m√°¬ª\\x99ng du kh√É¬¥ng, hay l√É\\xa0 √°¬ª\\x95ng ngh√Ñ¬© th√°¬ª\\x9di gian c√É¬≥ th√°¬ª\\x83 b√°¬∫¬ª cong √Ñ\\x91√Ü¬∞√°¬ª¬£c? R√°¬ª\\x93i s√É¬°ng mai √°¬ª\\x95ng c√É¬≥ khen m√É\\xa0y v√É¬¨ 'tinh th√°¬∫¬ßn tr√É¬°ch nhi√°¬ª\\x87m' kh√É¬¥ng? K√°¬ª\\x83 coi!\"}, {'role': 'user', 'content': \"Khen c√É¬°i g√É¬¨ m√É\\xa0 khen! S√É¬°ng nay √°¬ª\\x95ng ch√°¬ª\\x89 nh√É¬¨n l√Ü¬∞√°¬ª\\x9bt qua r√°¬ª\\x93i b√°¬∫¬£o 'C√Ö¬©ng √Ñ\\x91√Ü¬∞√°¬ª¬£c, c√É¬≥ m√°¬∫¬•y ch√°¬ª\\x97 c√°¬∫¬ßn ch√°¬ª\\x89nh nh√°¬∫¬π.' Ch√°¬ª\\x89nh nh√°¬∫¬π m√É\\xa0 l√É\\xa0m tao ph√°¬∫¬£i th√°¬ª¬©c √Ñ\\x91√°¬∫¬øn 1 gi√°¬ª\\x9d s√É¬°ng √Ñ\\x91√°¬ª\\x83 fix l√°¬ª\\x97i ch√É\\xadnh t√°¬∫¬£ √Ñ\\x91√°¬∫¬•y!\"}, {'role': 'assistant', 'content': \"√Ñ\\x90√°¬ª\\x89nh cao c√°¬ª¬ßa s√°¬ª¬± v√É¬¥ l√É¬Ω! C√É¬°i 'ch√°¬ª\\x89nh nh√°¬∫¬π' √Ñ\\x91√É¬≥ nghe nh√Ü¬∞ ki√°¬ª\\x83u 'Tao mu√°¬ª\\x91n m√É\\xa0y s√°¬ª\\xada c√°¬∫¬£ v√Ö¬© tr√°¬ª¬• n√É\\xa0y' v√°¬∫\\xady. M√É\\xa0y th√°¬ª¬©c √Ñ\\x91√°¬∫¬øn 1 gi√°¬ª\\x9d s√É¬°ng m√É\\xa0 ch√°¬ª\\x89 nh√°¬∫\\xadn √Ñ\\x91√Ü¬∞√°¬ª¬£c c√É¬°i c√É¬¢u 'C√Ö¬©ng √Ñ\\x91√Ü¬∞√°¬ª¬£c' √É¬°? M√É¬° √Ü¬°i, n√°¬∫¬øu l√É\\xa0 tao, tao √Ñ\\x91√É¬£ x√É¬© c√É¬°i slide √Ñ\\x91√É¬≥ ra l√É\\xa0m gi√°¬∫¬•y g√É¬≥i qu√É\\xa0 t√°¬∫¬∑ng sinh nh√°¬∫\\xadt √°¬ª\\x95ng r√°¬ª\\x93i. √°¬∫¬¢nh ngh√Ñ¬© m√É¬¨nh l√É\\xa0 ho√É\\xa0ng t√°¬ª\\xad ng√°¬ª¬ß trong r√°¬ª¬´ng h√°¬∫¬£, m√É\\xa0 vi√°¬ª\\x87c g√É¬¨ c√Ö¬©ng c√°¬∫¬ßn g√°¬∫¬•p g√É¬°p? Thi√°¬ª\\x87t h√°¬∫¬£? √°¬ª\\x94ng kh√É¬¥ng th√É¬®m n√É¬≥i c√É¬¢u c√°¬∫¬£m √Ü¬°n n√É\\xa0o lu√É¬¥n h√°¬∫¬£?\"}, {'role': 'user', 'content': '√Ñ\\x90√É¬∫ng r√°¬ª\\x93i. Kh√É¬¥ng m√°¬ª\\x99t l√°¬ª\\x9di n√É\\xa0o. Tao th√°¬∫¬•y m√É¬¨nh nh√Ü¬∞ c√É¬°i gi√°¬∫¬ª lau b√É\\xa0n ph√É\\xadm c√°¬ª¬ßa c√É¬¥ng ty v√°¬∫\\xady. √Ñ\\x90√É¬£ kh√É¬¥ng ti√°¬ª\\x81n OT c√É¬≤n b√°¬ª\\x8b coi nh√Ü¬∞ hi√°¬ª\\x83n nhi√É¬™n ph√°¬∫¬£i l√É\\xa0m. Th√°¬∫¬•t v√°¬ª\\x8dng to√É\\xa0n t√°¬∫\\xadp.'}, {'role': 'assistant', 'content': \"Tao th√°¬∫¬•y m√É¬¨nh c√É¬≤n t√°¬ª\\x87 h√Ü¬°n m√É\\xa0y, √É\\xadt nh√°¬∫¬•t m√É\\xa0y c√É¬≤n l√É\\xa0 'gi√°¬∫¬ª lau b√É\\xa0n ph√É\\xadm', tao ch√°¬∫¬Øc ch√°¬ª\\x89 l√É\\xa0 h√°¬∫¬°t b√°¬ª¬•i d√Ü¬∞√°¬ª\\x9bi g√°¬∫¬ßm b√É\\xa0n th√É¬¥i. Nh√Ü¬∞ng m√É\\xa0, m√É\\xa0y x√°¬ª¬©ng √Ñ\\x91√É¬°ng √Ñ\\x91√Ü¬∞√°¬ª¬£c c√É¬¥ng nh√°¬∫\\xadn h√Ü¬°n l√É\\xa0 c√É¬°i 'hi√°¬ª\\x83n nhi√É¬™n' √Ñ\\x91√É¬≥! S√°¬∫¬øp m√É\\xa0y ch√°¬∫¬Øc l√É\\xa0 √Ñ\\x91ang h√°¬ª\\x8dc c√É¬°ch nh√É¬¢n b√°¬∫¬£n ti√°¬ª\\x81n b√°¬∫¬±ng c√É¬°ch kh√É¬¥ng tr√°¬∫¬£ l√Ü¬∞√Ü¬°ng cho ai. R√°¬ª\\x93i c√É¬°i bu√°¬ª\\x95i s√É¬°ng 'ch√°¬ª\\x89nh nh√°¬∫¬π' √Ñ\\x91√É¬≥ c√É¬≤n c√É¬≥ drama g√É¬¨ n√°¬ª¬Øa kh√É¬¥ng? K√°¬ª\\x83 tao nghe cho b√°¬ª\\x9bt √°¬∫¬•m √°¬ª¬©c coi!\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # CELL 5 ‚Äî Tokenize & Prepare Dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# T·∫°o Dataset t·ª´ list dictionary ƒë√£ x·ª≠ l√Ω ·ªü Cell 3\n",
        "hf_dataset = Dataset.from_list(all_dialogues)\n",
        "\n",
        "def formatting_prompts_func(example):\n",
        "    \"\"\"\n",
        "    S·ª≠ d·ª•ng chat template chu·∫©n c·ªßa model (Llama-3).\n",
        "    Input: list of dicts [{'role': 'user', 'content': '...'}, ...]\n",
        "    Output: String ƒë√£ ƒë∆∞·ª£c format v·ªõi special tokens (<|start_header_id|>, v.v.)\n",
        "    \"\"\"\n",
        "    msgs = example[\"messages\"]\n",
        "\n",
        "    # apply_chat_template s·∫Ω t·ª± ƒë·ªông th√™m c√°c token h·ªá th·ªëng c·ªßa Llama 3\n",
        "    # tokenize=False ƒë·ªÉ tr·∫£ v·ªÅ string tr∆∞·ªõc, sau ƒë√≥ tokenizer s·∫Ω x·ª≠ l√Ω sau\n",
        "    text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "# B∆∞·ªõc 1: Format th√†nh text string chu·∫©n Llama 3\n",
        "hf_dataset = hf_dataset.map(formatting_prompts_func, batched=False)\n",
        "\n",
        "# B∆∞·ªõc 2: Tokenize\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\", # Pad ƒë·ªÉ batch training ·ªïn ƒë·ªãnh h∆°n\n",
        "    )\n",
        "\n",
        "tokenized_datasets = hf_dataset.map(tokenize_function, batched=True, remove_columns=[\"messages\", \"text\"])\n",
        "\n",
        "# G√°n labels = input_ids (cho Causal LM training)\n",
        "tokenized_datasets = tokenized_datasets.map(lambda x: {\"labels\": x[\"input_ids\"]}, batched=True)\n",
        "\n",
        "print(\"‚úÖ ƒê√£ tokenize xong. C·∫•u tr√∫c m·∫´u:\", tokenized_datasets[0].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "b408c4ac78b049e3bd87c6fee4c6d284",
            "729243f1f41d4c4d8dd4e68e0308b3bd",
            "0474e606404845d193cd66867b89d1a9",
            "9dddf8193d4a47d4acf0be028b48c166",
            "7327fd1271d84770bde9515f09d5f891",
            "066f8aa6c344483c91e7542929c5a2cf",
            "305914faefea4208a034e278cda69711",
            "a24a5eb7c0bd435fb1693c1d8077ae5f",
            "393b505e361a4151b44786b0098b0d96",
            "06b91ee7220e45a68e6dd0215f8eb0da",
            "d5b3da7f41b04731a740acc8c569943d"
          ]
        },
        "id": "8iAOzbE2yxQn",
        "outputId": "c14a017f-e07c-40c7-db11-abca210ee06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/959 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b408c4ac78b049e3bd87c6fee4c6d284"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns after tokenization: ['input_ids', 'attention_mask', 'labels']\n",
            "Example keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CELL 6 ‚Äî Load model (4-bit via BitsAndBytesConfig) + apply LoRA\n",
        "# ============================\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # Changed to 4-bit quantization\n",
        "    bnb_4bit_quant_type=\"nf4\", # Recommended type for 4-bit\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for computation\n",
        "    bnb_4bit_use_double_quant=True, # Double quantization for further memory savings\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map={\"\": 0}, # Explicitly map to GPU 0\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# LoRA config (tuneable)\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Enable gradient checkpointing for input gradients with PEFT models\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "print(\"Model loaded and LoRA applied.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opmawGn6yzlU",
        "outputId": "d6056999-d987-4692-9f3a-ea56184f6b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded and LoRA applied.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CELL 7 ‚Äî Data collator & TrainingArguments\n",
        "# ============================\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    label_pad_token_id=-100,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/output_model\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    bf16=False,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    remove_unused_columns=False,\n",
        "    gradient_checkpointing=True, # Enabled gradient checkpointing to save memory\n",
        "    report_to=\"none\"  # set \"wandb\" to enable\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efV3KzAfy1LT",
        "outputId": "8c531b83-b573-4604-d5e0-adceeb4d5104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CELL 8 ‚Äî Sanity checks before train\n",
        "# ============================\n",
        "# show a sample tensorization\n",
        "sample = hf_dataset[0]\n",
        "print(\"sample input_ids len:\", len(sample[\"input_ids\"]))\n",
        "print(\"sample labels len:\", len(sample[\"labels\"]))\n",
        "# check for empty samples\n",
        "bad = [i for i, x in enumerate(hf_dataset) if len(x[\"input_ids\"]) == 0]\n",
        "print(\"bad examples count:\", len(bad))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yIsQPfFy3vE",
        "outputId": "3b9e42d9-7f4f-4233-d9cd-a621d65b867e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample input_ids len: 1024\n",
            "sample labels len: 1024\n",
            "bad examples count: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "45ec492b",
        "outputId": "7af45ef1-aeee-4c47-e734-19de0e3ad67b"
      },
      "source": [
        "# @title # CELL 9 ‚Äî B·∫Øt ƒë·∫ßu Training & L∆∞u Model\n",
        "import torch\n",
        "\n",
        "# 1. B·∫Øt ƒë·∫ßu train\n",
        "print(\"üöÄ ƒêang b·∫Øt ƒë·∫ßu training... (B·∫°n c√≥ th·ªÉ ƒëi pha c√† ph√™ ‚òï)\")\n",
        "trainer.train()\n",
        "\n",
        "# 2. L∆∞u model LoRA v√† Tokenizer xu·ªëng ƒëƒ©a\n",
        "OUTPUT_DIR = \"/content/lora_model\"\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"‚úÖ ƒê√£ hu·∫•n luy·ªán xong!\")\n",
        "print(f\"üíæ Model LoRA v√† Tokenizer ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}\")\n",
        "print(\"=\"*50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [360/360 45:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.578800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.133900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.035300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.981900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.919700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.917800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.894100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved LoRA model to /content/lora_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # CELL 10 ‚Äî Test Model & Upload l√™n Hugging Face\n",
        "from transformers import pipeline, TextStreamer\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# --- PH·∫¶N 1: TEST MODEL NHANH ---\n",
        "print(\"üß™ ƒêang test th·ª≠ model v·ª´a train...\")\n",
        "\n",
        "# S·ª≠ d·ª•ng model ƒëang c√≥ trong b·ªô nh·ªõ (trainer.model) ƒë·ªÉ test ngay\n",
        "# Thi·∫øt l·∫≠p streamer ƒë·ªÉ ch·ªØ hi·ªán ra d·∫ßn d·∫ßn (gi·ªëng ChatGPT)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "def test_chat(prompt_text):\n",
        "    # T·∫°o ƒë·ªãnh d·∫°ng tin nh·∫Øn theo chu·∫©n Llama 3\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt_text}\n",
        "    ]\n",
        "\n",
        "    # Format input\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    print(f\"\\nUser: {prompt_text}\")\n",
        "    print(\"Assistant: \", end=\"\")\n",
        "\n",
        "    # Generate\n",
        "    _ = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=256,\n",
        "        streamer=streamer,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.7, # Ch·ªânh ƒë·ªô s√°ng t·∫°o (0.1 - 1.0)\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "# --- TH·ª¨ NGHI·ªÜM V·ªöI 1 C√ÇU H·ªéI M·∫™U ---\n",
        "test_chat(\"S·∫øp b·∫Øt tao l√†m th√™m gi·ªù m√† kh√¥ng tr·∫£ l∆∞∆°ng, tao ph·∫£i l√†m sao?\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "Mi8zwBLoy8Cb",
        "outputId": "ddbf2b64-a208-4d14-c44a-03c438573773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-600093050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/lora_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"H√¥m nay s·∫øp b·∫Øt t√¥i l√†m th√™m gi·ªù kh√¥ng tr·∫£ ti·ªÅn, t√¥i ph·∫£i ph·∫£n ·ª©ng sao?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generated_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1465\u001b[0m             )\n\u001b[1;32m   1466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2777\u001b[0m             \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2779\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2780\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2781\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PH·∫¶N 2: UPLOAD L√äN HUGGING FACE ---\n",
        "# B·ªè comment c√°c d√≤ng d∆∞·ªõi ƒë√¢y khi b·∫°n mu·ªën upload\n",
        "\n",
        "# print(\"\\n\" + \"=\"*30)\n",
        "# print(\"‚òÅÔ∏è B·∫ÆT ƒê·∫¶U UPLOAD L√äN HUGGING FACE\")\n",
        "# print(\"=\"*30)\n",
        "\n",
        "# 1. ƒêƒÉng nh·∫≠p (S·∫Ω hi·ªán khung nh·∫≠p Token n·∫øu ch∆∞a login)\n",
        "# login()\n",
        "\n",
        "# 2. ƒê·∫∑t t√™n Repo c·ªßa b·∫°n (Thay 'YourUsername' b·∫±ng t√™n user HF c·ªßa b·∫°n)\n",
        "# HF_USERNAME = \"YourUsername\"\n",
        "# REPO_NAME = \"funny-empathetic-companion-v1\"\n",
        "# FULL_REPO_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
        "\n",
        "# print(f\"ang ƒë·∫©y model l√™n: {FULL_REPO_ID} ...\")\n",
        "\n",
        "# 3. Push model (LoRA Adapter) & Tokenizer\n",
        "# model.push_to_hub(FULL_REPO_ID, use_auth_token=True)\n",
        "# tokenizer.push_to_hub(FULL_REPO_ID, use_auth_token=True)\n",
        "\n",
        "# print(f\"‚úÖ ƒê√£ upload th√†nh c√¥ng! Link: https://huggingface.co/{FULL_REPO_ID}\")"
      ],
      "metadata": {
        "id": "wyrfd6K2K1TX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}