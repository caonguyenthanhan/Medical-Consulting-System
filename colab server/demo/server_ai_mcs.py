# -*- coding: utf-8 -*-
"""server-AI-MCS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J37KPSwpYoSYz9lhVJyYvSnACqleIXpG
"""

# @title 1. C√†i ƒë·∫∑t Th∆∞ vi·ªán (Optimized)
# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán l√µi
# !pip install -q git+https://github.com/huggingface/transformers.git
# !pip install -q accelerate bitsandbytes sentencepiece
# !pip install -q fastapi uvicorn python-multipart pyngrok nest_asyncio
# !pip install -q gTTS librosa soundfile edge-tts
# # Faster-whisper nh·∫π v√† nhanh h∆°n b·∫£n g·ªëc c·ªßa OpenAI
# !pip install -q faster-whisper
# !apt-get install -y ffmpeg > /dev/null

print("‚úÖ ƒê√£ c√†i ƒë·∫∑t xong th∆∞ vi·ªán (Phi√™n b·∫£n t·ªëi ∆∞u VRAM).")

# @title 2. Kh·ªüi t·∫°o Model (Whisper ch·∫°y CPU ƒë·ªÉ c·ª©u GPU)
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig
from faster_whisper import WhisperModel
import os
import gc

# 1. D·ªçn d·∫πp s·∫°ch s·∫Ω b·ªô nh·ªõ tr∆∞·ªõc khi load
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()
gc.collect()

# --- C·∫§U H√åNH ---
CHAT_MODEL_ID = "unsloth/Llama-3.2-3B-Instruct"
VLM_MODEL_ID = "llava-hf/llava-1.5-7b-hf"

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚ñ∂ ƒêang ch·∫°y tr√™n thi·∫øt b·ªã ch√≠nh: {device.upper()}")
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

# C·∫•u h√¨nh 4-bit (Ch√¨a kh√≥a ƒë·ªÉ ch·∫°y tr√™n T4)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# ------------------------------------------------------------------
# A. Load Chat Model (Llama 3.2 3B - 4bit) -> GPU
# ------------------------------------------------------------------
print("‚è≥ [1/3] Loading Chat Model (GPU 4-bit)...")
chat_tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_ID)
chat_model = AutoModelForCausalLM.from_pretrained(
    CHAT_MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto"
)
print("   ‚úî Chat Model OK.")

# ------------------------------------------------------------------
# B. Load Vision Model (Llava 7B - 4bit) -> GPU
# ------------------------------------------------------------------
print("‚è≥ [2/3] Loading Vision Model (GPU 4-bit)...")
# L∆∞u √Ω: Llava 1.5 d√πng ki·∫øn tr√∫c c≈©, ta d√πng class chu·∫©n LlavaForConditionalGeneration ƒë·ªÉ tr√°nh warning
from transformers import LlavaForConditionalGeneration

vlm_processor = AutoProcessor.from_pretrained(VLM_MODEL_ID)
vlm_model = LlavaForConditionalGeneration.from_pretrained(
    VLM_MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto"
)
print("   ‚úî Vision Model OK.")

# ------------------------------------------------------------------
# C. Load Whisper (Faster-Whisper) -> CPU (ƒê·ªÇ TR√ÅNH OOM)
# ------------------------------------------------------------------
print("‚è≥ [3/3] Loading Faster-Whisper (CPU Int8)...")
# QUAN TR·ªåNG: device="cpu" v√† compute_type="int8"
# CPU c·ªßa Colab r·∫•t m·∫°nh, ch·∫°y int8 v·∫´n r·∫•t nhanh m√† kh√¥ng t·ªën 1MB VRAM n√†o
stt_model = WhisperModel("medium", device="cpu", compute_type="int8")

print("   ‚úî Whisper Model OK.")

# ------------------------------------------------------------------
print("\nüìä Tr·∫°ng th√°i VRAM sau khi load:")
print(torch.cuda.memory_summary(abbreviated=True))
print("üöÄ T·∫§T C·∫¢ MODEL ƒê√É S·∫¥N S√ÄNG (Llama+Llava on GPU, Whisper on CPU)!")

USE_NGROK = True
def pick_free_port(preferred: int = 8000) -> int:
    for p in [preferred] + list(range(preferred + 1, preferred + 50)):
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        try:
            s.bind(("0.0.0.0", p))
            s.close()
            return p
        except OSError:
            s.close()
            continue
    return preferred
PORT = pick_free_port(8000)

if USE_NGROK:
    from getpass import getpass
    NGROK_AUTH_TOKEN = getpass("Nh·∫≠p NGROK_AUTH_TOKEN: ")

# @title 3. Kh·ªüi ch·∫°y Server API (VRAM Safe Mode)
import base64
import uuid
import os
import json
import asyncio
import subprocess
import threading
import time
import socket
from io import BytesIO
from typing import List, Optional

from fastapi import FastAPI, File, UploadFile, Form, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
import nest_asyncio
from pyngrok import ngrok
from PIL import Image
import librosa
import soundfile as sf
import numpy as np
from gtts import gTTS

# Patch async cho Colab
try:
    nest_asyncio.apply()
except Exception:
    pass

app = FastAPI(title="Medical Consultation GPU API", version="2.1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==============================
# üî∂ DATA MODELS
# ==============================
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: Optional[str] = "llama-3.2-3b"
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512

class VisionMultiRequest(BaseModel):
    text: str
    images_base64: List[str]
    temperature: Optional[float] = 0.2
    max_tokens: Optional[int] = 256

class TTSRequest(BaseModel):
    text: str
    lang: Optional[str] = "vi"

# ==============================
# üî∑ 1. CHAT API
# ==============================
@app.post("/v1/chat/completions")
async def chat_completions(req: ChatRequest, x_mode: Optional[str] = Header(None)):
    try:
        input_text = chat_tokenizer.apply_chat_template(
            req.messages, tokenize=False, add_generation_prompt=True
        )
        inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")

        max_new = int(req.max_tokens or 256)
        temp = float(req.temperature or 0.7)
        try:
            with torch.no_grad():
                output = chat_model.generate(
                    **inputs,
                    max_new_tokens=max_new,
                    temperature=temp,
                    do_sample=True if temp > 0 else False,
                    pad_token_id=chat_tokenizer.eos_token_id
                )
        except Exception as gen_err:
            msg = str(gen_err)
            try:
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
            except Exception:
                pass
            gc.collect()
            if "CUDA out of memory" in msg or "CUDA" in msg:
                try:
                    with torch.no_grad():
                        output = chat_model.generate(
                            **inputs,
                            max_new_tokens=max(32, min(64, max_new)),
                            temperature=temp,
                            do_sample=True if temp > 0 else False,
                            pad_token_id=chat_tokenizer.eos_token_id
                        )
                except Exception as gen_err2:
                    return JSONResponse(status_code=500, content={"error": str(gen_err2)})
            else:
                return JSONResponse(status_code=500, content={"error": msg})

        generated_tokens = output[0][inputs.input_ids.shape[-1]:]
        response_text = chat_tokenizer.decode(generated_tokens, skip_special_tokens=True)

        del inputs, output
        torch.cuda.empty_cache()

        mode = (x_mode or ("pro" if (req.model or "").lower() == "pro" else "flash")).lower()
        rag_meta = {"used": mode == "pro", "retrieved": 0, "selected": 0}
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "choices": [{"index": 0, "message": {"role": "assistant", "content": response_text}, "finish_reason": "stop"}],
            "mode": mode,
            "rag": rag_meta
        }
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/v1/chat")
async def chat_simple(req: dict):
    # Proxy ƒë∆°n gi·∫£n cho chat
    msgs = req.get("messages", [])
    if not msgs: return {"reply": ""}

    input_text = chat_tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
    inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
    output = chat_model.generate(**inputs, max_new_tokens=256)
    text = chat_tokenizer.decode(output[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
    torch.cuda.empty_cache()
    return {"reply": text}

# ==============================
# üî∂ FRIEND CHAT API (tam-su)
# ==============================
@app.post("/v1/friend-chat/completions")
async def friend_chat_completions(req: ChatRequest, x_mode: Optional[str] = Header(None)):
    try:
        friend_system = "B·∫°n l√† m·ªôt ng∆∞·ªùi b·∫°n th√¢n, n√≥i chuy·ªán ƒë·ªùi th∆∞·ªùng b·∫±ng ti·∫øng Vi·ªát. C√°ch n√≥i t·ª± nhi√™n, g·∫ßn g≈©i, c√≥ th·ªÉ h√†i h∆∞·ªõc nh·∫π, d√πng t·ª´ ng·ªØ b√¨nh d√¢n. Nguy√™n t·∫Øc: ∆∞u ti√™n l·∫Øng nghe v√† ƒë·ªìng c·∫£m; kh√¥ng gi·∫£ng ƒë·∫°o l√Ω; kh√¥ng khuy√™n d·∫°y ngay tr·ª´ khi ng∆∞·ªùi d√πng h·ªèi r√µ; ph·∫£n h·ªìi gi·ªëng ng∆∞·ªùi th·∫≠t; c√≥ th·ªÉ h·ªèi l·∫°i 1 c√¢u ng·∫Øn ƒë·ªÉ hi·ªÉu th√™m c·∫£m x√∫c ng∆∞·ªùi n√≥i."
        msgs = req.messages or []
        if not msgs:
            msgs = [{"role": "system", "content": friend_system}]
        else:
            msgs = [{"role": "system", "content": friend_system}] + msgs
        input_text = chat_tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
        inputs = chat_tokenizer(input_text, return_tensors="pt").to("cuda")
        max_new = int(req.max_tokens or 256)
        temp = float(req.temperature or 0.7)
        try:
            with torch.no_grad():
                output = chat_model.generate(
                    **inputs,
                    max_new_tokens=max_new,
                    temperature=temp,
                    do_sample=True if temp > 0 else False,
                    pad_token_id=chat_tokenizer.eos_token_id
                )
        except Exception as gen_err:
            msg = str(gen_err)
            try:
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
            except Exception:
                pass
            gc.collect()
            if "CUDA out of memory" in msg or "CUDA" in msg:
                try:
                    with torch.no_grad():
                        output = chat_model.generate(
                            **inputs,
                            max_new_tokens=max(32, min(64, max_new)),
                            temperature=temp,
                            do_sample=True if temp > 0 else False,
                            pad_token_id=chat_tokenizer.eos_token_id
                        )
                except Exception as gen_err2:
                    return JSONResponse(status_code=500, content={"error": str(gen_err2)})
            else:
                return JSONResponse(status_code=500, content={"error": msg})
        generated_tokens = output[0][inputs.input_ids.shape[-1]:]
        response_text = chat_tokenizer.decode(generated_tokens, skip_special_tokens=True)
        del inputs, output
        torch.cuda.empty_cache()
        mode = (x_mode or ("pro" if (req.model or "").lower() == "pro" else "flash")).lower()
        rag_meta = {"used": mode == "pro", "retrieved": 0, "selected": 0}
        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "choices": [{"index": 0, "message": {"role": "assistant", "content": response_text}, "finish_reason": "stop"}],
            "mode": mode,
            "rag": rag_meta
        }
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# ==============================
# üî∂ 2. VISION API
# ==============================
@app.post("/v1/vision-multi")
async def vision_multi_api(req: VisionMultiRequest):
    try:
        full_response = []
        # Gi·ªõi h·∫°n x·ª≠ l√Ω 2 ·∫£nh ƒë·ªÉ tr√°nh tr√†n RAM
        for idx, b64_str in enumerate(req.images_base64[:2]):
            image_bytes = base64.b64decode(b64_str)
            image = Image.open(BytesIO(image_bytes)).convert("RGB")

            prompt = f"USER: <image>\n{req.text}\nASSISTANT:"
            inputs = vlm_processor(text=prompt, images=image, return_tensors="pt").to("cuda")

            with torch.no_grad():
                output = vlm_model.generate(**inputs, max_new_tokens=req.max_tokens)

            resp = vlm_processor.decode(output[0], skip_special_tokens=True).split("ASSISTANT:")[-1].strip()
            full_response.append(f"[·∫¢nh {idx+1}]: {resp}")

            del inputs, output, image
            torch.cuda.empty_cache()

        return {"success": True, "response": "\n".join(full_response)}
    except Exception as e:
        return {"success": False, "error": str(e)}

# ==============================
# üî∑ 3. TTS STREAMING (gTTS)
# ==============================
@app.post("/v1/tts/stream")
async def tts_stream_api(req: TTSRequest):
    async def generate_audio_stream():
        temp_filename = f"/content/tts_{uuid.uuid4().hex}.mp3"
        try:
            tts = gTTS(text=req.text, lang=req.lang)
            tts.save(temp_filename)
            chunk_size = 32 * 1024
            chunk_id = 0
            with open(temp_filename, "rb") as f:
                while True:
                    data = f.read(chunk_size)
                    if not data: break
                    b64_data = base64.b64encode(data).decode("utf-8")
                    yield json.dumps({"chunk_id": chunk_id, "audio_base64": b64_data}) + "\n"
                    chunk_id += 1
                    await asyncio.sleep(0.01)
            if os.path.exists(temp_filename): os.remove(temp_filename)
        except Exception as e:
            yield json.dumps({"error": str(e)}) + "\n"

    return StreamingResponse(generate_audio_stream(), media_type="application/x-ndjson")

# ==============================
# üî∂ 4. STT STREAMING (Faster-Whisper)
# ==============================
@app.post("/v1/stt/stream")
async def stt_stream_api(file: UploadFile = File(...)):
    async def transcribe_stream():
        temp_filename = f"/content/upload_{uuid.uuid4().hex}.wav"
        try:
            with open(temp_filename, "wb") as f:
                f.write(await file.read())

            # Faster-whisper tr·∫£ v·ªÅ generator
            segments, info = stt_model.transcribe(temp_filename, beam_size=5, language="vi")

            for segment in segments:
                payload = json.dumps({
                    "text": segment.text,
                    "start": segment.start,
                    "end": segment.end,
                    "partial": segment.text
                })
                yield payload + "\n"
                await asyncio.sleep(0.01)

            if os.path.exists(temp_filename): os.remove(temp_filename)
        except Exception as e:
            yield json.dumps({"error": str(e)}) + "\n"

    return StreamingResponse(transcribe_stream(), media_type="application/x-ndjson")

# ==============================
# üöÄ STARTUP
# ==============================
@app.get("/gpu/metrics")
async def gpu_metrics():
    # L·∫•y th√¥ng s·ªë GPU th·∫≠t
    try:
        r = subprocess.run(["nvidia-smi", "--query-gpu=utilization.gpu,memory.used,memory.total", "--format=csv,noheader,nounits"], capture_output=True, text=True)
        u, m_used, m_total = r.stdout.strip().split(",")
        return {"gpu_utilization": float(u), "mem_used": float(m_used), "mem_total": float(m_total)}
    except:
        return {}

@app.get("/health")
async def health():
    try:
        ok_chat = chat_model is not None
        ok_tokenizer = chat_tokenizer is not None
        ok_vlm = vlm_model is not None and vlm_processor is not None
        ok_stt = stt_model is not None
        return {"status": "ok", "chat": ok_chat, "tokenizer": ok_tokenizer, "vlm": ok_vlm, "stt": ok_stt}
    except Exception:
        return {"status": "degraded"}

@app.get("/")
async def root():
    return {"status": "ok"}

print("--- KH·ªûI ƒê·ªòNG SERVER (VRAM OPTIMIZED) ---")
if USE_NGROK and NGROK_AUTH_TOKEN:
    ngrok.set_auth_token(NGROK_AUTH_TOKEN)
    ngrok.kill()
    public_url = ngrok.connect(PORT)
    print(f"‚úÖ Public URL: {public_url.public_url}")
else:
    print("‚ö†Ô∏è Ch·∫°y Localhost")

def run_uvicorn():
    import uvicorn
    import asyncio
    try:
        config = uvicorn.Config(app, host="0.0.0.0", port=PORT, loop="asyncio", lifespan="on")
        server = uvicorn.Server(config)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(server.serve())
    except Exception as e:
        print("UVicorn start error:", str(e))

thread = threading.Thread(target=run_uvicorn, daemon=True)
thread.start()

try:
    while True: time.sleep(1)
except KeyboardInterrupt: pass
