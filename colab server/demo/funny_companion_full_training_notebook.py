# -*- coding: utf-8 -*-
"""funny_companion_full_training_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Loh18Cka4yAqg-cdZpGYmon2KkbWPOy

# CELL 0 â€” CÃ i thÆ° viá»‡n
"""

!pip install -q transformers accelerate peft datasets bitsandbytes trl sentencepiece
!pip install -q huggingface_hub

"""# CELL 1 â€” Import thÆ° viá»‡n"""

import os
import json
from glob import glob
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from peft import LoraConfig, get_peft_model
import torch
from huggingface_hub import notebook_login

"""# CELL 2 â€” Káº¿t ná»‘i Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

RAW_DIR = "/content/drive/MyDrive/DoctorAI - TroÌ›Ì£ lyÌ SuÌ›Ìc khoÌ‰e CaÌ nhaÌ‚n ThoÌ‚ng minh/tam_ly/raw_data"
CLEAN_DIR = "/content/drive/MyDrive/DoctorAI - TroÌ›Ì£ lyÌ SuÌ›Ìc khoÌ‰e CaÌ nhaÌ‚n ThoÌ‚ng minh/tam_ly"
os.makedirs(CLEAN_DIR, exist_ok=True)

# @title # CELL 3 â€” Load cÃ¡c file 6 cáº·p tá»« raw_data/ vÃ  merge thÃ nh 1 dataset_cleaned.jsonl

import os
import json
from glob import glob



# CÃ¡c file dáº¡ng topic_01_raw_6pairs.jsonl, topic_02_raw_6pairs.jsonl...
files = sorted(glob(f"{RAW_DIR}/topic_*_raw_6pairs.jsonl"))

print("ğŸ“ Found files:", files)

all_items = []

for f in files:
    print("ğŸ‘‰ Reading:", f)
    with open(f, "r", encoding="utf-8") as fr:
        for line in fr:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                # kiá»ƒm tra format chuáº©n
                if "messages" in obj and isinstance(obj["messages"], list):
                    all_items.append(obj)
            except Exception as e:
                print("âŒ Parse error:", f, e)
                continue

print("ğŸ“Š Tá»•ng sá»‘ há»™i thoáº¡i há»£p lá»‡:", len(all_items))

# LÆ°u file cleaned há»£p nháº¥t
clean_file = os.path.join(CLEAN_DIR, "dataset_cleaned.jsonl")
with open(clean_file, "w", encoding="utf-8") as fw:
    for item in all_items:
        fw.write(json.dumps(item, ensure_ascii=False) + "\n")

print("âœ” ÄÃ£ táº¡o file cleaned:", clean_file)

# @title # CELL 4 â€” PhÃ¢n tÃ­ch dataset cleaned

print("ğŸ“Š Tá»•ng há»™i thoáº¡i:", len(all_items))

lengths = [len(x["messages"]) for x in all_items]
avg_len = sum(lengths) / len(lengths)

print("ğŸ“ Sá»‘ messages trung bÃ¬nh / há»™i thoáº¡i:", avg_len)
print("ğŸ“ Tá»‘i thiá»ƒu:", min(lengths))
print("ğŸ“ Tá»‘i Ä‘a:", max(lengths))

"""# CELL 4 â€” PhÃ¢n tÃ­ch ngáº¯n dataset"""

print("Sá»‘ máº«u:", len(all_items))
lens = [len(x["messages"]) for x in all_items]
print("Sá»‘ messages trung bÃ¬nh:", sum(lens)/len(lens))

"""# CELL 5 â€” TRAIN Vá»šI LoRA"""

# ============================
# training_v3_full_for_unsloth.ipynb
# CELL 0 â€” Install libs (cháº¡y 1 láº§n)
# ============================
!pip install -q transformers accelerate peft datasets bitsandbytes trl sentencepiece safetensors huggingface_hub wandb

# ============================
# CELL 1 â€” Imports + constants
# ============================
import os, json, re
from glob import glob
from pathlib import Path
from typing import List, Dict

import datasets
from datasets import load_dataset

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
)
from transformers import BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

# --- Paths (chá»‰nh náº¿u cáº§n) ---
RAW_DIR = "/content/drive/MyDrive/DoctorAI - Trá»£ lÃ½ Sá»©c khá»e CÃ¡ nhÃ¢n ThÃ´ng minh/tam_ly/raw_data"
CLEAN_DIR = "/content/drive/MyDrive/DoctorAI - Trá»£ lÃ½ Sá»©c khá»e CÃ¡ nhÃ¢n ThÃ´ng minh/tam_ly"
os.makedirs(CLEAN_DIR, exist_ok=True)

# Model to use
MODEL_NAME = "unsloth/Llama-3.2-1B-Instruct"

# Grouping: má»—i dialogue = N_pairs (N_pairs * 2 messages)
PAIRS_PER_DIALOGUE = 6
MAX_LENGTH = 1024

print("RAW_DIR:", RAW_DIR)
print("CLEAN_DIR:", CLEAN_DIR)
print("MODEL_NAME:", MODEL_NAME)

# ============================
# CELL 2 â€” Utility: extract messages robustly from file text
# - há»— trá»£ multi-line JSON, pretty-print, broken lines
# - cá»‘ gáº¯ng báº¯t "role" & "content" báº±ng regex linh hoáº¡t
# ============================
def extract_messages_from_text(text: str) -> List[Dict]:
    """
    TrÃ­ch role+content tá»« má»™t vÄƒn báº£n báº¥t ká»³ báº±ng regex an toÃ n.
    Tráº£ vá» list cÃ¡c dict {"role":..., "content":...} theo thá»© tá»± xuáº¥t hiá»‡n.
    """
    messages = []
    # Regex an toÃ n: tÃ¬m "role": "xxx" vÃ  "content": "yyy" (allow escaped quotes)
    pattern = re.compile(r'"role"\s*:\s*"([^"]+)"\s*[,}].*?"content"\s*:\s*"((?:[^"\\]|\\.)*)"', re.DOTALL)
    for m in pattern.finditer(text):
        role = m.group(1).strip()
        content = m.group(2).encode('utf-8').decode('unicode_escape')  # unescape
        content = content.replace("\n", " ").strip()
        messages.append({"role": role, "content": content})
    return messages

def load_messages_from_file(path: str) -> List[Dict]:
    """
    Náº¿u file lÃ  JSONL (má»—i dÃ²ng 1 object) thÃ¬ parse dÃ²ng;
    else fallback Ä‘á»c toÃ n file vÃ  extract báº±ng regex.
    """
    msgs = []
    try:
        with open(path, "r", encoding="utf-8") as f:
            # quick check: if file looks like JSONL (many lines each starting with {"messages")
            sample = f.read(1000)
            f.seek(0)
            if "\n" in sample and '"messages"' in sample:
                # treat as JSONL or 1-line JSONL
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        if isinstance(obj, dict) and "messages" in obj:
                            for m in obj["messages"]:
                                if isinstance(m, dict) and "role" in m and "content" in m:
                                    msgs.append({"role": m["role"], "content": m["content"]})
                        else:
                            # fallback: try extract role/content inside this line
                            msgs += extract_messages_from_text(line)
                    except Exception:
                        # fallback to regex extraction on the line
                        msgs += extract_messages_from_text(line)
            else:
                # not JSONL-like -> read whole file and extract
                full = f.read()
                msgs = extract_messages_from_text(full)
    except Exception as e:
        print("Error reading", path, e)
    return msgs

# @title # CELL 3 â€” Xá»­ lÃ½ dá»¯ liá»‡u theo logic clear.py
import os
import json
import re
from glob import glob

# --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ---
# HÃ£y Ä‘áº£m báº£o RAW_DIR trá» Ä‘Ãºng tá»›i thÆ° má»¥c chá»©a cÃ¡c file topic_*.jsonl trÃªn Drive
RAW_DIR = "/content/drive/MyDrive/DoctorAI - TroÌ›Ì£ lyÌ SuÌ›Ìc khoÌ‰e CaÌ nhaÌ‚n ThoÌ‚ng minh/tam_ly/raw_data"
CLEAN_DIR = "/content/drive/MyDrive/DoctorAI - TroÌ›Ì£ lyÌ SuÌ›Ìc khoÌ‰e CaÌ nhaÌ‚n ThoÌ‚ng minh/tam_ly"
os.makedirs(CLEAN_DIR, exist_ok=True)

# --- HÃ€M Tá»ª FILE CLEAR.PY ---
def extract_messages_safely(text):
    """
    TrÃ­ch táº¥t cáº£ messages tá»« file báº±ng Regex (logic tá»« clear.py)
    """
    pattern = r'"role"\s*:\s*"([^"]+)"\s*,\s*"content"\s*:\s*"((?:[^"\\]|\\.)*)"'
    matches = re.findall(pattern, text, flags=re.DOTALL)

    messages = []
    for role, content in matches:
        # Xá»­ lÃ½ unescape kÃ½ tá»± unicode náº¿u cáº§n, vÃ  xÃ³a xuá»‘ng dÃ²ng thá»«a
        # Logic gá»‘c cá»§a clear.py: content.replace('\n', ' ').strip()
        cleaned_content = content.replace('\\n', ' ').replace('\n', ' ').strip()

        # Chuáº©n hÃ³a role cho Llama 3 (user/assistant)
        # clear.py giá»¯ nguyÃªn role gá»‘c, nhÆ°ng khi train cáº§n chuáº©n hÃ³a
        role_norm = "user" if "user" in role.lower() else "assistant"

        messages.append({
            "role": role_norm,
            "content": cleaned_content
        })
    return messages

def group_into_6pair_dialogues(messages):
    """
    Gom thÃ nh cÃ¡c há»™i thoáº¡i, má»—i há»™i thoáº¡i chá»©a 6 cáº·p (12 messages).
    """
    dialogues = []
    buffer = []
    pair_count = 0

    for msg in messages:
        buffer.append(msg)

        # Ä‘á»§ 2 messages (1 user, 1 assistant) = 1 cáº·p
        if len(buffer) % 2 == 0:
            pair_count += 1

        # Ä‘á»§ 6 cáº·p = 12 messages
        if pair_count == 6:
            dialogues.append({"messages": buffer.copy()})
            buffer.clear()
            pair_count = 0

    return dialogues

# --- THá»°C THI ---
files = sorted(glob(f"{RAW_DIR}/*.jsonl"))
print(f"ğŸ“ TÃ¬m tháº¥y {len(files)} files raw.")

all_dialogues = []

for f in files:
    try:
        with open(f, "r", encoding="utf-8") as fr:
            text = fr.read()

        # 1. TrÃ­ch xuáº¥t
        msgs = extract_messages_safely(text)

        # 2. Gom nhÃ³m
        dialogs = group_into_6pair_dialogues(msgs)

        if len(dialogs) > 0:
            all_dialogues.extend(dialogs)
            print(f"  âœ” {os.path.basename(f)}: Extracted {len(msgs)} msgs -> {len(dialogs)} dialogues")
        else:
            print(f"  âš ï¸ {os.path.basename(f)}: KhÃ´ng táº¡o Ä‘Æ°á»£c há»™i thoáº¡i nÃ o (Messages: {len(msgs)})")

    except Exception as e:
        print(f"  âŒ Lá»—i Ä‘á»c file {f}: {e}")

print(f"\nğŸ“Š Tá»”NG Cá»˜NG: {len(all_dialogues)} há»™i thoáº¡i sáº¡ch (má»—i há»™i thoáº¡i 12 tin nháº¯n).")

# LÆ°u file káº¿t quáº£
clean_file = os.path.join(CLEAN_DIR, "dataset_cleaned.jsonl")
with open(clean_file, "w", encoding="utf-8") as fw:
    for d in all_dialogues:
        fw.write(json.dumps(d, ensure_ascii=False) + "\n")

print(f"ğŸ’¾ ÄÃ£ lÆ°u dataset sáº¡ch táº¡i: {clean_file}")

# ============================
# CELL 4 â€” Quick validation & stats
# ============================
from collections import Counter
lens = [len(d["messages"]) for d in all_dialogues]
print("Dialogues:", len(all_dialogues))
print("Messages per dialog - min/avg/max:", min(lens), sum(lens)/len(lens), max(lens))
# check sample
print("Sample dialogue 0:", all_dialogues[0])

# @title # CELL 5 â€” Tokenize & Prepare Dataset
from datasets import Dataset

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Táº¡o Dataset tá»« list dictionary Ä‘Ã£ xá»­ lÃ½ á»Ÿ Cell 3
hf_dataset = Dataset.from_list(all_dialogues)

def formatting_prompts_func(example):
    """
    Sá»­ dá»¥ng chat template chuáº©n cá»§a model (Llama-3).
    Input: list of dicts [{'role': 'user', 'content': '...'}, ...]
    Output: String Ä‘Ã£ Ä‘Æ°á»£c format vá»›i special tokens (<|start_header_id|>, v.v.)
    """
    msgs = example["messages"]

    # apply_chat_template sáº½ tá»± Ä‘á»™ng thÃªm cÃ¡c token há»‡ thá»‘ng cá»§a Llama 3
    # tokenize=False Ä‘á»ƒ tráº£ vá» string trÆ°á»›c, sau Ä‘Ã³ tokenizer sáº½ xá»­ lÃ½ sau
    text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)

    return {"text": text}

# BÆ°á»›c 1: Format thÃ nh text string chuáº©n Llama 3
hf_dataset = hf_dataset.map(formatting_prompts_func, batched=False)

# BÆ°á»›c 2: Tokenize
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=MAX_LENGTH,
        padding="max_length", # Pad Ä‘á»ƒ batch training á»•n Ä‘á»‹nh hÆ¡n
    )

tokenized_datasets = hf_dataset.map(tokenize_function, batched=True, remove_columns=["messages", "text"])

# GÃ¡n labels = input_ids (cho Causal LM training)
tokenized_datasets = tokenized_datasets.map(lambda x: {"labels": x["input_ids"]}, batched=True)

print("âœ… ÄÃ£ tokenize xong. Cáº¥u trÃºc máº«u:", tokenized_datasets[0].keys())

# ============================
# CELL 6 â€” Load model (4-bit via BitsAndBytesConfig) + apply LoRA
# ============================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, # Changed to 4-bit quantization
    bnb_4bit_quant_type="nf4", # Recommended type for 4-bit
    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for computation
    bnb_4bit_use_double_quant=True, # Double quantization for further memory savings
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map={"": 0}, # Explicitly map to GPU 0
    quantization_config=bnb_config,
    trust_remote_code=True
)

# LoRA config (tuneable)
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, peft_config)

# Enable gradient checkpointing for input gradients with PEFT models
model.enable_input_require_grads()

print("Model loaded and LoRA applied.")

# ============================
# CELL 7 â€” Data collator & TrainingArguments
# ============================
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding=True,
    max_length=MAX_LENGTH,
    label_pad_token_id=-100,
    return_tensors="pt",
)

training_args = TrainingArguments(
    output_dir="/content/output_model",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    logging_steps=50,
    save_steps=500,
    bf16=False,
    fp16=True,
    optim="paged_adamw_8bit",
    remove_unused_columns=False,
    gradient_checkpointing=True, # Enabled gradient checkpointing to save memory
    report_to="none"  # set "wandb" to enable
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=hf_dataset,
    data_collator=data_collator,
)

print("Trainer ready.")

# ============================
# CELL 8 â€” Sanity checks before train
# ============================
# show a sample tensorization
sample = hf_dataset[0]
print("sample input_ids len:", len(sample["input_ids"]))
print("sample labels len:", len(sample["labels"]))
# check for empty samples
bad = [i for i, x in enumerate(hf_dataset) if len(x["input_ids"]) == 0]
print("bad examples count:", len(bad))

# @title # CELL 9 â€” Báº¯t Ä‘áº§u Training & LÆ°u Model
import torch

# 1. Báº¯t Ä‘áº§u train
print("ğŸš€ Äang báº¯t Ä‘áº§u training... (Báº¡n cÃ³ thá»ƒ Ä‘i pha cÃ  phÃª â˜•)")
trainer.train()

# 2. LÆ°u model LoRA vÃ  Tokenizer xuá»‘ng Ä‘Ä©a
OUTPUT_DIR = "/content/lora_model"
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("="*50)
print(f"âœ… ÄÃ£ huáº¥n luyá»‡n xong!")
print(f"ğŸ’¾ Model LoRA vÃ  Tokenizer Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {OUTPUT_DIR}")
print("="*50)

# @title # CELL 10 â€” Test Model & Upload lÃªn Hugging Face
from transformers import pipeline, TextStreamer
from huggingface_hub import login, HfApi

# --- PHáº¦N 1: TEST MODEL NHANH ---
print("ğŸ§ª Äang test thá»­ model vá»«a train...")

# Sá»­ dá»¥ng model Ä‘ang cÃ³ trong bá»™ nhá»› (trainer.model) Ä‘á»ƒ test ngay
# Thiáº¿t láº­p streamer Ä‘á»ƒ chá»¯ hiá»‡n ra dáº§n dáº§n (giá»‘ng ChatGPT)
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

def test_chat(prompt_text):
    # Táº¡o Ä‘á»‹nh dáº¡ng tin nháº¯n theo chuáº©n Llama 3
    messages = [
        {"role": "user", "content": prompt_text}
    ]

    # Format input
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to("cuda")

    print(f"\nUser: {prompt_text}")
    print("Assistant: ", end="")

    # Generate
    _ = model.generate(
        inputs,
        max_new_tokens=256,
        streamer=streamer,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.7, # Chá»‰nh Ä‘á»™ sÃ¡ng táº¡o (0.1 - 1.0)
        do_sample=True,
        top_p=0.9
    )

# --- THá»¬ NGHIá»†M Vá»šI 1 CÃ‚U Há»I MáºªU ---
test_chat("Sáº¿p báº¯t tao lÃ m thÃªm giá» mÃ  khÃ´ng tráº£ lÆ°Æ¡ng, tao pháº£i lÃ m sao?")

# --- PHáº¦N 2: UPLOAD LÃŠN HUGGING FACE ---
# Bá» comment cÃ¡c dÃ²ng dÆ°á»›i Ä‘Ã¢y khi báº¡n muá»‘n upload

# print("\n" + "="*30)
# print("â˜ï¸ Báº®T Äáº¦U UPLOAD LÃŠN HUGGING FACE")
# print("="*30)

# 1. ÄÄƒng nháº­p (Sáº½ hiá»‡n khung nháº­p Token náº¿u chÆ°a login)
# login()

# 2. Äáº·t tÃªn Repo cá»§a báº¡n (Thay 'YourUsername' báº±ng tÃªn user HF cá»§a báº¡n)
# HF_USERNAME = "YourUsername"
# REPO_NAME = "funny-empathetic-companion-v1"
# FULL_REPO_ID = f"{HF_USERNAME}/{REPO_NAME}"

# print(f"ang Ä‘áº©y model lÃªn: {FULL_REPO_ID} ...")

# 3. Push model (LoRA Adapter) & Tokenizer
# model.push_to_hub(FULL_REPO_ID, use_auth_token=True)
# tokenizer.push_to_hub(FULL_REPO_ID, use_auth_token=True)

# print(f"âœ… ÄÃ£ upload thÃ nh cÃ´ng! Link: https://huggingface.co/{FULL_REPO_ID}")