# -*- coding: utf-8 -*-
"""server-AI-MCS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J37KPSwpYoSYz9lhVJyYvSnACqleIXpG
"""

!pip install transformers accelerate sentencepiece bitsandbytes
!pip install TTS  # XTTS-v2
!pip install git+https://github.com/openai/whisper.git
!pip install torchvision pillow

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

CHAT_MODEL = "unsloth/Llama-3.2-3B"

chat_tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL)
chat_model = AutoModelForCausalLM.from_pretrained(
    CHAT_MODEL,
    torch_dtype=torch.float16,
    device_map="auto"
)

from transformers import LlavaNextForConditionalGeneration, AutoProcessor

VLM_MODEL = "llava-hf/llava-1.5-7b-hf"

vlm_processor = AutoProcessor.from_pretrained(VLM_MODEL)
vlm_model = LlavaNextForConditionalGeneration.from_pretrained(
    VLM_MODEL,
    torch_dtype=torch.float16,
    device_map="auto"
)

import whisper

stt_model = whisper.load_model("large-v3").to("cuda")

# @title üíä M√°y "Ch·ªØa L√†nh" Gi·ªçng S√≥c Chu·ªôt (Full Code)
# 1. C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng (Ch·∫°y ng·∫ßm ƒë·ªÉ kh√¥ng r·ªëi m·∫Øt)
import os
print("‚è≥ ƒêang c√†i ƒë·∫∑t th∆∞ vi·ªán... (Ch·ªù x√≠u nh√©)")
os.system("apt-get install -y ffmpeg > /dev/null")
os.system("pip install -q gTTS librosa soundfile")

import random
from gtts import gTTS
import librosa
import soundfile as sf
import numpy as np
from IPython.display import Audio, display

# --- 2. C·∫§U H√åNH N·ªòI DUNG & HI·ªÜU ·ª®NG ---

# Danh s√°ch c√°c c√¢u "li·ªáu ph√°p t√¢m l√Ω" h√†i h∆∞·ªõc
healing_quotes = [
    "A l√¥! ƒê·ª´ng √°p l·ª±c n·ªØa. Vi·ªác h√¥m nay ch∆∞a xong th√¨... mai l√†m. Tr√°i ƒë·∫•t v·∫´n quay m√†!",
    "H√≠t v√†o th·∫≠t s√¢u... Th·ªü ra h·∫øt n·ªói bu·ªìn... Ch·ªâ gi·ªØ l·∫°i ti·ªÅn v√† ƒë·ªì ƒÉn ngon th√¥i nh√©.",
    "B·∫°n g√¨ ∆°i, deadline ch·ªâ l√† h∆∞ v√¥. S·ª©c kh·ªèe m·ªõi l√† vƒ©nh c·ª≠u. ƒêi ng·ªß s·ªõm ƒëi!",
    "C·ªë l√™n! Cu·ªôc ƒë·ªùi l√† nh·ªØng c√∫ t√°t, nh∆∞ng m√¨nh c√≥ th·ªÉ n√© m√†. C∆∞·ªùi l√™n c√°i xem n√†o!",
    "Alo t·ªïng ƒë√†i v≈© tr·ª• ƒë√¢y. Ch√∫ng t√¥i x√°c nh·∫≠n b·∫°n r·∫•t tuy·ªát v·ªùi, ch·ªâ h∆°i thi·∫øu ng·ªß ch√∫t th√¥i."
]

# Ch·ªçn ng·∫´u nhi√™n 1 c√¢u
text_input = random.choice(healing_quotes)

# C·∫•u h√¨nh ƒë·ªô "H√†i h∆∞·ªõc"
SPEED_RATE = 1.3  # T·ªëc ƒë·ªô ( > 1.0 l√† nhanh)
PITCH_STEPS = 5   # ƒê·ªô cao ( > 0 l√† gi·ªçng tr·∫ª con/s√≥c chu·ªôt)

# --- 3. X·ª¨ L√ù CH√çNH ---

def create_funny_therapy(text):
    print(f"\nüìù Bot ƒëang ƒë·ªçc: \"{text}\"")

    # B1: T·∫°o gi·ªçng g·ªëc (Google TTS - ·ªîn ƒë·ªãnh nh·∫•t)
    temp_file = "temp_voice.mp3"
    try:
        tts = gTTS(text=text, lang='vi')
        tts.save(temp_file)
    except Exception as e:
        print("‚ùå L·ªói k·∫øt n·ªëi Google. Ki·ªÉm tra l·∫°i m·∫°ng nh√©.")
        return None

    # B2: X·ª≠ l√Ω √¢m thanh (Bi·∫øn gi·ªçng)
    try:
        # Load file
        y, sr = librosa.load(temp_file, sr=None)

        # TƒÉng t·ªëc
        y_fast = librosa.effects.time_stretch(y, rate=SPEED_RATE)

        # TƒÉng tone
        y_funny = librosa.effects.pitch_shift(y_fast, sr=sr, n_steps=PITCH_STEPS)

        # L∆∞u file k·∫øt qu·∫£
        output_file = "giong_giam_stress.wav"
        sf.write(output_file, y_funny, sr)

        return output_file, sr
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω √¢m thanh: {e}")
        return None

# --- 4. CH·∫†Y V√Ä PH√ÅT ---

result = create_funny_therapy(text_input)

if result:
    file_path, rate = result
    print("\n(Ôæâ‚óï„ÉÆ‚óï)Ôæâ*:ÔΩ•Ôæü‚úß Xong r·ªìi! M·ªùi b·∫°n nghe li·ªÅu thu·ªëc tinh th·∫ßn n√†y:")
    display(Audio(file_path, rate=rate, autoplay=True)) # T·ª± ƒë·ªông ph√°t

# ==========================
# üìå CELL 4 ‚Äî FULL API SERVER
# ==========================

import base64
import uuid
import os
from io import BytesIO
from fastapi import FastAPI, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import asyncio
import json
import subprocess
from pydantic import BaseModel
from PIL import Image
import torch
from fastapi.responses import FileResponse

app = FastAPI(title="Medical Consultation GPU API", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==============================
# üî∂ REQUEST MODELS
# ==============================

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: list[ChatMessage]

class VisionChatRequest(BaseModel):
    text: str
    image_base64: str

class VisionMultiRequest(BaseModel):
    text: str
    images_base64: list[str]

class TextToSpeechRequest(BaseModel):
    text: str

class TextToSpeechStreamRequest(BaseModel):
    text: str


# ==============================
# üî∑ 1) CHAT COMPLETION API
# ==============================

@app.post("/v1/chat")
async def chat_api(req: ChatRequest):

    user_msg = req.messages[-1].content

    inputs = chat_tokenizer(user_msg, return_tensors="pt").to("cuda")
    output = chat_model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7
    )

    text = chat_tokenizer.decode(output[0], skip_special_tokens=True)
    return {"reply": text}

@app.post("/v1/chat/completions")
async def chat_completions(req: ChatRequest):
    user_msg = req.messages[-1].content
    inputs = chat_tokenizer(user_msg, return_tensors="pt").to("cuda")
    output = chat_model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7
    )
    text = chat_tokenizer.decode(output[0], skip_special_tokens=True)
    return {
        "choices": [
            {
                "message": {
                    "role": "assistant",
                    "content": text
                }
            }
        ]
    }


# ==============================
# üî∂ 2) VISION-LANGUAGE API (LLAVA)
# ==============================

@app.post("/v1/vision")
async def vision_api(req: VisionChatRequest):

    # Decode base64 ‚Üí image
    image_bytes = base64.b64decode(req.image_base64)
    image = Image.open(BytesIO(image_bytes)).convert("RGB")

    inputs = vlm_processor(
        text=req.text,
        images=image,
        return_tensors="pt"
    ).to("cuda")

    output = vlm_model.generate(**inputs, max_new_tokens=256)
    text = vlm_processor.tokenizer.decode(output[0], skip_special_tokens=True)

    return {"reply": text}

@app.post("/v1/vision-multi")
async def vision_multi_api(req: VisionMultiRequest):
    replies = []
    for b64 in req.images_base64:
        image_bytes = base64.b64decode(b64)
        image = Image.open(BytesIO(image_bytes)).convert("RGB")
        inputs = vlm_processor(text=req.text, images=image, return_tensors="pt").to("cuda")
        output = vlm_model.generate(**inputs, max_new_tokens=256)
        t = vlm_processor.tokenizer.decode(output[0], skip_special_tokens=True)
        replies.append(t)
    return {"reply": "\n".join(replies)}


# ==============================
# üî∑ 3) TEXT TO SPEECH (XTTS v2)
# ==============================

@app.post("/v1/tts")
async def tts_api(req: TextToSpeechRequest):

    file_id = uuid.uuid4().hex
    out_path = f"/content/{file_id}.wav"

    tts.tts_to_file(
        text=req.text,
        file_path=out_path
    )

    return {
        "audio_url": f"/files/{file_id}.wav"
    }

@app.post("/v1/tts/stream")
async def tts_stream_api(req: TextToSpeechStreamRequest):
    async def gen():
        text = req.text.strip()
        parts = []
        buf = []
        for ch in text:
            buf.append(ch)
            if len(buf) >= 180 or ch in ".!?\n":
                parts.append("".join(buf).strip())
                buf = []
        if buf:
            parts.append("".join(buf).strip())
        idx = 0
        for p in parts:
            file_id = uuid.uuid4().hex
            out_path = f"/content/{file_id}.wav"
            tts.tts_to_file(text=p, file_path=out_path)
            with open(out_path, "rb") as f:
                b = f.read()
            payload = {"chunk_id": idx, "audio_base64": base64.b64encode(b).decode("ascii")}
            idx += 1
            yield json.dumps(payload) + "\n"
            await asyncio.sleep(0)
    return StreamingResponse(gen(), media_type="application/json")


# Serve audio files
@app.get("/files/{filename}")
async def get_audio(filename: str):
    path = f"/content/{filename}"
    return FileResponse(path, media_type="audio/wav")

@app.get("/health")
async def health():
    return {"status": "ok"}


# ==============================
# üî∂ 4) SPEECH TO TEXT (WHISPER)
# ==============================

@app.post("/v1/stt")
async def stt_api(file: UploadFile = File(...)):

    # Save uploaded file
    temp_path = "/content/input_audio.wav"
    with open(temp_path, "wb") as f:
        f.write(await file.read())

    # Transcribe using Whisper
    result = stt_model.transcribe(temp_path, language="vi")

    return {"text": result["text"]}

@app.post("/v1/stt/stream")
async def stt_stream_api(file: UploadFile = File(...)):
    temp_path = "/content/input_audio_stream.wav"
    with open(temp_path, "wb") as f:
        f.write(await file.read())
    full = stt_model.transcribe(temp_path, language="vi")
    text = full["text"]
    async def gen():
        tokens = text.split()
        cur = []
        for i, t in enumerate(tokens):
            cur.append(t)
            if len(cur) >= 12 or i == len(tokens) - 1:
                yield json.dumps({"partial": " ".join(cur)}) + "\n"
                cur = []
                await asyncio.sleep(0)
    return StreamingResponse(gen(), media_type="application/json")

@app.get("/gpu/metrics")
async def gpu_metrics():
    data = {}
    try:
        q = [
            "nvidia-smi",
            "--query-gpu=temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.used",
            "--format=csv,noheader,nounits",
        ]
        r = subprocess.run(q, capture_output=True, text=True)
        if r.returncode == 0:
            s = r.stdout.strip().split(",")
            data["gpu_temperature"] = float(s[0])
            data["gpu_utilization"] = float(s[1])
            data["mem_utilization"] = float(s[2])
            data["mem_total"] = float(s[3])
            data["mem_used"] = float(s[4])
    except Exception:
        pass
    return data


# ==============================
# üî∑ SERVER READY
# ==============================

print(f"üöÄ API Ready! Chat / Vision / TTS / STT ƒëang ch·∫°y tr√™n PORT {PORT}")

# CELL 5 ‚Äî START SERVER with PYNGROK (v2)

print("--- Kh·ªüi Ch·∫°y Server ---")

if IN_COLAB and USE_NGROK:
    try:
        token = NGROK_AUTH_TOKEN  # l·∫•y t·ª´ cell 2
        if token:
            print("üîë NGROK_AUTH_TOKEN ƒë√£ s·∫µn s√†ng.")

            # Kill process c≈© ƒë·ªÉ tr√°nh l·ªói ERR_NGROK_105
            ngrok.kill()

            import threading, time
            def run_server():
                import uvicorn
                uvicorn.run(app, host="0.0.0.0", port=PORT)
            th = threading.Thread(target=run_server, daemon=True)
            th.start()
            time.sleep(1)

            # M·ªü tunnel cho PORT ƒë√£ ch·ªçn
            public_url = ngrok.connect(PORT)
            print(f"‚úÖ Server URL (Ngrok): {public_url.public_url}")
            print(f"üëâ Copy URL n√†y d√°n v√†o Agent.")
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y token")

    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói Ngrok: {e}")
        print("‚ö†Ô∏è Server s·∫Ω ch·∫°y Localhost")

else:
    print("üåê Kh√¥ng ch·∫°y Colab ho·∫∑c USE_NGROK = False ‚Üí Server ch·∫°y Localhost")