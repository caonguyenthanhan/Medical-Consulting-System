{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZnU_zhclTUuY"
      },
      "outputs": [],
      "source": [
        "# Cell 1: install dependencies (ch·∫°y 1 l·∫ßn)\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(pkg):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "# FastAPI + uvicorn + ngrok + transformers/hf\n",
        "pip_install(\"fastapi\")\n",
        "pip_install(\"uvicorn[standard]\")\n",
        "pip_install(\"pyngrok\")\n",
        "pip_install(\"huggingface_hub\")\n",
        "pip_install(\"transformers>=4.30.0\")\n",
        "pip_install(\"accelerate\")\n",
        "pip_install(\"safetensors\")\n",
        "# Optional fallback llama.cpp (n·∫øu mu·ªën d√πng GGUF local)\n",
        "pip_install(\"llama-cpp-python\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: c·∫•u h√¨nh (ch·ªânh theo nhu c·∫ßu)\n",
        "import os\n",
        "\n",
        "# Hugging Face model id (text-generation / causal). Thay b·∫±ng model b·∫°n mu·ªën.\n",
        "# V√≠ d·ª•: \"gpt2\" (test), ho·∫∑c m·ªôt model l·ªõn h∆°n: \"meta-llama/Llama-2-13b-chat-hf\" (c·∫ßn token + VRAM)\n",
        "HF_MODEL = os.environ.get(\"HF_MODEL\", \"gpt2\")\n",
        "HF_TOKEN = os.environ.get(\"HF_HUB_TOKEN\", \"\")  # recommended: set trong Colab runtime env\n",
        "PORT = int(os.environ.get(\"PORT\", 8000))\n",
        "USE_NGROK = True\n",
        "NGROK_TOKEN = os.environ.get(\"NGROK_TOKEN\", \"\")\n",
        "\n",
        "# Fallback GGUF path (local) n·∫øu HF kh√¥ng kh·∫£ d·ª•ng\n",
        "GGUF_PATH = os.environ.get(\"GGUF_PATH\", \"/content/models/model.gguf\")\n"
      ],
      "metadata": {
        "id": "0Dt2yk5pV6Or"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load Hugging Face model ƒë√∫ng c√°ch v·ªõi accelerate (KH√îNG truy·ªÅn device cho pipeline)\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Torch CUDA available:\", torch.cuda.is_available(), \"device:\", device)\n",
        "\n",
        "tokenizer = None\n",
        "text_gen = None\n",
        "llama_llm = None\n",
        "\n",
        "try:\n",
        "    if HF_TOKEN:\n",
        "        login(HF_TOKEN)\n",
        "\n",
        "    print(\"Loading HF model:\", HF_MODEL)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL, use_fast=True)\n",
        "\n",
        "    # Load model b·∫±ng accelerate ‚Üí KH√îNG .to(\"cuda\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        HF_MODEL,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    # ‚ùó‚ùó IMPORTANT: KH√îNG TRUY·ªÄN device=0 CHO PIPELINE (accelerate model b·ªã crash)\n",
        "    if torch.cuda.is_available():\n",
        "        generator_device = 0\n",
        "    else:\n",
        "        generator_device = -1\n",
        "\n",
        "    text_gen = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        # KH√îNG truy·ªÅn device v√¨ accelerate ƒë√£ qu·∫£n l√Ω thi·∫øt b·ªã r·ªìi\n",
        "        return_full_text=False\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ HF model loaded OK (accelerate + pipeline)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è HF model failed:\", e)\n",
        "    # b·ªè fallback n·∫øu b·∫°n kh√¥ng d√πng GGUF\n",
        "    llama_llm = None\n",
        "\n",
        "if text_gen is None and llama_llm is None:\n",
        "    raise RuntimeError(\"No model available ‚Äî HF failed and no GGUF provided.\")\n"
      ],
      "metadata": {
        "id": "LsYeOoMBV6GL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2a7d20-7101-47d0-f048-b311b330ac8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch CUDA available: True device: 0\n",
            "Loading HF model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ HF model loaded OK (accelerate + pipeline)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: FastAPI server v·ªõi endpoint /v1/chat/completions\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "import asyncio\n",
        "import json\n",
        "\n",
        "app = FastAPI()\n",
        "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
        "\n",
        "def messages_to_prompt(messages):\n",
        "    \"\"\"\n",
        "    Chuy·ªÉn danh s√°ch messages [{'role':'system'/'user'/'assistant','content':...}]\n",
        "    th√†nh prompt text d·∫°ng ƒë∆°n gi·∫£n. B·∫°n c√≥ th·ªÉ thay ƒë·ªïi policy prompt ·ªü ƒë√¢y.\n",
        "    \"\"\"\n",
        "    if not messages:\n",
        "        return \"\"\n",
        "    parts = []\n",
        "    for m in messages:\n",
        "        role = m.get(\"role\",\"user\")\n",
        "        content = m.get(\"content\",\"\")\n",
        "        if role == \"system\":\n",
        "            parts.append(f\"[SYSTEM]\\n{content}\\n\")\n",
        "        elif role == \"user\":\n",
        "            parts.append(f\"User: {content}\\n\")\n",
        "        else:\n",
        "            parts.append(f\"Assistant: {content}\\n\")\n",
        "    # k·∫øt h·ª£p v√† th√™m instruction cho assistant\n",
        "    return \"\\n\".join(parts) + \"\\nAssistant:\"\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\"status\":\"ok\", \"model_loaded\": bool(text_gen or llama_llm)}\n",
        "\n",
        "@app.post(\"/v1/chat/completions\")\n",
        "async def chat_completions(request: Request):\n",
        "    payload = await request.json()\n",
        "    # H·ªó tr·ª£ c·∫£ 'messages' (OpenAI style) v√† 'prompt' (simple)\n",
        "    messages = payload.get(\"messages\")\n",
        "    prompt = payload.get(\"prompt\") or payload.get(\"question\") or payload.get(\"message\")\n",
        "    system = payload.get(\"system\", \"\")\n",
        "    temperature = float(payload.get(\"temperature\", 0.2))\n",
        "    max_tokens = int(payload.get(\"max_tokens\", 256))\n",
        "\n",
        "    if messages:\n",
        "        # ensure system message first if provided separately\n",
        "        if system:\n",
        "            messages = [{\"role\":\"system\",\"content\":system}] + messages\n",
        "        text_prompt = messages_to_prompt(messages)\n",
        "    else:\n",
        "        # build minimal prompt\n",
        "        text_prompt = ((\"[SYSTEM]\\n\" + system + \"\\n\") if system else \"\") + str(prompt or \"\")\n",
        "\n",
        "    # HF pipeline generation\n",
        "    try:\n",
        "        if text_gen:\n",
        "            gen = text_gen(\n",
        "                text_prompt,\n",
        "                do_sample=temperature>0,\n",
        "                temperature=temperature,\n",
        "                max_new_tokens=max_tokens,\n",
        "                top_p=0.9,\n",
        "                eos_token_id=tokenizer.eos_token_id if tokenizer is not None else None,\n",
        "            )\n",
        "            out_text = gen[0][\"generated_text\"]\n",
        "            # If pipeline returns full prompt + generation, we can strip prompt prefix\n",
        "            if out_text.startswith(text_prompt):\n",
        "                out_text = out_text[len(text_prompt):].strip()\n",
        "            return {\"choices\":[{\"message\":{\"role\":\"assistant\",\"content\":out_text}}]}\n",
        "        elif llama_llm:\n",
        "            # llama_cpp style: either create_chat_completion or call\n",
        "            try:\n",
        "                r = llama_llm.create_chat_completion(messages=[m for m in (messages or [{\"role\":\"user\",\"content\":text_prompt}])], temperature=temperature, max_tokens=max_tokens)\n",
        "                out_text = r[\"choices\"][0][\"message\"][\"content\"]\n",
        "            except Exception:\n",
        "                # fallback: simple call\n",
        "                r = llama_llm(text_prompt, max_tokens=max_tokens, temperature=temperature)\n",
        "                out_text = r.get(\"choices\", [{}])[0].get(\"text\",\"\")\n",
        "            return {\"choices\":[{\"message\":{\"role\":\"assistant\",\"content\":out_text}}]}\n",
        "        else:\n",
        "            return {\"error\":\"No model available\"}, 503\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Generation failed: {str(e)}\"}, 500\n",
        "\n",
        "# Run uvicorn server in background (so notebook doesn't block)\n",
        "def _run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")\n",
        "\n",
        "import threading\n",
        "t = threading.Thread(target=_run_server, daemon=True)\n",
        "t.start()\n",
        "print(\"FastAPI server started on port\", PORT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0oyYqlgV4cc",
        "outputId": "18b0983f-3214-4587-a8f5-a54aa39385f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI server started on port 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"NGROK_TOKEN\"] = \"YOUR_REAL_TOKEN_HERE\"\n",
        "NGROK_TOKEN = \"YOUR_REAL_TOKEN_HERE\"\n"
      ],
      "metadata": {
        "id": "Ys0-Dz7n5hu8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import socket\n",
        "\n",
        "def find_free_port():\n",
        "    s = socket.socket()\n",
        "    s.bind(('', 0))   # OS t·ª± t√¨m port free\n",
        "    port = s.getsockname()[1]\n",
        "    s.close()\n",
        "    return port\n",
        "\n",
        "print(\"Free port:\", find_free_port())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-47Ic949adM",
        "outputId": "cb598d7c-ec87-438b-ae8d-e3bdafef0f4c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free port: 47489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 ‚Äî Ngrok v3 ·ªïn ƒë·ªãnh (t·ª± ki·ªÉm tra ti·∫øn tr√¨nh, retry, l·∫•y URL ch·∫Øc ch·∫Øn)\n",
        "\n",
        "import os\n",
        "import subprocess, threading, time, requests, psutil\n",
        "\n",
        "PUBLIC_PORT = find_free_port()\n",
        "\n",
        "NGROK_TOKEN = os.environ.get(\"NGROK_TOKEN\", \"\").strip()\n",
        "if not NGROK_TOKEN:\n",
        "    raise ValueError(\"‚ùå NGROK_TOKEN ch∆∞a ƒë∆∞·ª£c ƒë·∫∑t trong m√¥i tr∆∞·ªùng!\")\n",
        "\n",
        "print(\"‚úîÔ∏è ƒê√£ l·∫•y NGROK_TOKEN t·ª´ m√¥i tr∆∞·ªùng.\")\n",
        "\n",
        "# --- Step 1: T·∫£i ngrok v3 ---\n",
        "!wget -q -O ngrok.zip https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -o ngrok.zip >/dev/null 2>&1\n",
        "\n",
        "# --- Step 2: C·∫•u h√¨nh token ---\n",
        "!./ngrok config add-authtoken $NGROK_TOKEN\n",
        "\n",
        "# --- Step 3: Ch·∫°y tunnel ---\n",
        "def run_ngrok():\n",
        "    subprocess.Popen([\"./ngrok\", \"http\", str(PUBLIC_PORT)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "threading.Thread(target=run_ngrok, daemon=True).start()\n",
        "\n",
        "print(\"‚è≥ ƒêang kh·ªüi ƒë·ªông ngrok...\")\n",
        "\n",
        "# --- Step 4: ƒê·ª£i ngrok kh·ªüi ƒë·ªông ho√†n to√†n ---\n",
        "def wait_for_ngrok(timeout=20):\n",
        "    for i in range(timeout):\n",
        "        # Ki·ªÉm tra process c√≥ ch·∫°y kh√¥ng\n",
        "        for p in psutil.process_iter(attrs=['cmdline']):\n",
        "            if p.info['cmdline'] and \"ngrok\" in \" \".join(p.info['cmdline']):\n",
        "                # th·ª≠ query API\n",
        "                try:\n",
        "                    r = requests.get(\"http://localhost:4040/api/tunnels\", timeout=1)\n",
        "                    if r.status_code == 200:\n",
        "                        return r.json()\n",
        "                except:\n",
        "                    pass\n",
        "        time.sleep(1)\n",
        "    return None\n",
        "\n",
        "tunnels = wait_for_ngrok()\n",
        "\n",
        "# --- Step 5: In URL ho·∫∑c b√°o l·ªói ---\n",
        "if tunnels:\n",
        "    public_url = tunnels[\"tunnels\"][0][\"public_url\"]\n",
        "    print(\"üîó NGROK URL:\", public_url)\n",
        "    print(\"üîó Health:\", public_url + \"/health\")\n",
        "else:\n",
        "    print(\"‚ùå Ngrok KH√îNG kh·ªüi ƒë·ªông ƒë∆∞·ª£c.\")\n",
        "    print(\"üî• G·ª£i √Ω s·ª≠a l·ªói:\")\n",
        "    print(\"1) Ki·ªÉm tra token c√≥ ƒë√∫ng ƒë·ªãnh d·∫°ng v3 (b·∫Øt ƒë·∫ßu b·∫±ng 2P...) kh√¥ng?\")\n",
        "    print(\"2) Colab c√≥ block port (th·ª≠ PORT = 8081 ho·∫∑c 9000)\")\n",
        "    print(\"3) Ch·∫°y l·∫°i cell 5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd9Lyw_l31Fv",
        "outputId": "f2fbc7d9-ef20-43a7-fb0d-f571e802bab9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è ƒê√£ l·∫•y NGROK_TOKEN t·ª´ m√¥i tr∆∞·ªùng.\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "‚è≥ ƒêang kh·ªüi ƒë·ªông ngrok...\n",
            "‚ùå Ngrok KH√îNG kh·ªüi ƒë·ªông ƒë∆∞·ª£c.\n",
            "üî• G·ª£i √Ω s·ª≠a l·ªói:\n",
            "1) Ki·ªÉm tra token c√≥ ƒë√∫ng ƒë·ªãnh d·∫°ng v3 (b·∫Øt ƒë·∫ßu b·∫±ng 2P...) kh√¥ng?\n",
            "2) Colab c√≥ block port (th·ª≠ PORT = 8081 ho·∫∑c 9000)\n",
            "3) Ch·∫°y l·∫°i cell 5\n"
          ]
        }
      ]
    }
  ]
}