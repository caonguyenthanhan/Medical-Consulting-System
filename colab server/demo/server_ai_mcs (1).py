# -*- coding: utf-8 -*-
"""server-AI-MCS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J37KPSwpYoSYz9lhVJyYvSnACqleIXpG
"""

# !pip install transformers accelerate sentencepiece bitsandbytes
# !pip install TTS  # XTTS-v2
# !pip install git+https://github.com/openai/whisper.git
# !pip install torchvision pillow

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os

CHAT_MODEL = "unsloth/Llama-3.2-3B"

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

chat_tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL)
chat_model = AutoModelForCausalLM.from_pretrained(
    CHAT_MODEL,
    load_in_4bit=True,
    device_map="auto",
    bnb_4bit_compute_dtype=torch.float16
)

from transformers import LlavaNextForConditionalGeneration, AutoProcessor

VLM_MODEL = "llava-hf/llava-1.5-7b-hf"

vlm_processor = AutoProcessor.from_pretrained(VLM_MODEL)
vlm_model = LlavaNextForConditionalGeneration.from_pretrained(
    VLM_MODEL,
    torch_dtype=torch.float16,
    device_map="auto"
)

from TTS.api import TTS

tts = TTS("coqui/XTTS-v2").to("cpu")

import whisper

stt_model = whisper.load_model("medium").to("cpu")

# ==========================
# üìå CELL 4 ‚Äî FULL API SERVER
# ==========================

import base64
import uuid
import os
from io import BytesIO
from fastapi import FastAPI, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import asyncio
import json
import subprocess
from pydantic import BaseModel
from PIL import Image
import torch
from fastapi.responses import FileResponse

app = FastAPI(title="Medical Consultation GPU API", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==============================
# üî∂ REQUEST MODELS
# ==============================

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: list[ChatMessage]
    model: str | None = None
    temperature: float | None = 0.7
    max_tokens: int | None = 256

class VisionChatRequest(BaseModel):
    text: str
    image_base64: str

class VisionMultiRequest(BaseModel):
    text: str
    images_base64: list[str]

class TextToSpeechRequest(BaseModel):
    text: str

class TextToSpeechStreamRequest(BaseModel):
    text: str


# ==============================
# üî∑ 1) CHAT COMPLETION API
# ==============================

@app.post("/v1/chat")
async def chat_api(req: ChatRequest, x_mode: str | None = None):
    user_msg = req.messages[-1].content
    mode = (x_mode or ("pro" if (req.model or "").lower() == "pro" else "flash")).lower()
    inputs = chat_tokenizer(user_msg, return_tensors="pt").to("cuda")
    output = chat_model.generate(
        **inputs,
        max_new_tokens=min(int(req.max_tokens or 256), 256),
        temperature=float(req.temperature or 0.7),
        do_sample=True if float(req.temperature or 0.7) > 0 else False
    )
    text = chat_tokenizer.decode(output[0], skip_special_tokens=True)
    return {"reply": text, "mode": mode}

@app.post("/v1/chat/completions")
async def chat_completions(req: ChatRequest, x_mode: str | None = None):
    user_msg = req.messages[-1].content
    mode = (x_mode or ("pro" if (req.model or "").lower() == "pro" else "flash")).lower()
    inputs = chat_tokenizer(user_msg, return_tensors="pt").to("cuda")
    output = chat_model.generate(
        **inputs,
        max_new_tokens=min(int(req.max_tokens or 256), 256),
        temperature=float(req.temperature or 0.7),
        do_sample=True if float(req.temperature or 0.7) > 0 else False
    )
    text = chat_tokenizer.decode(output[0], skip_special_tokens=True)
    return {
        "choices": [
            {
                "message": {
                    "role": "assistant",
                    "content": text
                }
            }
        ],
        "mode": mode
    }


# ==============================
# üî∂ 2) VISION-LANGUAGE API (LLAVA)
# ==============================

@app.post("/v1/vision")
async def vision_api(req: VisionChatRequest):

    # Decode base64 ‚Üí image
    image_bytes = base64.b64decode(req.image_base64)
    image = Image.open(BytesIO(image_bytes)).convert("RGB")

    inputs = vlm_processor(
        text=req.text,
        images=image,
        return_tensors="pt"
    ).to("cuda")

    output = vlm_model.generate(**inputs, max_new_tokens=256)
    text = vlm_processor.tokenizer.decode(output[0], skip_special_tokens=True)

    return {"reply": text}

@app.post("/v1/vision-multi")
async def vision_multi_api(req: VisionMultiRequest):
    replies = []
    for b64 in req.images_base64:
        image_bytes = base64.b64decode(b64)
        image = Image.open(BytesIO(image_bytes)).convert("RGB")
        inputs = vlm_processor(text=req.text, images=image, return_tensors="pt").to("cuda")
        output = vlm_model.generate(**inputs, max_new_tokens=256)
        t = vlm_processor.tokenizer.decode(output[0], skip_special_tokens=True)
        replies.append(t)
    return {"reply": "\n".join(replies)}


# ==============================
# üî∑ 3) TEXT TO SPEECH (XTTS v2)
# ==============================

@app.post("/v1/tts")
async def tts_api(req: TextToSpeechRequest):

    file_id = uuid.uuid4().hex
    out_path = f"/content/{file_id}.wav"

    tts.tts_to_file(
        text=req.text,
        file_path=out_path
    )

    return {
        "audio_url": f"/files/{file_id}.wav"
    }

@app.post("/v1/tts/stream")
async def tts_stream_api(req: TextToSpeechStreamRequest):
    async def gen():
        text = req.text.strip()
        parts = []
        buf = []
        for ch in text:
            buf.append(ch)
            if len(buf) >= 180 or ch in ".!?\n":
                parts.append("".join(buf).strip())
                buf = []
        if buf:
            parts.append("".join(buf).strip())
        idx = 0
        for p in parts:
            file_id = uuid.uuid4().hex
            out_path = f"/content/{file_id}.wav"
            tts.tts_to_file(text=p, file_path=out_path)
            with open(out_path, "rb") as f:
                b = f.read()
            payload = {"chunk_id": idx, "audio_base64": base64.b64encode(b).decode("ascii")}
            idx += 1
            yield json.dumps(payload) + "\n"
            await asyncio.sleep(0)
    return StreamingResponse(gen(), media_type="application/json")


# Serve audio files
@app.get("/files/{filename}")
async def get_audio(filename: str):
    path = f"/content/{filename}"
    return FileResponse(path, media_type="audio/wav")

@app.get("/health")
async def health():
    return {"status": "ok"}


# ==============================
# üî∂ 4) SPEECH TO TEXT (WHISPER)
# ==============================

@app.post("/v1/stt")
async def stt_api(file: UploadFile = File(...)):

    # Save uploaded file
    temp_path = "/content/input_audio.wav"
    with open(temp_path, "wb") as f:
        f.write(await file.read())

    # Transcribe using Whisper
    result = stt_model.transcribe(temp_path, language="vi")

    return {"text": result["text"]}

@app.post("/v1/stt/stream")
async def stt_stream_api(file: UploadFile = File(...)):
    temp_path = "/content/input_audio_stream.wav"
    with open(temp_path, "wb") as f:
        f.write(await file.read())
    full = stt_model.transcribe(temp_path, language="vi")
    text = full["text"]
    async def gen():
        tokens = text.split()
        cur = []
        for i, t in enumerate(tokens):
            cur.append(t)
            if len(cur) >= 12 or i == len(tokens) - 1:
                yield json.dumps({"partial": " ".join(cur)}) + "\n"
                cur = []
                await asyncio.sleep(0)
    return StreamingResponse(gen(), media_type="application/json")

@app.get("/gpu/metrics")
async def gpu_metrics():
    data = {}
    try:
        q = [
            "nvidia-smi",
            "--query-gpu=temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.used",
            "--format=csv,noheader,nounits",
        ]
        r = subprocess.run(q, capture_output=True, text=True)
        if r.returncode == 0:
            s = r.stdout.strip().split(",")
            data["gpu_temperature"] = float(s[0])
            data["gpu_utilization"] = float(s[1])
            data["mem_utilization"] = float(s[2])
            data["mem_total"] = float(s[3])
            data["mem_used"] = float(s[4])
    except Exception:
        pass
    return data


# ==============================
# üî∑ SERVER READY
# ==============================

print(f"üöÄ API Ready! Chat / Vision / TTS / STT ƒëang ch·∫°y tr√™n PORT {PORT}")

# CELL 5 ‚Äî START SERVER with PYNGROK (v2)

print("--- Kh·ªüi Ch·∫°y Server ---")

if IN_COLAB and USE_NGROK:
    try:
        token = NGROK_AUTH_TOKEN  # l·∫•y t·ª´ cell 2
        if token:
            print("üîë NGROK_AUTH_TOKEN ƒë√£ s·∫µn s√†ng.")

            # Kill process c≈© ƒë·ªÉ tr√°nh l·ªói ERR_NGROK_105
            ngrok.kill()

            import threading, time
            def run_server():
                import uvicorn
                uvicorn.run(app, host="0.0.0.0", port=PORT)
            th = threading.Thread(target=run_server, daemon=True)
            th.start()
            time.sleep(1)

            # M·ªü tunnel cho PORT ƒë√£ ch·ªçn
            public_url = ngrok.connect(PORT)
            print(f"‚úÖ Server URL (Ngrok): {public_url.public_url}")
            print(f"üëâ Copy URL n√†y d√°n v√†o Agent.")
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y token")

    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói Ngrok: {e}")
        print("‚ö†Ô∏è Server s·∫Ω ch·∫°y Localhost")

else:
    print("üåê Kh√¥ng ch·∫°y Colab ho·∫∑c USE_NGROK = False ‚Üí Server ch·∫°y Localhost")
